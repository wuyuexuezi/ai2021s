{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业2：纯天然手工制作神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作业中，你将体验到如何用最基本的矩阵运算来实现一个前馈神经网络。在该实现中，唯一需要使用到的软件包是Numpy。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第1题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用 Git 的基本操作，对第一次作业进行一次修订。具体要求如下：\n",
    "\n",
    "1. 将库的名称统一为 “ai2021s”。\n",
    "2. 在库的根目录创建文件夹 `HW1`，将第一次作业提交的 Notebook 文件重命名为 `numpy_practice.ipynb`，并移至 `HW1` 文件夹中。\n",
    "3. 提交这一修改，提交信息为 “reorganizing project files”。\n",
    "4. 从本次作业起，均按上述规则建立文件夹。本次作业的文件名为`fnn.ipynb`。\n",
    "5. 优化作业1中各函数的编写，直接对原始文件修改并利用 Git 命令提交。根据本次修改的结果，你将对作业1获得最多20分的加分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第2题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) 根据作业1的内容，编写**数值稳定**的函数 `sigmoid(x)` 和 `softplus(x)`，要求它适用于 Numpy 向量和矩阵，即当 `x` 是一维或二维数组时，返回一个相同大小和形状的数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0856306   0.99734545  0.2829785  -1.50629471 -0.57860025  1.65143654]\n",
      "Sigmoid(v) =\n",
      " [0.25244196 0.73053634 0.57027629 0.18148857 0.35925474 0.83908511]\n",
      "Sigmoid(m) =\n",
      " [[0.08116076 0.39438602 0.78004631]\n",
      " [0.29593301 0.33650995 0.47634044]\n",
      " [0.81628676 0.34549479 0.39079256]\n",
      " [0.39308777 0.90078077 0.89905661]\n",
      " [0.73185488 0.59536432 0.67642017]]\n",
      "Softplus(v) =\n",
      " [0.29094333 1.31132175 0.84461281 0.20026791 0.44512331 1.82687967]\n",
      "Softplus(m) =\n",
      " [[0.08464411 0.50151249 1.51433825]\n",
      " [0.35088177 0.41024142 0.6469135 ]\n",
      " [1.69437919 0.42387573 0.49559644]\n",
      " [0.49937109 2.31042345 2.29319537]\n",
      " [1.31622694 0.90476816 1.12830942]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(linewidth=100) \n",
    "\n",
    "def sigmoid(x):\n",
    "    res = np.zeros_like(x)\n",
    "    # Implementation here\n",
    "    res = np.where(x>=0,1/(1+np.exp(-x)),np.exp(x)/(1+np.exp(x)))\n",
    "    return res\n",
    "\n",
    "def softplus(x):\n",
    "    res = np.zeros_like(x)\n",
    "    # Implementation here\n",
    "    res = np.where(x>=0,x+np.log(1+np.exp(-x)),np.log(1+np.exp(x)))\n",
    "    return res\n",
    "\n",
    "np.random.seed(123)\n",
    "vec = np.random.normal(size=6)\n",
    "mat = np.random.normal(size=(5, 3))\n",
    "print(vec)\n",
    "print(\"Sigmoid(v) =\\n\", sigmoid(vec))\n",
    "print(\"Sigmoid(m) =\\n\", sigmoid(mat))\n",
    "print(\"Softplus(v) =\\n\", softplus(vec))\n",
    "print(\"Softplus(m) =\\n\", softplus(mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) 推导 Sigmoid 和 Softplus 函数的导数，编写**数值稳定**的函数 `d_sigmoid(x)` 和 `d_softplus(x)` 计算对应的导数，同样要求适用于向量和矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma(x)=1/(1+\\exp(-x)),\\ \\sigma'(x)=(1-\\sigma(x))*\\sigma(x)$\n",
    "\n",
    "$\\mathrm{softplus}(x)=\\log(1+\\exp(x)),\\ \\mathrm{softplus}'(x)=\\sigma(x)$\n",
    "\n",
    "注意到 Sigmoid 和 Softplus 之间的联系了吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid'(v) =\n",
      " [0.18871502 0.196853   0.24506124 0.14855047 0.23019077 0.13502129]\n",
      "Sigmoid'(m) =\n",
      " [[0.07457369 0.23884569 0.17157406]\n",
      " [0.20835666 0.223271   0.24944023]\n",
      " [0.14996269 0.22612814 0.23807373]\n",
      " [0.23856977 0.08937477 0.09075383]\n",
      " [0.19624332 0.24090565 0.21887592]]\n",
      "Softplus'(v) =\n",
      " [0.25244196 0.73053634 0.57027629 0.18148857 0.35925474 0.83908511]\n",
      "Softplus'(m) =\n",
      " [[0.08116076 0.39438602 0.78004631]\n",
      " [0.29593301 0.33650995 0.47634044]\n",
      " [0.81628676 0.34549479 0.39079256]\n",
      " [0.39308777 0.90078077 0.89905661]\n",
      " [0.73185488 0.59536432 0.67642017]]\n"
     ]
    }
   ],
   "source": [
    "def d_sigmoid(x):\n",
    "    res = np.zeros_like(x)\n",
    "    # Implementation here\n",
    "    res = (1-sigmoid(x))*sigmoid(x)\n",
    "    return res\n",
    "\n",
    "def d_softplus(x):\n",
    "    res = np.zeros_like(x)\n",
    "    # Implementation here\n",
    "    res = sigmoid(x)\n",
    "    return res\n",
    "\n",
    "print(\"Sigmoid'(v) =\\n\", d_sigmoid(vec))\n",
    "print(\"Sigmoid'(m) =\\n\", d_sigmoid(mat))\n",
    "print(\"Softplus'(v) =\\n\", d_softplus(vec))\n",
    "print(\"Softplus'(m) =\\n\", d_softplus(mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) 编写函数 `relu(x)` 和 `d_relu(x)` 来实现 ReLU 函数及其导数，$\\mathrm{ReLU}(x)=\\max(x,0)$，要求适用于向量和矩阵。对于不可导的点，可以将导数取为左导数或右导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU(v) =\n",
      " [0.         0.99734545 0.2829785  0.         0.         1.65143654]\n",
      "ReLU(m) =\n",
      " [[0.         0.         1.26593626]\n",
      " [0.         0.         0.        ]\n",
      " [1.49138963 0.         0.        ]\n",
      " [0.         2.20593008 2.18678609]\n",
      " [1.0040539  0.3861864  0.73736858]]\n",
      "ReLU'(v) =\n",
      " [0 1 1 0 0 1]\n",
      "ReLU'(m) =\n",
      " [[0 0 1]\n",
      " [0 0 0]\n",
      " [1 0 0]\n",
      " [0 1 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    res = np.zeros_like(x)\n",
    "    # Implementation here\n",
    "    res = np.where(x>=0,x,0)\n",
    "    return res\n",
    "\n",
    "def d_relu(x):\n",
    "    res = np.zeros_like(x)\n",
    "    # Implementation here\n",
    "    res = np.where(x>0,1,0)   #这里0处采用左导数\n",
    "    return res\n",
    "\n",
    "print(\"ReLU(v) =\\n\", relu(vec))\n",
    "print(\"ReLU(m) =\\n\", relu(mat))\n",
    "print(\"ReLU'(v) =\\n\", d_relu(vec))\n",
    "print(\"ReLU'(m) =\\n\", d_relu(mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第3题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "练习标量的反向传播算法。考虑一个两层的前馈神经网络（见第5周课件107页），其中数据和权重都是标量：\n",
    "\n",
    "![](img/model1.png)\n",
    "\n",
    "如下为正向的计算过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1.2345\n",
    "w1 = 0.111\n",
    "b1 = 0.222\n",
    "w2 = 0.333\n",
    "b2 = -0.444\n",
    "\n",
    "a0 = x\n",
    "z1 = w1 * a0 + b1\n",
    "a1 = sigmoid(z1)\n",
    "z2 = w2 * a1 + b2\n",
    "a2 = z2\n",
    "\n",
    "yhat = a2\n",
    "y = 5.4321"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令损失函数为 $l=(y-\\hat{y})^2$，则可以计算出 $l$ 对 $a_2=\\hat{y}$ 的导数为 $\\mathrm{d}l/\\mathrm{d}a_2=2(a_2-y)$："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_l_a2 = 2.0 * (a2 - y) # This is the \"upstream derivative\" associated with a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在将 $a_2$ 的导数反向传播给 $z_2$。首先计算 $a_2$ 对 $z_2$ 的“局部导数”：因为 $a_2=z_2$，所以局部导数为1。然后将“上游导数”乘以“局部导数”，即得到“下游导数”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11.360055548261299\n"
     ]
    }
   ],
   "source": [
    "d_a2_z2 = 1.0 # The \"local derivative\" of a2 with respect to z2\n",
    "d_l_z2 = d_l_a2 * d_a2_z2 # The \"downstream derivative\" of l with respect to z2\n",
    "\n",
    "print(d_l_z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来将 $z_2$ 的导数继续向下游传播。在上一层的计算中得到的“下游导数”现在变成了“上游导数”，进而向 $a_1$、$w_2$ 和 $b_2$ 传播。依然进行局部导数的计算，可以得到 $\\mathrm{d}z_2/\\mathrm{d}a_1=w_2$, $\\mathrm{d}z_2/\\mathrm{d}w_2=a_1$, $\\mathrm{d}z_2/\\mathrm{d}b_2=1$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_z2_a1 = w2\n",
    "d_z2_w2 = a1\n",
    "d_z2_b2 = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是通过链式法则，将“上游导数”乘以“局部导数”得到“下游导数”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.7828984975710127\n",
      "-6.688862995036224\n",
      "-11.360055548261299\n"
     ]
    }
   ],
   "source": [
    "d_l_a1 = d_l_z2 * d_z2_a1\n",
    "d_l_w2 = d_l_z2 * d_z2_w2\n",
    "d_l_b2 = d_l_z2 * d_z2_b2\n",
    "\n",
    "print(d_l_a1)\n",
    "print(d_l_w2)\n",
    "print(d_l_b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请完成后续的过程，计算出 $\\mathrm{d}l/\\mathrm{d}w_1$ 和 $\\mathrm{d}l/\\mathrm{d}b_1$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "d_l_w1 = ...\n",
    "d_l_b1 = ...\n",
    "\n",
    "print(d_l_w1)\n",
    "print(d_l_b1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_l_w1 =  -1.0718788521752436\n",
      "d_l_b1 =  -0.8682696250913273\n"
     ]
    }
   ],
   "source": [
    "d_a1_z1 = d_sigmoid(a1) \n",
    "d_l_z1 = d_a1_z1*d_l_a1\n",
    "\n",
    "d_z1_w1 = x\n",
    "d_z1_b1 = 1.0\n",
    "\n",
    "d_l_w1 = d_l_z1*d_z1_w1\n",
    "d_l_b1 = d_l_z1*d_z1_b1\n",
    "\n",
    "print('d_l_w1 = ',d_l_w1)\n",
    "print('d_l_b1 = ',d_l_b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第4题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在考虑一个更实际的场景，首先，数据包含多个观测，每个观测占据一行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0.0, 0.0],\n",
    "              [0.0, 1.0],\n",
    "              [1.0, 0.0],\n",
    "              [1.0, 1.0]])\n",
    "y = np.array([[0.0],\n",
    "              [1.0],\n",
    "              [1.0],\n",
    "              [0.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出输入的维度为 $p=2$，输出的维度为 $d=1$，样本量为 $n=4$。我们将构建一个两层的前馈神经网络，其中隐藏层的维度为 $r=5$。计算流程如下：\n",
    "\n",
    "![](img/model2.png)\n",
    "\n",
    "$Z_1=XW_1+\\mathbf{1}_nb_1^T,\\quad A_1=\\mathrm{softplus}(Z_1)$\n",
    "\n",
    "$Z_2=A_1W_2+\\mathbf{1}_nb_2^T,\\quad A_2=\\mathrm{sigmoid}(Z_2)$\n",
    "\n",
    "其中 $\\mathbf{1}_n$ 为元素全为1的 $n\\times 1$ 向量，$W_1$ 为 $p\\times r$ 矩阵，$b_1$ 为 $r\\times 1$ 向量，$W_2$ 为 $r\\times d$ 矩阵，$b_2$ 为 $d\\times 1$ 向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99818247]\n",
      " [0.98868386]\n",
      " [0.99961553]\n",
      " [0.99139015]]\n"
     ]
    }
   ],
   "source": [
    "p = 2\n",
    "d = 1\n",
    "n = 4\n",
    "r = 5\n",
    "\n",
    "np.random.seed(123)\n",
    "w1 = np.random.normal(size=(p, r))\n",
    "b1 = np.random.normal(size=(r, 1))\n",
    "w2 = np.random.normal(size=(r, d))\n",
    "b2 = np.random.normal(size=(d, 1))\n",
    "\n",
    "a0 = x\n",
    "ones = np.ones((n, 1))\n",
    "z1 = a0.dot(w1) + ones.dot(b1.T)\n",
    "a1 = softplus(z1)\n",
    "z2 = a1.dot(w2) + ones.dot(b2.T)\n",
    "a2 = sigmoid(z2)\n",
    "\n",
    "phat = a2\n",
    "print(phat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义第 $i$ 个观测的损失函数为 $l(y_i,\\hat{p}_i)=-y_i \\cdot \\log(\\hat{p}_i)-(1-y_i) \\cdot \\log(1-\\hat{p}_i)$，请推导出 $l$ 对 $\\hat{p}_i$ 的导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathrm{d}l/\\mathrm{d}\\hat{p}_i=-y_i/\\hat{p}_i+(1-y_i)*\\hat{p}_i/(1-\\hat{p}_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义整体的损失函数为 $L(y,\\hat{p})=n^{-1}\\sum_{i=1}^n l(y_i,p_i)$，请给出 $L$ 对 $\\hat{p}_i$ 的导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathrm{d}L/\\mathrm{d}\\hat{p}_i=n^{-1}*\\mathrm{d}l/\\mathrm{d}\\hat{p}_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 $\\mathrm{d}L/\\mathrm{d}\\hat{p}=(\\mathrm{d}L/\\mathrm{d}\\hat{p}_1,\\ldots,\\mathrm{d}L/\\mathrm{d}\\hat{p}_n)^T$ 看作一个向量，计算出其结果并赋给变量 `d_l_a2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "d_l_a2 = ...\n",
    "print(d_l_a2)\n",
    "\n",
    "if d_l_a2.shape != a2.shape:\n",
    "    print(\"Shapes do not match! Please redo the calculation.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[137.29924583]\n",
      " [ -0.25286142]\n",
      " [ -0.25009615]\n",
      " [ 28.78650883]]\n"
     ]
    }
   ],
   "source": [
    "d_l_a2 = 0.25*(-y/a2+(1-y)*a2/(1-a2))\n",
    "print(d_l_a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`d_l_a2` 是损失函数对 $A_2$ 的“上游导数”，我们需要将其传播给下游 $Z_2$。由于 $A_2=\\mathrm{sigmoid}(Z_2)$，根据课件中反向传播的规则1，可以得到下游导数为\n",
    "\n",
    "$\\mathrm{d}L/\\mathrm{d}Z_2=\\mathrm{d}L/\\mathrm{d}A_2\\circ \\mathrm{sigmoid}'(Z_2)$，\n",
    "\n",
    "其中 $\\mathrm{sigmoid}'$ 是 Sigmoid 的导数，$\\circ$ 是向量或矩阵的逐元素相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.49092060e-01]\n",
      " [-2.82903496e-03]\n",
      " [-9.61178724e-05]\n",
      " [ 2.45713607e-01]]\n"
     ]
    }
   ],
   "source": [
    "d_l_z2 = d_l_a2 * d_sigmoid(z2)\n",
    "print(d_l_z2)\n",
    "\n",
    "if d_l_z2.shape != z2.shape:\n",
    "    print(\"Shapes do not match! Please redo the calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，再将 $Z_2$ 的“上游导数”传给 $A_1$、$W_2$ 和 $b_2$。根据规则2，\n",
    "\n",
    "$\\mathrm{d}L/\\mathrm{d}A_1=(\\mathrm{d}L/\\mathrm{d}Z_2)W_2^T$，\n",
    "\n",
    "$\\mathrm{d}L/\\mathrm{d}W_2=A_1^T(\\mathrm{d}L/\\mathrm{d}Z_2)$，\n",
    "\n",
    "$\\mathrm{d}L/\\mathrm{d}b_2=(\\mathrm{d}L/\\mathrm{d}Z_2)^T\\mathbf{1}_n$。\n",
    "\n",
    "请根据如上公式计算对应的导数，并赋值给相应的变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "d_l_a1 = ...\n",
    "print(\"d_A1:\\n\", d_l_a1)\n",
    "\n",
    "d_l_w2 = ...\n",
    "print(\"d_W2:\\n\", d_l_w2)\n",
    "\n",
    "d_l_b2 = ...\n",
    "print(\"d_b2:\\n\", d_l_b2)\n",
    "\n",
    "if (d_l_a1.shape != a1.shape) or (d_l_w2.shape != w2.shape) or (d_l_b2.shape != b2.shape):\n",
    "    print(\"Shapes do not match! Please redo the calculation.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.08193454e-01  5.49479669e-01  5.44711053e-01  2.50101854e-01  9.61959659e-02]\n",
      " [ 1.22879494e-03 -6.24065331e-03 -6.18649429e-03 -2.84050357e-03 -1.09253482e-03]\n",
      " [ 4.17489205e-05 -2.12029306e-04 -2.10189226e-04 -9.65075245e-05 -3.71194151e-05]\n",
      " [-1.06726019e-01  5.42027038e-01  5.37323098e-01  2.46709705e-01  9.48912532e-02]]\n",
      "[[-1.08193454e-01  5.49479669e-01  5.44711053e-01  2.50101854e-01  9.61959659e-02]\n",
      " [ 1.22879494e-03 -6.24065331e-03 -6.18649429e-03 -2.84050357e-03 -1.09253482e-03]\n",
      " [ 4.17489205e-05 -2.12029306e-04 -2.10189226e-04 -9.65075245e-05 -3.71194151e-05]\n",
      " [-1.06726019e-01  5.42027038e-01  5.37323098e-01  2.46709705e-01  9.48912532e-02]]\n",
      "[0.49188051]\n"
     ]
    }
   ],
   "source": [
    "d_l_a1 = d_l_z2.dot(w2.T)\n",
    "print(d_l_a1)\n",
    "\n",
    "d_l_w2 = a1.T.dot(d_l_z2)\n",
    "print(d_l_a1)\n",
    "\n",
    "d_l_b2 = d_l_z2.T.dot(np.ones(n*1))\n",
    "print(d_l_b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请完成后续的过程，计算出 $\\mathrm{d}L/\\mathrm{d}W_1$ 和 $\\mathrm{d}L/\\mathrm{d}b_1$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "d_l_w1 = ...\n",
    "print(\"d_W1:\\n\", d_l_w1)\n",
    "\n",
    "d_l_b1 = ...\n",
    "print(\"d_b1:\\n\", d_l_b1)\n",
    "\n",
    "if (d_l_w1.shape != w1.shape) or (d_l_b1.shape != b1.shape):\n",
    "    print(\"Shapes do not match! Please redo the calculation.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.13388918 -0.1289426  -0.17883841 -0.12699446 -0.12363566]\n",
      " [ 0.67997999  0.65485789  0.9082626   0.64496391  0.62790567]\n",
      " [ 0.67407884  0.64917476  0.90038031  0.63936664  0.62245644]\n",
      " [ 0.30950055  0.29806594  0.41340594  0.29356258  0.28579833]\n",
      " [ 0.11904232  0.11464426  0.15900715  0.11291214  0.1099258 ]]\n",
      "[[ 0.47920202  0.46149773  0.64007952  0.45452514  0.4425037 ]\n",
      " [-1.87169458 -1.80254416 -2.50005911 -1.77531024 -1.7283562 ]]\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "d_l_z1 = d_l_a1.T.dot(d_softplus(a1))\n",
    "print(d_l_z1)\n",
    "d_l_w1 = w1.dot(d_l_z1)\n",
    "print(d_l_w1)\n",
    "d_l_b1 = np.ones(r*1).dot(d_l_z1.T)\n",
    "print(d_l_b1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第5题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将第4题中的步骤封装成三个函数：`forward()` 计算正向传播结果，得到预测值向量 `phat`，`loss()` 计算整体损失函数值，`backward()` 计算反向传播后各参数的导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, w1, b1, w2, b2):\n",
    "    # Implementation here\n",
    "    a0 = x\n",
    "    ones = np.ones((n, 1))\n",
    "    z1 = a0.dot(w1) + ones.dot(b1.T)\n",
    "    a1 = softplus(z1)\n",
    "    z2 = a1.dot(w2) + ones.dot(b2.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    return z1, a1, z2, a2\n",
    "\n",
    "def loss_function(y, phat):\n",
    "    loss = np.sum(-y*np.log(phat)-(1-y)*np.log(1-phat))/n\n",
    "    # Implementation here\n",
    "    return loss\n",
    "\n",
    "def backward(x, w1, b1, w2, b2, z1, a1, z2, a2, y):\n",
    "    # Implementation here\n",
    "    d_l_a2 = 0.25*(-y/a2+(1-y)*a2/(1-a2))\n",
    "    d_l_z2 = d_l_a2 * d_sigmoid(z2)\n",
    "    d_l_a1 = d_l_z2.dot(w2.T)\n",
    "    d_l_w2 = a1.T.dot(d_l_z2)\n",
    "    d_l_b2 = d_l_z2.T.dot(np.ones(n*1))\n",
    "    d_l_z1 = d_l_a1.T.dot(d_softplus(a1))\n",
    "    d_l_w1 = w1.dot(d_l_z1.T)\n",
    "    d_l_b1 = np.ones(r*1).dot(d_l_z1.T).reshape(r,1)     #这里不做reshape的话是一个（r,）的数组，在迭代中会出bug\n",
    "    return d_l_w1, d_l_b1, d_l_w2, d_l_b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与之前的结果互相印证："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phat =\n",
      " [[0.99818247]\n",
      " [0.98868386]\n",
      " [0.99961553]\n",
      " [0.99139015]]\n",
      "loss = 2.7692224724634995\n",
      "(dw1, db1, dw2, db2) =\n",
      " (array([[ 0.22897316, -1.16288087, -1.1527889 , -0.52929831, -0.20358251],\n",
      "       [ 0.11489203, -0.58349957, -0.57843571, -0.26558639, -0.10215174]]), array([[-0.69230031],\n",
      "       [ 3.51597007],\n",
      "       [ 3.48545699],\n",
      "       [ 1.60033334],\n",
      "       [ 0.61553167]]), array([[0.25532954],\n",
      "       [0.20922572],\n",
      "       [0.80549136],\n",
      "       [0.18789877],\n",
      "       [0.15733642]]), array([0.49188051]))\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.0, 0.0],\n",
    "              [0.0, 1.0],\n",
    "              [1.0, 0.0],\n",
    "              [1.0, 1.0]])\n",
    "y = np.array([[0.0],\n",
    "              [1.0],\n",
    "              [1.0],\n",
    "              [0.0]])\n",
    "p = 2\n",
    "d = 1\n",
    "n = 4\n",
    "r = 5\n",
    "\n",
    "np.random.seed(123)\n",
    "w1 = np.random.normal(size=(p, r))\n",
    "b1 = np.random.normal(size=(r, 1))\n",
    "w2 = np.random.normal(size=(r, d))\n",
    "b2 = np.random.normal(size=(d, 1))\n",
    "\n",
    "z1, a1, z2, a2 = forward(x, w1, b1, w2, b2)\n",
    "phat = a2\n",
    "loss = loss_function(y, phat)\n",
    "grads = backward(x, w1, b1, w2, b2, z1, a1, z2, a2, y)\n",
    "\n",
    "print(\"phat =\\n\", phat)\n",
    "print(\"loss =\", loss)\n",
    "print(\"(dw1, db1, dw2, db2) =\\n\", grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后将这三步放在循环中，并利用梯度下降法拟合出 $x$ 和 $y$ 之间的函数关系。下面是一个完整的神经网络模型，我们可以修改隐藏层的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss = 4.616747553776374, prediction = [5.14780976e-04 3.24250638e-04 2.94601507e-05 9.50104717e-06]\n",
      "iteration 50, loss = 0.9392673903224034, prediction = [0.37162552 0.86417037 0.0742406  0.42075004]\n",
      "iteration 100, loss = 0.8449963576137276, prediction = [0.52651407 0.91399594 0.21915262 0.64100122]\n",
      "iteration 150, loss = 0.7920146164373032, prediction = [0.53745485 0.89083036 0.29445752 0.65313705]\n",
      "iteration 200, loss = 0.7532444339080195, prediction = [0.53585975 0.86092913 0.34938893 0.64799031]\n",
      "iteration 250, loss = 0.7274336187724053, prediction = [0.53541843 0.83289074 0.39456256 0.64309511]\n",
      "iteration 300, loss = 0.710192747881431, prediction = [0.53661198 0.80861578 0.432197   0.63950431]\n",
      "iteration 350, loss = 0.6982728362256287, prediction = [0.5387252  0.78825551 0.46354109 0.6367038 ]\n",
      "iteration 400, loss = 0.6896199543008914, prediction = [0.54120352 0.77144081 0.48968797 0.63426602]\n",
      "iteration 450, loss = 0.6829713235511354, prediction = [0.54371376 0.75767417 0.51159511 0.63194642]\n",
      "iteration 500, loss = 0.6775510748104242, prediction = [0.54607327 0.74646654 0.53006818 0.62962183]\n",
      "iteration 550, loss = 0.672880302430645, prediction = [0.54819072 0.73738194 0.54576832 0.62723835]\n",
      "iteration 600, loss = 0.6686624926322606, prediction = [0.55002802 0.73004694 0.55923126 0.62477948]\n",
      "iteration 650, loss = 0.6647143811231291, prediction = [0.55157724 0.7241474  0.57088877 0.62224804]\n",
      "iteration 700, loss = 0.6609237727846682, prediction = [0.55284701 0.71942119 0.58108823 0.61965618]\n",
      "iteration 750, loss = 0.6572234613625181, prediction = [0.55385451 0.71565038 0.59010913 0.61701994]\n",
      "iteration 800, loss = 0.6535748911834165, prediction = [0.55462081 0.71265402 0.59817655 0.61435644]\n",
      "iteration 850, loss = 0.6499578022176518, prediction = [0.55516827 0.71028201 0.6054718  0.61168232]\n",
      "iteration 900, loss = 0.6463636022950929, prediction = [0.555519   0.70840977 0.61214102 0.60901311]\n",
      "iteration 950, loss = 0.6427910897276878, prediction = [0.55569412 0.70693398 0.61830192 0.60636293]\n",
      "iteration 1000, loss = 0.6392436730077136, prediction = [0.55571337 0.70576892 0.62404918 0.60374441]\n",
      "iteration 1050, loss = 0.635727551282212, prediction = [0.55559496 0.70484357 0.62945877 0.6011688 ]\n",
      "iteration 1100, loss = 0.6322505145761568, prediction = [0.55535557 0.70409911 0.63459135 0.59864604]\n",
      "iteration 1150, loss = 0.6288211448671346, prediction = [0.55501041 0.70348703 0.63949515 0.59618487]\n",
      "iteration 1200, loss = 0.6254482765499411, prediction = [0.5545733  0.70296741 0.64420811 0.59379297]\n",
      "iteration 1250, loss = 0.6221406245283464, prediction = [0.55405678 0.70250762 0.64875983 0.59147703]\n",
      "iteration 1300, loss = 0.6189065204206062, prediction = [0.55347219 0.70208124 0.653173   0.58924283]\n",
      "iteration 1350, loss = 0.6157537185034778, prediction = [0.55282977 0.70166715 0.65746465 0.58709533]\n",
      "iteration 1400, loss = 0.6126892470036537, prediction = [0.55213872 0.70124879 0.6616472  0.58503865]\n",
      "iteration 1450, loss = 0.6097192896721482, prediction = [0.55140728 0.70081357 0.66572927 0.58307616]\n",
      "iteration 1500, loss = 0.6068490888332807, prediction = [0.55064277 0.7003524  0.66971643 0.58121044]\n",
      "iteration 1550, loss = 0.6040828652993582, prediction = [0.54985165 0.69985922 0.67361178 0.5794433 ]\n",
      "iteration 1600, loss = 0.6014237533342431, prediction = [0.54903951 0.69933067 0.67741644 0.57777579]\n",
      "iteration 1650, loss = 0.598873750650205, prediction = [0.54821117 0.69876579 0.68113004 0.57620817]\n",
      "iteration 1700, loss = 0.5964336845012761, prediction = [0.54737066 0.69816573 0.68475105 0.5747399 ]\n",
      "iteration 1750, loss = 0.5941031954684315, prediction = [0.54652126 0.69753351 0.68827712 0.5733697 ]\n",
      "iteration 1800, loss = 0.5918807406368511, prediction = [0.54566554 0.69687379 0.69170534 0.57209545]\n",
      "iteration 1850, loss = 0.5897636176310317, prediction = [0.5448054  0.69619261 0.69503253 0.57091433]\n",
      "iteration 1900, loss = 0.5877480104715904, prediction = [0.54394211 0.6954972  0.69825541 0.56982273]\n",
      "iteration 1950, loss = 0.5858290575144145, prediction = [0.54307635 0.69479571 0.70137081 0.5688164 ]\n",
      "iteration 2000, loss = 0.5840009408943752, prediction = [0.54220828 0.69409704 0.70437579 0.56789041]\n",
      "iteration 2050, loss = 0.5822569959899375, prediction = [0.54133759 0.69341059 0.70726781 0.56703929]\n",
      "iteration 2100, loss = 0.5805898385206713, prediction = [0.54046359 0.692746   0.71004484 0.56625706]\n",
      "iteration 2150, loss = 0.5789915060545415, prediction = [0.53958523 0.692113   0.7127054  0.56553736]\n",
      "iteration 2200, loss = 0.577453609998307, prediction = [0.53870124 0.69152116 0.71524866 0.56487351]\n",
      "iteration 2250, loss = 0.5759674936252192, prediction = [0.53781016 0.6909797  0.7176745  0.56425864]\n",
      "iteration 2300, loss = 0.5745243913986231, prediction = [0.5369104  0.6904973  0.71998346 0.56368577]\n",
      "iteration 2350, loss = 0.5731155848002347, prediction = [0.53600038 0.69008197 0.72217684 0.56314794]\n",
      "iteration 2400, loss = 0.5717325500712445, prediction = [0.53507849 0.68974088 0.72425662 0.5626383 ]\n",
      "iteration 2450, loss = 0.5703670937074897, prediction = [0.53414324 0.68948027 0.72622543 0.56215018]\n",
      "iteration 2500, loss = 0.5690114721840407, prediction = [0.53319327 0.68930536 0.72808657 0.56167722]\n",
      "iteration 2550, loss = 0.5676584931729232, prediction = [0.53222736 0.68922029 0.72984388 0.5612134 ]\n",
      "iteration 2600, loss = 0.5663015964043285, prediction = [0.53124454 0.68922812 0.73150173 0.56075312]\n",
      "iteration 2650, loss = 0.5649349132462299, prediction = [0.53024402 0.68933078 0.73306494 0.56029126]\n",
      "iteration 2700, loss = 0.5635533049809448, prediction = [0.52922526 0.68952917 0.73453866 0.55982317]\n",
      "iteration 2750, loss = 0.562152380586944, prediction = [0.52818795 0.68982315 0.73592837 0.55934474]\n",
      "iteration 2800, loss = 0.5607284955473584, prediction = [0.52713204 0.69021163 0.73723973 0.55885238]\n",
      "iteration 2850, loss = 0.5592787337729391, prediction = [0.52605766 0.69069266 0.73847856 0.55834303]\n",
      "iteration 2900, loss = 0.5578008751301143, prediction = [0.52496517 0.69126353 0.7396507  0.55781412]\n",
      "iteration 2950, loss = 0.5562933513011008, prediction = [0.52385511 0.69192085 0.74076199 0.5572636 ]\n",
      "iteration 3000, loss = 0.5547551927813317, prediction = [0.52272818 0.6926607  0.74181822 0.55668984]\n",
      "iteration 3050, loss = 0.553185969757758, prediction = [0.52158519 0.69347869 0.742825   0.55609167]\n",
      "iteration 3100, loss = 0.5515857294344528, prediction = [0.5204271  0.69437011 0.74378778 0.55546828]\n",
      "iteration 3150, loss = 0.5499549321074444, prediction = [0.51925493 0.69533    0.74471178 0.55481923]\n",
      "iteration 3200, loss = 0.5482943879677635, prediction = [0.51806975 0.69635325 0.74560194 0.55414436]\n",
      "iteration 3250, loss = 0.546605196257665, prediction = [0.51687268 0.69743473 0.74646292 0.55344381]\n",
      "iteration 3300, loss = 0.5448886880440706, prediction = [0.51566487 0.69856927 0.74729907 0.55271791]\n",
      "iteration 3350, loss = 0.5431463735252797, prediction = [0.51444743 0.69975183 0.7481144  0.55196721]\n",
      "iteration 3400, loss = 0.5413798944669633, prediction = [0.51322149 0.70097748 0.74891261 0.55119241]\n",
      "iteration 3450, loss = 0.5395909820815603, prediction = [0.51198814 0.70224149 0.74969705 0.55039433]\n",
      "iteration 3500, loss = 0.5377814204272693, prediction = [0.5107484  0.70353933 0.75047077 0.54957387]\n",
      "iteration 3550, loss = 0.5359530152107906, prediction = [0.5095033  0.70486672 0.75123645 0.54873204]\n",
      "iteration 3600, loss = 0.5341075677308367, prediction = [0.50825376 0.70621964 0.75199652 0.54786987]\n",
      "iteration 3650, loss = 0.5322468535939433, prediction = [0.50700067 0.70759434 0.75275308 0.54698843]\n",
      "iteration 3700, loss = 0.5303726057654918, prediction = [0.50574487 0.70898734 0.75350797 0.54608881]\n",
      "iteration 3750, loss = 0.5284865014816765, prediction = [0.50448711 0.71039545 0.75426277 0.54517208]\n",
      "iteration 3800, loss = 0.5265901525365441, prediction = [0.50322812 0.71181573 0.7550188  0.54423932]\n",
      "iteration 3850, loss = 0.524685098466618, prediction = [0.50196854 0.71324551 0.75577721 0.54329159]\n",
      "iteration 3900, loss = 0.5227728021787932, prediction = [0.50070896 0.71468237 0.7565389  0.54232991]\n",
      "iteration 3950, loss = 0.5208546476005373, prediction = [0.49944994 0.71612411 0.75730462 0.5413553 ]\n",
      "iteration 4000, loss = 0.518931938971131, prediction = [0.49819196 0.71756878 0.75807494 0.5403687 ]\n",
      "iteration 4050, loss = 0.5170059014355052, prediction = [0.49693547 0.71901462 0.7588503  0.53937105]\n",
      "iteration 4100, loss = 0.5150776826457886, prediction = [0.49568087 0.72046005 0.75963102 0.53836324]\n",
      "iteration 4150, loss = 0.513148355118038, prediction = [0.49442854 0.72190368 0.76041728 0.53734611]\n",
      "iteration 4200, loss = 0.5112189191315623, prediction = [0.4931788  0.72334428 0.76120919 0.53632047]\n",
      "iteration 4250, loss = 0.5092903059948902, prediction = [0.49193195 0.72478076 0.76200677 0.53528708]\n",
      "iteration 4300, loss = 0.5073633815352936, prediction = [0.49068826 0.72621217 0.76280996 0.53424667]\n",
      "iteration 4350, loss = 0.5054389496976872, prediction = [0.48944797 0.72763766 0.76361865 0.53319994]\n",
      "iteration 4400, loss = 0.5035177561637175, prediction = [0.48821129 0.72905652 0.76443266 0.53214752]\n",
      "iteration 4450, loss = 0.5016004919230646, prediction = [0.48697843 0.7304681  0.76525181 0.53109005]\n",
      "iteration 4500, loss = 0.4996877967467369, prediction = [0.48574956 0.73187186 0.76607584 0.53002809]\n",
      "iteration 4550, loss = 0.49778026252673535, prediction = [0.48452484 0.73326733 0.76690449 0.52896221]\n",
      "iteration 4600, loss = 0.4958784364582725, prediction = [0.48330441 0.73465409 0.76773748 0.52789291]\n",
      "iteration 4650, loss = 0.49398282405014504, prediction = [0.48208841 0.73603182 0.7685745  0.52682068]\n",
      "iteration 4700, loss = 0.49209389195616915, prediction = [0.48087695 0.7374002  0.76941523 0.52574599]\n",
      "iteration 4750, loss = 0.490212070626199, prediction = [0.47967015 0.73875901 0.77025937 0.52466927]\n",
      "iteration 4800, loss = 0.48833775677939395, prediction = [0.47846809 0.74010803 0.77110657 0.52359092]\n",
      "iteration 4850, loss = 0.4864713157053809, prediction = [0.47727088 0.74144709 0.77195652 0.52251134]\n",
      "iteration 4900, loss = 0.4846130834010074, prediction = [0.47607858 0.74277607 0.7728089  0.52143088]\n",
      "iteration 4950, loss = 0.4827633685516721, prediction = [0.47489128 0.74409485 0.77366338 0.52034988]\n",
      "iteration 5000, loss = 0.4809224543669468, prediction = [0.47370905 0.74540336 0.77451965 0.51926866]\n",
      "iteration 5050, loss = 0.47909060028047756, prediction = [0.47253193 0.74670153 0.7753774  0.51818753]\n",
      "iteration 5100, loss = 0.47726804352413266, prediction = [0.47136    0.74798932 0.77623632 0.51710678]\n",
      "iteration 5150, loss = 0.4754550005860835, prediction = [0.47019329 0.7492667  0.77709613 0.51602666]\n",
      "iteration 5200, loss = 0.47365166856208074, prediction = [0.46903187 0.75053368 0.77795654 0.51494744]\n",
      "iteration 5250, loss = 0.47185822640865654, prediction = [0.46787576 0.75179024 0.77881728 0.51386935]\n",
      "iteration 5300, loss = 0.4700748361064063, prediction = [0.46672501 0.7530364  0.77967807 0.51279261]\n",
      "iteration 5350, loss = 0.4683016437408791, prediction = [0.46557965 0.75427219 0.78053866 0.51171744]\n",
      "iteration 5400, loss = 0.4665387805080082, prediction = [0.46443972 0.75549765 0.78139882 0.51064402]\n",
      "iteration 5450, loss = 0.46478636365040116, prediction = [0.46330523 0.7567128  0.7822583  0.50957256]\n",
      "iteration 5500, loss = 0.46304449733026143, prediction = [0.46217622 0.75791771 0.78311687 0.50850321]\n",
      "iteration 5550, loss = 0.4613132734441582, prediction = [0.46105271 0.75911242 0.78397433 0.50743615]\n",
      "iteration 5600, loss = 0.459592772384384, prediction = [0.45993472 0.76029699 0.78483046 0.50637153]\n",
      "iteration 5650, loss = 0.457883063751183, prediction = [0.45882225 0.76147149 0.78568507 0.5053095 ]\n",
      "iteration 5700, loss = 0.45618420701970985, prediction = [0.45771534 0.76263597 0.78653798 0.50425019]\n",
      "iteration 5750, loss = 0.45449625216522516, prediction = [0.45661398 0.76379052 0.787389   0.50319373]\n",
      "iteration 5800, loss = 0.4528192402496791, prediction = [0.45551819 0.76493521 0.78823797 0.50214023]\n",
      "iteration 5850, loss = 0.4511532039725427, prediction = [0.45442798 0.76607011 0.78908473 0.50108982]\n",
      "iteration 5900, loss = 0.4494981681884802, prediction = [0.45334334 0.76719529 0.78992912 0.50004259]\n",
      "iteration 5950, loss = 0.4478541503942097, prediction = [0.45226429 0.76831084 0.79077101 0.49899865]\n",
      "iteration 6000, loss = 0.4462211611866824, prediction = [0.45119082 0.76941685 0.79161025 0.49795808]\n",
      "iteration 6050, loss = 0.4445992046945362, prediction = [0.45012294 0.77051339 0.79244672 0.49692098]\n",
      "iteration 6100, loss = 0.44298827898459003, prediction = [0.44906064 0.77160054 0.79328031 0.49588741]\n",
      "iteration 6150, loss = 0.44138837644500806, prediction = [0.44800392 0.7726784  0.79411088 0.49485746]\n",
      "iteration 6200, loss = 0.4397994841466218, prediction = [0.44695277 0.77374704 0.79493835 0.4938312 ]\n",
      "iteration 6250, loss = 0.43822158418378226, prediction = [0.44590718 0.77480655 0.7957626  0.49280869]\n",
      "iteration 6300, loss = 0.43665465399600223, prediction = [0.44486716 0.77585703 0.79658354 0.49178998]\n",
      "iteration 6350, loss = 0.43509866667155483, prediction = [0.44383268 0.77689855 0.79740109 0.49077514]\n",
      "iteration 6400, loss = 0.43355359123410764, prediction = [0.44280374 0.7779312  0.79821517 0.48976422]\n",
      "iteration 6450, loss = 0.43201939291339375, prediction = [0.44178032 0.77895507 0.79902569 0.48875726]\n",
      "iteration 6500, loss = 0.43049603340084797, prediction = [0.44076242 0.77997025 0.79983258 0.4877543 ]\n",
      "iteration 6550, loss = 0.4289834710910755, prediction = [0.43975002 0.78097682 0.80063578 0.48675539]\n",
      "iteration 6600, loss = 0.4274816613099625, prediction = [0.4387431  0.78197486 0.80143523 0.48576057]\n",
      "iteration 6650, loss = 0.4259905565301769, prediction = [0.43774164 0.78296448 0.80223086 0.48476986]\n",
      "iteration 6700, loss = 0.42451010657477417, prediction = [0.43674563 0.78394574 0.80302263 0.4837833 ]\n",
      "iteration 6750, loss = 0.423040258809561, prediction = [0.43575506 0.78491875 0.80381048 0.48280091]\n",
      "iteration 6800, loss = 0.42158095832484266, prediction = [0.43476989 0.78588357 0.80459438 0.48182271]\n",
      "iteration 6850, loss = 0.4201321481071345, prediction = [0.43379011 0.7868403  0.80537427 0.48084874]\n",
      "iteration 6900, loss = 0.41869376920138474, prediction = [0.43281571 0.78778902 0.80615013 0.47987901]\n",
      "iteration 6950, loss = 0.41726576086422285, prediction = [0.43184665 0.78872982 0.80692191 0.47891353]\n",
      "iteration 7000, loss = 0.4158480607087209, prediction = [0.43088293 0.78966277 0.80768959 0.47795233]\n",
      "iteration 7050, loss = 0.4144406048411227, prediction = [0.4299245  0.79058796 0.80845315 0.47699541]\n",
      "iteration 7100, loss = 0.41304332798997045, prediction = [0.42897136 0.79150547 0.80921254 0.47604278]\n",
      "iteration 7150, loss = 0.41165616362804003, prediction = [0.42802347 0.79241538 0.80996776 0.47509446]\n",
      "iteration 7200, loss = 0.41027904408745963, prediction = [0.42708081 0.79331777 0.81071879 0.47415044]\n",
      "iteration 7250, loss = 0.40891190066838573, prediction = [0.42614337 0.79421273 0.8114656  0.47321075]\n",
      "iteration 7300, loss = 0.40755466374156757, prediction = [0.4252111  0.79510032 0.81220819 0.47227537]\n",
      "iteration 7350, loss = 0.40620726284513, prediction = [0.42428399 0.79598064 0.81294654 0.47134431]\n",
      "iteration 7400, loss = 0.4048696267758761, prediction = [0.42336201 0.79685375 0.81368064 0.47041758]\n",
      "iteration 7450, loss = 0.4035416836754069, prediction = [0.42244513 0.79771973 0.81441049 0.46949516]\n",
      "iteration 7500, loss = 0.4022233611113185, prediction = [0.42153332 0.79857866 0.81513608 0.46857707]\n",
      "iteration 7550, loss = 0.40091458615374764, prediction = [0.42062657 0.79943062 0.8158574  0.46766329]\n",
      "iteration 7600, loss = 0.39961528544750496, prediction = [0.41972483 0.80027567 0.81657446 0.46675382]\n",
      "iteration 7650, loss = 0.39832538528002615, prediction = [0.41882809 0.8011139  0.81728726 0.46584866]\n",
      "iteration 7700, loss = 0.3970448116453664, prediction = [0.41793631 0.80194537 0.81799579 0.4649478 ]\n",
      "iteration 7750, loss = 0.39577349030444164, prediction = [0.41704947 0.80277016 0.81870006 0.46405122]\n",
      "iteration 7800, loss = 0.3945113468417116, prediction = [0.41616753 0.80358833 0.81940007 0.46315893]\n",
      "iteration 7850, loss = 0.3932583067184987, prediction = [0.41529047 0.80439997 0.82009584 0.46227092]\n",
      "iteration 7900, loss = 0.392014295323111, prediction = [0.41441826 0.80520513 0.82078736 0.46138717]\n",
      "iteration 7950, loss = 0.390779238017943, prediction = [0.41355087 0.80600389 0.82147465 0.46050766]\n",
      "iteration 8000, loss = 0.3895530601837096, prediction = [0.41268827 0.80679632 0.82215772 0.4596324 ]\n",
      "iteration 8050, loss = 0.3883356872609653, prediction = [0.41183043 0.80758248 0.82283657 0.45876137]\n",
      "iteration 8100, loss = 0.38712704478904986, prediction = [0.41097732 0.80836244 0.82351122 0.45789455]\n",
      "iteration 8150, loss = 0.38592705844259584, prediction = [0.41012891 0.80913627 0.82418168 0.45703193]\n",
      "iteration 8200, loss = 0.3847356540657284, prediction = [0.40928517 0.80990403 0.82484797 0.45617349]\n",
      "iteration 8250, loss = 0.3835527577040755, prediction = [0.40844608 0.81066578 0.8255101  0.45531923]\n",
      "iteration 8300, loss = 0.38237829563470643, prediction = [0.40761159 0.81142159 0.82616808 0.45446912]\n",
      "iteration 8350, loss = 0.38121219439410614, prediction = [0.40678169 0.81217153 0.82682193 0.45362314]\n",
      "iteration 8400, loss = 0.38005438080428977, prediction = [0.40595635 0.81291565 0.82747167 0.45278129]\n",
      "iteration 8450, loss = 0.3789047819971569, prediction = [0.40513552 0.81365402 0.82811732 0.45194355]\n",
      "iteration 8500, loss = 0.3777633254371753, prediction = [0.40431919 0.8143867  0.82875889 0.45110989]\n",
      "iteration 8550, loss = 0.3766299389424875, prediction = [0.40350731 0.81511374 0.82939641 0.45028031]\n",
      "iteration 8600, loss = 0.37550455070452027, prediction = [0.40269987 0.81583521 0.83002988 0.44945477]\n",
      "iteration 8650, loss = 0.37438708930617526, prediction = [0.40189684 0.81655117 0.83065934 0.44863327]\n",
      "iteration 8700, loss = 0.3732774837386844, prediction = [0.40109818 0.81726167 0.8312848  0.44781578]\n",
      "iteration 8750, loss = 0.37217566341719244, prediction = [0.40030385 0.81796677 0.83190628 0.44700229]\n",
      "iteration 8800, loss = 0.371081558195138, prediction = [0.39951385 0.81866652 0.83252381 0.44619278]\n",
      "iteration 8850, loss = 0.3699950983775, prediction = [0.39872813 0.81936099 0.8331374  0.44538722]\n",
      "iteration 8900, loss = 0.3689162147329688, prediction = [0.39794666 0.82005023 0.83374709 0.4445856 ]\n",
      "iteration 8950, loss = 0.36784483850509875, prediction = [0.39716941 0.82073429 0.83435288 0.44378789]\n",
      "iteration 9000, loss = 0.366780901422499, prediction = [0.39639637 0.82141323 0.8349548  0.44299408]\n",
      "iteration 9050, loss = 0.3657243357081157, prediction = [0.39562749 0.8220871  0.83555288 0.44220415]\n",
      "iteration 9100, loss = 0.3646750740876542, prediction = [0.39486274 0.82275595 0.83614714 0.44141808]\n",
      "iteration 9150, loss = 0.3636330497971853, prediction = [0.39410211 0.82341984 0.8367376  0.44063584]\n",
      "iteration 9200, loss = 0.3625981965899884, prediction = [0.39334555 0.82407881 0.83732428 0.43985741]\n",
      "iteration 9250, loss = 0.3615704487426643, prediction = [0.39259304 0.82473292 0.83790722 0.43908278]\n",
      "iteration 9300, loss = 0.3605497410605662, prediction = [0.39184456 0.82538222 0.83848643 0.43831192]\n",
      "iteration 9350, loss = 0.35953600888258064, prediction = [0.39110007 0.82602675 0.83906193 0.43754482]\n",
      "iteration 9400, loss = 0.3585291880852999, prediction = [0.39035954 0.82666657 0.83963376 0.43678144]\n",
      "iteration 9450, loss = 0.3575292150866166, prediction = [0.38962295 0.82730172 0.84020193 0.43602177]\n",
      "iteration 9500, loss = 0.3565360268487769, prediction = [0.38889027 0.82793226 0.84076647 0.43526579]\n",
      "iteration 9550, loss = 0.3555495608809168, prediction = [0.38816147 0.82855822 0.84132741 0.43451348]\n",
      "iteration 9600, loss = 0.3545697552411199, prediction = [0.38743652 0.82917966 0.84188477 0.43376481]\n",
      "iteration 9650, loss = 0.35359654853801814, prediction = [0.3867154  0.82979662 0.84243857 0.43301977]\n",
      "iteration 9700, loss = 0.3526298799319628, prediction = [0.38599807 0.83040914 0.84298884 0.43227832]\n",
      "iteration 9750, loss = 0.3516696891357936, prediction = [0.38528452 0.83101728 0.8435356  0.43154046]\n",
      "iteration 9800, loss = 0.35071591641522826, prediction = [0.38457471 0.83162107 0.84407888 0.43080616]\n",
      "iteration 9850, loss = 0.3497685025888948, prediction = [0.38386861 0.83222057 0.8446187  0.4300754 ]\n",
      "iteration 9900, loss = 0.34882738902802835, prediction = [0.3831662  0.8328158  0.84515509 0.42934815]\n",
      "iteration 9950, loss = 0.34789251765585294, prediction = [0.38246746 0.83340682 0.84568807 0.4286244 ]\n",
      "iteration 10000, loss = 0.3469638309466687, prediction = [0.38177235 0.83399367 0.84621767 0.42790412]\n",
      "iteration 10050, loss = 0.34604127192466116, prediction = [0.38108084 0.83457639 0.84674391 0.4271873 ]\n",
      "iteration 10100, loss = 0.3451247841624508, prediction = [0.38039293 0.83515502 0.84726681 0.4264739 ]\n",
      "iteration 10150, loss = 0.3442143117794001, prediction = [0.37970856 0.8357296  0.84778641 0.42576392]\n",
      "iteration 10200, loss = 0.3433097994396932, prediction = [0.37902773 0.83630018 0.84830272 0.42505733]\n",
      "iteration 10250, loss = 0.34241119235020046, prediction = [0.37835041 0.83686678 0.84881576 0.4243541 ]\n",
      "iteration 10300, loss = 0.3415184362581506, prediction = [0.37767656 0.83742946 0.84932558 0.42365422]\n",
      "iteration 10350, loss = 0.34063147744861, prediction = [0.37700617 0.83798824 0.84983218 0.42295766]\n",
      "iteration 10400, loss = 0.3397502627417962, prediction = [0.37633921 0.83854317 0.85033559 0.42226441]\n",
      "iteration 10450, loss = 0.3388747394902273, prediction = [0.37567565 0.83909429 0.85083584 0.42157445]\n",
      "iteration 10500, loss = 0.33800485557572396, prediction = [0.37501548 0.83964163 0.85133294 0.42088774]\n",
      "iteration 10550, loss = 0.33714055940627413, prediction = [0.37435865 0.84018524 0.85182694 0.42020428]\n",
      "iteration 10600, loss = 0.33628179991276586, prediction = [0.37370516 0.84072513 0.85231784 0.41952404]\n",
      "iteration 10650, loss = 0.33542852654560823, prediction = [0.37305498 0.84126136 0.85280567 0.418847  ]\n",
      "iteration 10700, loss = 0.33458068927123824, prediction = [0.37240807 0.84179396 0.85329045 0.41817314]\n",
      "iteration 10750, loss = 0.3337382385685299, prediction = [0.37176442 0.84232296 0.85377222 0.41750244]\n",
      "iteration 10800, loss = 0.3329011254251103, prediction = [0.37112401 0.8428484  0.85425099 0.41683488]\n",
      "iteration 10850, loss = 0.332069301333593, prediction = [0.37048681 0.84337031 0.85472678 0.41617043]\n",
      "iteration 10900, loss = 0.33124271828773255, prediction = [0.3698528  0.84388873 0.85519962 0.41550909]\n",
      "iteration 10950, loss = 0.3304213287785107, prediction = [0.36922194 0.84440369 0.85566953 0.41485083]\n",
      "iteration 11000, loss = 0.32960508579015946, prediction = [0.36859424 0.84491522 0.85613653 0.41419562]\n",
      "iteration 11050, loss = 0.3287939427961274, prediction = [0.36796964 0.84542336 0.85660065 0.41354346]\n",
      "iteration 11100, loss = 0.3279878537549929, prediction = [0.36734815 0.84592814 0.85706191 0.41289431]\n",
      "iteration 11150, loss = 0.3271867731063356, prediction = [0.36672973 0.84642958 0.85752034 0.41224816]\n",
      "iteration 11200, loss = 0.32639065576656495, prediction = [0.36611435 0.84692773 0.85797594 0.411605  ]\n",
      "iteration 11250, loss = 0.3255994571247156, prediction = [0.36550201 0.84742261 0.85842875 0.41096479]\n",
      "iteration 11300, loss = 0.32481313303820947, prediction = [0.36489267 0.84791425 0.85887879 0.41032752]\n",
      "iteration 11350, loss = 0.3240316398285972, prediction = [0.36428632 0.84840269 0.85932608 0.40969318]\n",
      "iteration 11400, loss = 0.3232549342772721, prediction = [0.36368293 0.84888795 0.85977064 0.40906174]\n",
      "iteration 11450, loss = 0.32248297362117123, prediction = [0.36308248 0.84937007 0.8602125  0.40843319]\n",
      "iteration 11500, loss = 0.32171571554845824, prediction = [0.36248496 0.84984907 0.86065167 0.4078075 ]\n",
      "iteration 11550, loss = 0.3209531181941999, prediction = [0.36189033 0.85032499 0.86108817 0.40718466]\n",
      "iteration 11600, loss = 0.3201951401360337, prediction = [0.36129858 0.85079784 0.86152203 0.40656464]\n",
      "iteration 11650, loss = 0.319441740389829, prediction = [0.36070969 0.85126767 0.86195327 0.40594743]\n",
      "iteration 11700, loss = 0.31869287840535354, prediction = [0.36012363 0.85173449 0.86238191 0.40533302]\n",
      "iteration 11750, loss = 0.31794851406193414, prediction = [0.35954039 0.85219834 0.86280796 0.40472138]\n",
      "iteration 11800, loss = 0.3172086076641274, prediction = [0.35895995 0.85265924 0.86323146 0.40411249]\n",
      "iteration 11850, loss = 0.316473119937396, prediction = [0.35838228 0.85311722 0.86365241 0.40350634]\n",
      "iteration 11900, loss = 0.3157420120237899, prediction = [0.35780737 0.85357231 0.86407085 0.4029029 ]\n",
      "iteration 11950, loss = 0.31501524547764737, prediction = [0.3572352  0.85402453 0.86448678 0.40230217]\n",
      "iteration 12000, loss = 0.31429278226129986, prediction = [0.35666574 0.85447391 0.86490024 0.40170412]\n",
      "iteration 12050, loss = 0.3135745847407989, prediction = [0.35609898 0.85492048 0.86531123 0.40110873]\n",
      "iteration 12100, loss = 0.3128606156816559, prediction = [0.3555349  0.85536426 0.86571978 0.400516  ]\n",
      "iteration 12150, loss = 0.3121508382446034, prediction = [0.35497348 0.85580527 0.86612592 0.39992589]\n",
      "iteration 12200, loss = 0.3114452159813742, prediction = [0.35441469 0.85624355 0.86652964 0.3993384 ]\n",
      "iteration 12250, loss = 0.3107437128305032, prediction = [0.35385853 0.85667911 0.86693099 0.3987535 ]\n",
      "iteration 12300, loss = 0.3100462931131531, prediction = [0.35330497 0.85711199 0.86732997 0.39817118]\n",
      "iteration 12350, loss = 0.3093529215289605, prediction = [0.352754   0.85754219 0.86772661 0.39759143]\n",
      "iteration 12400, loss = 0.30866356315191235, prediction = [0.35220559 0.85796976 0.86812092 0.39701422]\n",
      "iteration 12450, loss = 0.30797818342624494, prediction = [0.35165973 0.85839471 0.86851292 0.39643954]\n",
      "iteration 12500, loss = 0.3072967481623726, prediction = [0.35111639 0.85881706 0.86890263 0.39586737]\n",
      "iteration 12550, loss = 0.3066192235328425, prediction = [0.35057557 0.85923685 0.86929006 0.39529769]\n",
      "iteration 12600, loss = 0.30594557606831985, prediction = [0.35003725 0.85965408 0.86967525 0.3947305 ]\n",
      "iteration 12650, loss = 0.3052757726536038, prediction = [0.3495014  0.86006879 0.87005819 0.39416577]\n",
      "iteration 12700, loss = 0.30460978052367144, prediction = [0.34896801 0.86048099 0.87043892 0.39360348]\n",
      "iteration 12750, loss = 0.303947567259757, prediction = [0.34843706 0.86089071 0.87081745 0.39304363]\n",
      "iteration 12800, loss = 0.30328910078545707, prediction = [0.34790853 0.86129798 0.87119379 0.3924862 ]\n",
      "iteration 12850, loss = 0.3026343493628745, prediction = [0.34738241 0.8617028  0.87156797 0.39193116]\n",
      "iteration 12900, loss = 0.3019832815887894, prediction = [0.34685869 0.86210521 0.87194    0.39137851]\n",
      "iteration 12950, loss = 0.3013358663908692, prediction = [0.34633733 0.86250522 0.87230989 0.39082822]\n",
      "iteration 13000, loss = 0.3006920730239053, prediction = [0.34581834 0.86290286 0.87267767 0.39028029]\n",
      "iteration 13050, loss = 0.3000518710660922, prediction = [0.34530169 0.86329814 0.87304335 0.3897347 ]\n",
      "iteration 13100, loss = 0.2994152304153318, prediction = [0.34478736 0.86369109 0.87340695 0.38919143]\n",
      "iteration 13150, loss = 0.2987821212855801, prediction = [0.34427534 0.86408172 0.87376848 0.38865047]\n",
      "iteration 13200, loss = 0.29815251420322364, prediction = [0.34376561 0.86447006 0.87412796 0.3881118 ]\n",
      "iteration 13250, loss = 0.2975263800034923, prediction = [0.34325816 0.86485613 0.87448541 0.38757541]\n",
      "iteration 13300, loss = 0.296903689826909, prediction = [0.34275297 0.86523994 0.87484084 0.38704129]\n",
      "iteration 13350, loss = 0.29628441511577064, prediction = [0.34225002 0.86562151 0.87519427 0.38650941]\n",
      "iteration 13400, loss = 0.295668527610669, prediction = [0.34174931 0.86600087 0.87554571 0.38597977]\n",
      "iteration 13450, loss = 0.29505599934704313, prediction = [0.34125081 0.86637803 0.87589518 0.38545234]\n",
      "iteration 13500, loss = 0.29444680265176915, prediction = [0.34075451 0.86675301 0.87624269 0.38492713]\n",
      "iteration 13550, loss = 0.29384091013978436, prediction = [0.34026039 0.86712584 0.87658827 0.3844041 ]\n",
      "iteration 13600, loss = 0.2932382947107486, prediction = [0.33976845 0.86749652 0.87693192 0.38388325]\n",
      "iteration 13650, loss = 0.29263892954573845, prediction = [0.33927865 0.86786507 0.87727365 0.38336457]\n",
      "iteration 13700, loss = 0.2920427881039781, prediction = [0.338791   0.86823152 0.8776135  0.38284803]\n",
      "iteration 13750, loss = 0.29144984411960656, prediction = [0.33830548 0.86859588 0.87795146 0.38233364]\n",
      "iteration 13800, loss = 0.290860071598477, prediction = [0.33782206 0.86895817 0.87828755 0.38182136]\n",
      "iteration 13850, loss = 0.2902734448149932, prediction = [0.33734074 0.86931841 0.8786218  0.38131119]\n",
      "iteration 13900, loss = 0.2896899383089812, prediction = [0.33686151 0.86967661 0.8789542  0.38080312]\n",
      "iteration 13950, loss = 0.28910952688259506, prediction = [0.33638434 0.87003279 0.87928479 0.38029713]\n",
      "iteration 14000, loss = 0.28853218559725635, prediction = [0.33590923 0.87038697 0.87961356 0.37979321]\n",
      "iteration 14050, loss = 0.28795788977062914, prediction = [0.33543615 0.87073916 0.87994054 0.37929135]\n",
      "iteration 14100, loss = 0.28738661497362994, prediction = [0.33496511 0.87108938 0.88026573 0.37879153]\n",
      "iteration 14150, loss = 0.28681833702746906, prediction = [0.33449608 0.87143764 0.88058916 0.37829374]\n",
      "iteration 14200, loss = 0.28625303200072816, prediction = [0.33402904 0.87178397 0.88091084 0.37779796]\n",
      "iteration 14250, loss = 0.2856906762064727, prediction = [0.333564   0.87212838 0.88123077 0.37730419]\n",
      "iteration 14300, loss = 0.2851312461993928, prediction = [0.33310093 0.87247088 0.88154897 0.37681242]\n",
      "iteration 14350, loss = 0.28457471877298185, prediction = [0.33263981 0.87281149 0.88186547 0.37632262]\n",
      "iteration 14400, loss = 0.2840210709567474, prediction = [0.33218065 0.87315023 0.88218025 0.37583479]\n",
      "iteration 14450, loss = 0.2834702800134528, prediction = [0.33172342 0.8734871  0.88249336 0.37534891]\n",
      "iteration 14500, loss = 0.28292232343639234, prediction = [0.33126811 0.87382213 0.88280478 0.37486498]\n",
      "iteration 14550, loss = 0.28237717894670045, prediction = [0.33081471 0.87415533 0.88311454 0.37438297]\n",
      "iteration 14600, loss = 0.28183482449068925, prediction = [0.3303632  0.87448672 0.88342266 0.37390289]\n",
      "iteration 14650, loss = 0.28129523823722025, prediction = [0.32991358 0.8748163  0.88372913 0.37342471]\n",
      "iteration 14700, loss = 0.280758398575108, prediction = [0.32946583 0.8751441  0.88403398 0.37294842]\n",
      "iteration 14750, loss = 0.28022428411055167, prediction = [0.32901994 0.87547012 0.88433722 0.37247402]\n",
      "iteration 14800, loss = 0.2796928736646011, prediction = [0.32857589 0.87579439 0.88463885 0.37200149]\n",
      "iteration 14850, loss = 0.27916414627065256, prediction = [0.32813368 0.87611692 0.8849389  0.37153082]\n",
      "iteration 14900, loss = 0.27863808117197153, prediction = [0.3276933  0.87643771 0.88523737 0.37106199]\n",
      "iteration 14950, loss = 0.2781146578192505, prediction = [0.32725472 0.87675679 0.88553427 0.370595  ]\n",
      "iteration 15000, loss = 0.2775938558681931, prediction = [0.32681794 0.87707416 0.88582962 0.37012984]\n",
      "iteration 15050, loss = 0.2770756551771281, prediction = [0.32638295 0.87738985 0.88612343 0.36966649]\n",
      "iteration 15100, loss = 0.27656003580465427, prediction = [0.32594974 0.87770386 0.88641571 0.36920495]\n",
      "iteration 15150, loss = 0.27604697800731154, prediction = [0.32551829 0.8780162  0.88670646 0.36874519]\n",
      "iteration 15200, loss = 0.27553646223728395, prediction = [0.32508859 0.8783269  0.88699572 0.36828722]\n",
      "iteration 15250, loss = 0.27502846914012613, prediction = [0.32466063 0.87863596 0.88728347 0.36783102]\n",
      "iteration 15300, loss = 0.2745229795525221, prediction = [0.32423441 0.8789434  0.88756974 0.36737657]\n",
      "iteration 15350, loss = 0.27401997450007, prediction = [0.3238099  0.87924922 0.88785453 0.36692388]\n",
      "iteration 15400, loss = 0.27351943519509436, prediction = [0.3233871  0.87955345 0.88813786 0.36647292]\n",
      "iteration 15450, loss = 0.27302134303448455, prediction = [0.322966   0.87985609 0.88841974 0.36602369]\n",
      "iteration 15500, loss = 0.27252567959756124, prediction = [0.32254659 0.88015715 0.88870018 0.36557618]\n",
      "iteration 15550, loss = 0.2720324266439686, prediction = [0.32212885 0.88045665 0.88897918 0.36513037]\n",
      "iteration 15600, loss = 0.2715415661115936, prediction = [0.32171278 0.8807546  0.88925677 0.36468626]\n",
      "iteration 15650, loss = 0.27105308011450857, prediction = [0.32129836 0.88105102 0.88953294 0.36424383]\n",
      "iteration 15700, loss = 0.2705669509409443, prediction = [0.32088559 0.8813459  0.88980771 0.36380308]\n",
      "iteration 15750, loss = 0.27008316105128144, prediction = [0.32047445 0.88163928 0.8900811  0.363364  ]\n",
      "iteration 15800, loss = 0.26960169307607373, prediction = [0.32006493 0.88193115 0.8903531  0.36292657]\n",
      "iteration 15850, loss = 0.26912252981409196, prediction = [0.31965703 0.88222152 0.89062373 0.36249079]\n",
      "iteration 15900, loss = 0.2686456542303928, prediction = [0.31925072 0.88251042 0.890893   0.36205664]\n",
      "iteration 15950, loss = 0.2681710494544133, prediction = [0.31884602 0.88279784 0.89116093 0.36162412]\n",
      "iteration 16000, loss = 0.2676986987780873, prediction = [0.31844289 0.88308381 0.89142751 0.36119321]\n",
      "iteration 16050, loss = 0.2672285856539864, prediction = [0.31804134 0.88336833 0.89169276 0.36076392]\n",
      "iteration 16100, loss = 0.2667606936934852, prediction = [0.31764135 0.88365142 0.89195669 0.36033621]\n",
      "iteration 16150, loss = 0.266295006664946, prediction = [0.31724292 0.88393308 0.89221931 0.3599101 ]\n",
      "iteration 16200, loss = 0.26583150849193143, prediction = [0.31684603 0.88421332 0.89248063 0.35948557]\n",
      "iteration 16250, loss = 0.2653701832514341, prediction = [0.31645067 0.88449216 0.89274066 0.3590626 ]\n",
      "iteration 16300, loss = 0.2649110151721338, prediction = [0.31605684 0.8847696  0.8929994  0.35864119]\n",
      "iteration 16350, loss = 0.26445398863267044, prediction = [0.31566452 0.88504567 0.89325686 0.35822134]\n",
      "iteration 16400, loss = 0.2639990881599445, prediction = [0.31527371 0.88532035 0.89351307 0.35780303]\n",
      "iteration 16450, loss = 0.2635462984274357, prediction = [0.3148844  0.88559368 0.89376801 0.35738624]\n",
      "iteration 16500, loss = 0.26309560425354184, prediction = [0.31449658 0.88586565 0.89402171 0.35697099]\n",
      "iteration 16550, loss = 0.26264699059994256, prediction = [0.31411023 0.88613628 0.89427417 0.35655724]\n",
      "iteration 16600, loss = 0.2622004425699781, prediction = [0.31372535 0.88640557 0.8945254  0.356145  ]\n",
      "iteration 16650, loss = 0.2617559454070539, prediction = [0.31334194 0.88667354 0.89477541 0.35573426]\n",
      "iteration 16700, loss = 0.2613134844930603, prediction = [0.31295997 0.8869402  0.89502421 0.35532501]\n",
      "iteration 16750, loss = 0.2608730453468169, prediction = [0.31257945 0.88720555 0.8952718  0.35491724]\n",
      "iteration 16800, loss = 0.2604346136225312, prediction = [0.31220037 0.88746961 0.8955182  0.35451093]\n",
      "iteration 16850, loss = 0.2599981751082808, prediction = [0.3118227  0.88773238 0.89576341 0.35410609]\n",
      "iteration 16900, loss = 0.259563715724513, prediction = [0.31144646 0.88799388 0.89600744 0.35370271]\n",
      "iteration 16950, loss = 0.2591312215225632, prediction = [0.31107162 0.88825411 0.8962503  0.35330077]\n",
      "iteration 17000, loss = 0.25870067868319196, prediction = [0.31069819 0.88851308 0.896492   0.35290026]\n",
      "iteration 17050, loss = 0.2582720735151391, prediction = [0.31032614 0.8887708  0.89673255 0.35250119]\n",
      "iteration 17100, loss = 0.25784539245369903, prediction = [0.30995548 0.88902729 0.89697195 0.35210354]\n",
      "iteration 17150, loss = 0.2574206220593114, prediction = [0.30958619 0.88928254 0.89721021 0.35170729]\n",
      "iteration 17200, loss = 0.2569977490161692, prediction = [0.30921827 0.88953657 0.89744734 0.35131246]\n",
      "iteration 17250, loss = 0.25657676013084657, prediction = [0.30885171 0.88978939 0.89768335 0.35091902]\n",
      "iteration 17300, loss = 0.25615764233094074, prediction = [0.3084865  0.890041   0.89791825 0.35052697]\n",
      "iteration 17350, loss = 0.25574038266373283, prediction = [0.30812263 0.89029142 0.89815203 0.3501363 ]\n",
      "iteration 17400, loss = 0.255324968294866, prediction = [0.30776009 0.89054064 0.89838472 0.349747  ]\n",
      "iteration 17450, loss = 0.2549113865070378, prediction = [0.30739888 0.89078869 0.89861632 0.34935907]\n",
      "iteration 17500, loss = 0.25449962469871124, prediction = [0.30703899 0.89103557 0.89884683 0.34897249]\n",
      "iteration 17550, loss = 0.2540896703828399, prediction = [0.30668041 0.89128129 0.89907627 0.34858727]\n",
      "iteration 17600, loss = 0.25368151118560917, prediction = [0.30632314 0.89152585 0.89930463 0.34820339]\n",
      "iteration 17650, loss = 0.25327513484519626, prediction = [0.30596716 0.89176926 0.89953194 0.34782084]\n",
      "iteration 17700, loss = 0.2528705292105411, prediction = [0.30561246 0.89201154 0.89975819 0.34743962]\n",
      "iteration 17750, loss = 0.2524676822401344, prediction = [0.30525905 0.89225268 0.89998339 0.34705972]\n",
      "iteration 17800, loss = 0.2520665820008222, prediction = [0.30490691 0.89249271 0.90020755 0.34668113]\n",
      "iteration 17850, loss = 0.2516672166666236, prediction = [0.30455604 0.89273161 0.90043067 0.34630384]\n",
      "iteration 17900, loss = 0.2512695745175629, prediction = [0.30420642 0.89296942 0.90065277 0.34592786]\n",
      "iteration 17950, loss = 0.25087364393851797, prediction = [0.30385806 0.89320612 0.90087385 0.34555316]\n",
      "iteration 18000, loss = 0.25047941341808033, prediction = [0.30351094 0.89344173 0.90109392 0.34517975]\n",
      "iteration 18050, loss = 0.25008687154743214, prediction = [0.30316506 0.89367625 0.90131298 0.34480761]\n",
      "iteration 18100, loss = 0.24969600701923506, prediction = [0.3028204  0.8939097  0.90153104 0.34443675]\n",
      "iteration 18150, loss = 0.24930680862653332, prediction = [0.30247697 0.89414208 0.90174811 0.34406714]\n",
      "iteration 18200, loss = 0.24891926526167063, prediction = [0.30213475 0.8943734  0.9019642  0.34369879]\n",
      "iteration 18250, loss = 0.24853336591522093, prediction = [0.30179374 0.89460366 0.9021793  0.34333169]\n",
      "iteration 18300, loss = 0.24814909967492987, prediction = [0.30145394 0.89483287 0.90239343 0.34296583]\n",
      "iteration 18350, loss = 0.24776645572467385, prediction = [0.30111532 0.89506105 0.9026066  0.34260121]\n",
      "iteration 18400, loss = 0.24738542334342617, prediction = [0.3007779  0.89528819 0.9028188  0.34223781]\n",
      "iteration 18450, loss = 0.247005991904241, prediction = [0.30044166 0.8955143  0.90303005 0.34187563]\n",
      "iteration 18500, loss = 0.24662815087324613, prediction = [0.30010659 0.8957394  0.90324036 0.34151467]\n",
      "iteration 18550, loss = 0.24625188980865165, prediction = [0.29977268 0.89596348 0.90344972 0.34115492]\n",
      "iteration 18600, loss = 0.24587719835976513, prediction = [0.29943994 0.89618656 0.90365815 0.34079636]\n",
      "iteration 18650, loss = 0.24550406626602694, prediction = [0.29910836 0.89640863 0.90386565 0.34043901]\n",
      "iteration 18700, loss = 0.24513248335604865, prediction = [0.29877792 0.89662972 0.90407222 0.34008284]\n",
      "iteration 18750, loss = 0.24476243954667048, prediction = [0.29844862 0.89684981 0.90427788 0.33972785]\n",
      "iteration 18800, loss = 0.24439392484202477, prediction = [0.29812046 0.89706893 0.90448263 0.33937404]\n",
      "iteration 18850, loss = 0.24402692933261433, prediction = [0.29779343 0.89728708 0.90468647 0.33902139]\n",
      "iteration 18900, loss = 0.24366144319440108, prediction = [0.29746752 0.89750426 0.90488942 0.33866991]\n",
      "iteration 18950, loss = 0.24329745668790503, prediction = [0.29714272 0.89772048 0.90509147 0.33831959]\n",
      "iteration 19000, loss = 0.24293496015731442, prediction = [0.29681904 0.89793575 0.90529264 0.33797042]\n",
      "iteration 19050, loss = 0.24257394402960844, prediction = [0.29649646 0.89815007 0.90549292 0.33762239]\n",
      "iteration 19100, loss = 0.24221439881368653, prediction = [0.29617498 0.89836345 0.90569233 0.3372755 ]\n",
      "iteration 19150, loss = 0.24185631509951339, prediction = [0.29585458 0.89857589 0.90589087 0.33692974]\n",
      "iteration 19200, loss = 0.2414996835572697, prediction = [0.29553528 0.89878741 0.90608854 0.33658511]\n",
      "iteration 19250, loss = 0.24114449493651713, prediction = [0.29521705 0.898998   0.90628535 0.3362416 ]\n",
      "iteration 19300, loss = 0.24079074006536894, prediction = [0.2948999  0.89920768 0.90648131 0.3358992 ]\n",
      "iteration 19350, loss = 0.24043840984967457, prediction = [0.29458382 0.89941644 0.90667643 0.33555791]\n",
      "iteration 19400, loss = 0.24008749527221268, prediction = [0.29426879 0.8996243  0.9068707  0.33521772]\n",
      "iteration 19450, loss = 0.23973798739189173, prediction = [0.29395483 0.89983127 0.90706413 0.33487863]\n",
      "iteration 19500, loss = 0.2393898773429653, prediction = [0.29364191 0.90003733 0.90725673 0.33454063]\n",
      "iteration 19550, loss = 0.23904315633424988, prediction = [0.29333004 0.90024252 0.90744851 0.33420372]\n",
      "iteration 19600, loss = 0.23869781564835924, prediction = [0.29301921 0.90044682 0.90763946 0.33386789]\n",
      "iteration 19650, loss = 0.23835384664094256, prediction = [0.29270941 0.90065024 0.9078296  0.33353312]\n",
      "iteration 19700, loss = 0.23801124073993202, prediction = [0.29240064 0.9008528  0.90801892 0.33319943]\n",
      "iteration 19750, loss = 0.23766998944480539, prediction = [0.29209289 0.90105449 0.90820745 0.3328668 ]\n",
      "iteration 19800, loss = 0.23733008432584843, prediction = [0.29178616 0.90125532 0.90839517 0.33253523]\n",
      "iteration 19850, loss = 0.23699151702343285, prediction = [0.29148043 0.9014553  0.90858209 0.33220471]\n",
      "iteration 19900, loss = 0.23665427924729956, prediction = [0.29117572 0.90165443 0.90876822 0.33187523]\n",
      "iteration 19950, loss = 0.23631836277585289, prediction = [0.290872   0.90185272 0.90895357 0.3315468 ]\n",
      "iteration 20000, loss = 0.2359837594554599, prediction = [0.29056928 0.90205017 0.90913814 0.3312194 ]\n",
      "iteration 20050, loss = 0.23565046119976016, prediction = [0.29026755 0.90224678 0.90932193 0.33089303]\n",
      "iteration 20100, loss = 0.23531845998898432, prediction = [0.28996681 0.90244258 0.90950495 0.33056769]\n",
      "iteration 20150, loss = 0.23498774786927873, prediction = [0.28966704 0.90263755 0.9096872  0.33024337]\n",
      "iteration 20200, loss = 0.23465831695203873, prediction = [0.28936824 0.9028317  0.90986869 0.32992006]\n",
      "iteration 20250, loss = 0.23433015941325153, prediction = [0.28907042 0.90302504 0.91004943 0.32959775]\n",
      "iteration 20300, loss = 0.23400326749284361, prediction = [0.28877356 0.90321758 0.91022941 0.32927646]\n",
      "iteration 20350, loss = 0.23367763349403797, prediction = [0.28847766 0.90340931 0.91040865 0.32895616]\n",
      "iteration 20400, loss = 0.233353249782719, prediction = [0.28818271 0.90360025 0.91058714 0.32863686]\n",
      "iteration 20450, loss = 0.23303010878680386, prediction = [0.28788871 0.9037904  0.91076489 0.32831854]\n",
      "iteration 20500, loss = 0.2327082029956223, prediction = [0.28759565 0.90397976 0.91094191 0.32800121]\n",
      "iteration 20550, loss = 0.23238752495930048, prediction = [0.28730353 0.90416834 0.9111182  0.32768486]\n",
      "iteration 20600, loss = 0.23206806728815782, prediction = [0.28701234 0.90435614 0.91129377 0.32736948]\n",
      "iteration 20650, loss = 0.2317498226521053, prediction = [0.28672209 0.90454318 0.91146861 0.32705506]\n",
      "iteration 20700, loss = 0.23143278378005366, prediction = [0.28643276 0.90472944 0.91164274 0.32674162]\n",
      "iteration 20750, loss = 0.2311169434593281, prediction = [0.28614434 0.90491494 0.91181616 0.32642913]\n",
      "iteration 20800, loss = 0.23080229453508877, prediction = [0.28585684 0.90509969 0.91198887 0.3261176 ]\n",
      "iteration 20850, loss = 0.23048882990975877, prediction = [0.28557025 0.90528368 0.91216088 0.32580701]\n",
      "iteration 20900, loss = 0.23017654254245978, prediction = [0.28528457 0.90546693 0.91233218 0.32549737]\n",
      "iteration 20950, loss = 0.22986542544845184, prediction = [0.28499978 0.90564943 0.9125028  0.32518868]\n",
      "iteration 21000, loss = 0.2295554716985811, prediction = [0.28471589 0.90583119 0.91267272 0.32488091]\n",
      "iteration 21050, loss = 0.22924667441873406, prediction = [0.28443289 0.90601221 0.91284196 0.32457408]\n",
      "iteration 21100, loss = 0.22893902678929676, prediction = [0.28415078 0.90619251 0.91301051 0.32426817]\n",
      "iteration 21150, loss = 0.2286325220446228, prediction = [0.28386955 0.90637208 0.91317839 0.32396319]\n",
      "iteration 21200, loss = 0.22832715347250415, prediction = [0.2835892  0.90655093 0.91334559 0.32365912]\n",
      "iteration 21250, loss = 0.22802291441364986, prediction = [0.28330972 0.90672906 0.91351212 0.32335596]\n",
      "iteration 21300, loss = 0.22771979826117145, prediction = [0.28303111 0.90690648 0.91367799 0.32305371]\n",
      "iteration 21350, loss = 0.22741779846007293, prediction = [0.28275336 0.90708319 0.91384319 0.32275237]\n",
      "iteration 21400, loss = 0.22711690850674537, prediction = [0.28247647 0.9072592  0.91400774 0.32245192]\n",
      "iteration 21450, loss = 0.2268171219484713, prediction = [0.28220043 0.90743451 0.91417163 0.32215237]\n",
      "iteration 21500, loss = 0.22651843238293062, prediction = [0.28192525 0.90760912 0.91433487 0.32185371]\n",
      "iteration 21550, loss = 0.2262208334577141, prediction = [0.28165091 0.90778304 0.91449746 0.32155593]\n",
      "iteration 21600, loss = 0.22592431886984154, prediction = [0.28137742 0.90795627 0.91465941 0.32125904]\n",
      "iteration 21650, loss = 0.22562888236528675, prediction = [0.28110476 0.90812882 0.91482073 0.32096302]\n",
      "iteration 21700, loss = 0.22533451773850688, prediction = [0.28083294 0.90830069 0.91498141 0.32066788]\n",
      "iteration 21750, loss = 0.22504121883197578, prediction = [0.28056195 0.90847189 0.91514145 0.32037361]\n",
      "iteration 21800, loss = 0.22474897953572742, prediction = [0.28029178 0.90864241 0.91530087 0.3200802 ]\n",
      "iteration 21850, loss = 0.22445779378689656, prediction = [0.28002243 0.90881227 0.91545967 0.31978765]\n",
      "iteration 21900, loss = 0.22416765556927212, prediction = [0.2797539  0.90898146 0.91561784 0.31949595]\n",
      "iteration 21950, loss = 0.22387855891285194, prediction = [0.27948619 0.90914999 0.9157754  0.31920511]\n",
      "iteration 22000, loss = 0.22359049789340302, prediction = [0.27921928 0.90931787 0.91593234 0.31891512]\n",
      "iteration 22050, loss = 0.22330346663202572, prediction = [0.27895318 0.90948509 0.91608868 0.31862597]\n",
      "iteration 22100, loss = 0.22301745929472594, prediction = [0.27868788 0.90965167 0.9162444  0.31833766]\n",
      "iteration 22150, loss = 0.22273247009198843, prediction = [0.27842337 0.90981761 0.91639953 0.31805018]\n",
      "iteration 22200, loss = 0.22244849327835658, prediction = [0.27815966 0.9099829  0.91655406 0.31776354]\n",
      "iteration 22250, loss = 0.22216552315201743, prediction = [0.27789674 0.91014756 0.91670799 0.31747773]\n",
      "iteration 22300, loss = 0.22188355405439, prediction = [0.27763461 0.91031158 0.91686133 0.31719273]\n",
      "iteration 22350, loss = 0.22160258036971908, prediction = [0.27737325 0.91047497 0.91701408 0.31690856]\n",
      "iteration 22400, loss = 0.2213225965246728, prediction = [0.27711267 0.91063774 0.91716624 0.31662521]\n",
      "iteration 22450, loss = 0.2210435969879461, prediction = [0.27685287 0.91079989 0.91731782 0.31634267]\n",
      "iteration 22500, loss = 0.22076557626986587, prediction = [0.27659384 0.91096141 0.91746883 0.31606093]\n",
      "iteration 22550, loss = 0.22048852892200377, prediction = [0.27633557 0.91112233 0.91761925 0.31578   ]\n",
      "iteration 22600, loss = 0.2202124495367905, prediction = [0.27607807 0.91128263 0.91776911 0.31549988]\n",
      "iteration 22650, loss = 0.21993733274713606, prediction = [0.27582132 0.91144232 0.9179184  0.31522054]\n",
      "iteration 22700, loss = 0.21966317322605328, prediction = [0.27556533 0.91160141 0.91806712 0.31494201]\n",
      "iteration 22750, loss = 0.21938996568628633, prediction = [0.27531009 0.9117599  0.91821528 0.31466426]\n",
      "iteration 22800, loss = 0.21911770487994173, prediction = [0.2750556  0.91191779 0.91836287 0.31438729]\n",
      "iteration 22850, loss = 0.21884638559812492, prediction = [0.27480186 0.91207508 0.91850992 0.31411111]\n",
      "iteration 22900, loss = 0.21857600267058136, prediction = [0.27454885 0.91223179 0.91865641 0.31383571]\n",
      "iteration 22950, loss = 0.21830655096533785, prediction = [0.27429659 0.91238791 0.91880235 0.31356108]\n",
      "iteration 23000, loss = 0.21803802538835365, prediction = [0.27404505 0.91254344 0.91894774 0.31328723]\n",
      "iteration 23050, loss = 0.217770420883169, prediction = [0.27379425 0.91269839 0.91909259 0.31301414]\n",
      "iteration 23100, loss = 0.21750373243056248, prediction = [0.27354417 0.91285277 0.9192369  0.31274182]\n",
      "iteration 23150, loss = 0.21723795504820914, prediction = [0.27329481 0.91300657 0.91938067 0.31247025]\n",
      "iteration 23200, loss = 0.21697308379034447, prediction = [0.27304618 0.9131598  0.9195239  0.31219945]\n",
      "iteration 23250, loss = 0.2167091137474299, prediction = [0.27279826 0.91331246 0.9196666  0.3119294 ]\n",
      "iteration 23300, loss = 0.21644604004582235, prediction = [0.27255106 0.91346455 0.91980878 0.31166009]\n",
      "iteration 23350, loss = 0.21618385784744987, prediction = [0.27230456 0.91361609 0.91995043 0.31139154]\n",
      "iteration 23400, loss = 0.21592256234948626, prediction = [0.27205877 0.91376707 0.92009155 0.31112373]\n",
      "iteration 23450, loss = 0.21566214878403345, prediction = [0.27181368 0.91391749 0.92023216 0.31085665]\n",
      "iteration 23500, loss = 0.21540261241780517, prediction = [0.2715693  0.91406736 0.92037225 0.31059032]\n",
      "iteration 23550, loss = 0.21514394855181312, prediction = [0.2713256  0.91421668 0.92051182 0.31032472]\n",
      "iteration 23600, loss = 0.21488615252105953, prediction = [0.27108261 0.91436545 0.92065088 0.31005984]\n",
      "iteration 23650, loss = 0.21462921969423, prediction = [0.2708403  0.91451368 0.92078944 0.3097957 ]\n",
      "iteration 23700, loss = 0.2143731454733909, prediction = [0.27059868 0.91466137 0.92092749 0.30953227]\n",
      "iteration 23750, loss = 0.21411792529369067, prediction = [0.27035774 0.91480852 0.92106503 0.30926957]\n",
      "iteration 23800, loss = 0.21386355462306175, prediction = [0.27011748 0.91495514 0.92120207 0.30900758]\n",
      "iteration 23850, loss = 0.2136100289619291, prediction = [0.2698779  0.91510123 0.92133862 0.30874631]\n",
      "iteration 23900, loss = 0.21335734384292007, prediction = [0.26963899 0.91524679 0.92147467 0.30848575]\n",
      "iteration 23950, loss = 0.2131054948305758, prediction = [0.26940075 0.91539182 0.92161023 0.30822589]\n",
      "iteration 24000, loss = 0.21285447752106884, prediction = [0.26916318 0.91553633 0.9217453  0.30796674]\n",
      "iteration 24050, loss = 0.2126042875419208, prediction = [0.26892628 0.91568032 0.92187988 0.30770829]\n",
      "iteration 24100, loss = 0.21235492055172653, prediction = [0.26869003 0.9158238  0.92201398 0.30745054]\n",
      "iteration 24150, loss = 0.2121063722398761, prediction = [0.26845445 0.91596675 0.9221476  0.30719348]\n",
      "iteration 24200, loss = 0.21185863832628504, prediction = [0.26821952 0.9161092  0.92228073 0.30693711]\n",
      "iteration 24250, loss = 0.2116117145611235, prediction = [0.26798524 0.91625114 0.92241339 0.30668143]\n",
      "iteration 24300, loss = 0.2113655967245499, prediction = [0.26775161 0.91639258 0.92254558 0.30642644]\n",
      "iteration 24350, loss = 0.21112028062644805, prediction = [0.26751863 0.91653351 0.92267729 0.30617213]\n",
      "iteration 24400, loss = 0.21087576210616488, prediction = [0.26728629 0.91667394 0.92280854 0.30591849]\n",
      "iteration 24450, loss = 0.21063203703225258, prediction = [0.26705459 0.91681387 0.92293932 0.30566554]\n",
      "iteration 24500, loss = 0.21038910130221344, prediction = [0.26682353 0.9169533  0.92306963 0.30541325]\n",
      "iteration 24550, loss = 0.21014695084224683, prediction = [0.2665931  0.91709225 0.92319949 0.30516164]\n",
      "iteration 24600, loss = 0.20990558160699735, prediction = [0.26636331 0.9172307  0.92332888 0.30491069]\n",
      "iteration 24650, loss = 0.20966498957931018, prediction = [0.26613414 0.91736867 0.92345782 0.30466041]\n",
      "iteration 24700, loss = 0.20942517076998207, prediction = [0.2659056  0.91750615 0.9235863  0.30441079]\n",
      "iteration 24750, loss = 0.20918612121752128, prediction = [0.26567768 0.91764315 0.92371434 0.30416183]\n",
      "iteration 24800, loss = 0.20894783698790695, prediction = [0.26545039 0.91777967 0.92384192 0.30391352]\n",
      "iteration 24850, loss = 0.2087103141743505, prediction = [0.26522371 0.91791572 0.92396905 0.30366587]\n",
      "iteration 24900, loss = 0.2084735488970615, prediction = [0.26499764 0.91805129 0.92409574 0.30341887]\n",
      "iteration 24950, loss = 0.2082375373030142, prediction = [0.26477219 0.91818638 0.92422199 0.30317251]\n",
      "iteration 25000, loss = 0.2080022755657167, prediction = [0.26454735 0.91832101 0.9243478  0.3029268 ]\n",
      "iteration 25050, loss = 0.20776775988498453, prediction = [0.26432312 0.91845517 0.92447317 0.30268172]\n",
      "iteration 25100, loss = 0.20753398648671215, prediction = [0.26409948 0.91858887 0.92459811 0.30243729]\n",
      "iteration 25150, loss = 0.2073009516226519, prediction = [0.26387645 0.9187221  0.92472261 0.3021935 ]\n",
      "iteration 25200, loss = 0.20706865157019197, prediction = [0.26365402 0.91885487 0.92484668 0.30195033]\n",
      "iteration 25250, loss = 0.20683708263213596, prediction = [0.26343218 0.91898719 0.92497032 0.3017078 ]\n",
      "iteration 25300, loss = 0.20660624113648923, prediction = [0.26321094 0.91911905 0.92509354 0.30146589]\n",
      "iteration 25350, loss = 0.20637612343624184, prediction = [0.26299029 0.91925046 0.92521633 0.30122461]\n",
      "iteration 25400, loss = 0.206146725909157, prediction = [0.26277023 0.91938142 0.9253387  0.30098396]\n",
      "iteration 25450, loss = 0.20591804495756094, prediction = [0.26255075 0.91951193 0.92546065 0.30074392]\n",
      "iteration 25500, loss = 0.20569007700813458, prediction = [0.26233185 0.919642   0.92558218 0.3005045 ]\n",
      "iteration 25550, loss = 0.2054628185117077, prediction = [0.26211353 0.91977162 0.92570329 0.30026569]\n",
      "iteration 25600, loss = 0.2052362659430556, prediction = [0.2618958 0.9199008 0.925824  0.3000275]\n",
      "iteration 25650, loss = 0.20501041580069518, prediction = [0.26167863 0.92002954 0.92594429 0.29978991]\n",
      "iteration 25700, loss = 0.20478526460668744, prediction = [0.26146204 0.92015785 0.92606417 0.29955293]\n",
      "iteration 25750, loss = 0.20456080890643813, prediction = [0.26124602 0.92028572 0.92618364 0.29931656]\n",
      "iteration 25800, loss = 0.2043370452685028, prediction = [0.26103057 0.92041316 0.92630271 0.29908078]\n",
      "iteration 25850, loss = 0.2041139702843926, prediction = [0.26081568 0.92054017 0.92642138 0.29884561]\n",
      "iteration 25900, loss = 0.203891580568381, prediction = [0.26060136 0.92066675 0.92653965 0.29861103]\n",
      "iteration 25950, loss = 0.20366987275731585, prediction = [0.2603876  0.92079291 0.92665751 0.29837704]\n",
      "iteration 26000, loss = 0.2034488435104297, prediction = [0.26017439 0.92091864 0.92677498 0.29814365]\n",
      "iteration 26050, loss = 0.20322848950915337, prediction = [0.25996174 0.92104395 0.92689206 0.29791085]\n",
      "iteration 26100, loss = 0.20300880745693367, prediction = [0.25974964 0.92116885 0.92700874 0.29767863]\n",
      "iteration 26150, loss = 0.20278979407904696, prediction = [0.2595381  0.92129332 0.92712503 0.29744699]\n",
      "iteration 26200, loss = 0.20257144612242217, prediction = [0.2593271  0.92141738 0.92724093 0.29721594]\n",
      "iteration 26250, loss = 0.20235376035545866, prediction = [0.25911664 0.92154103 0.92735645 0.29698547]\n",
      "iteration 26300, loss = 0.20213673356785072, prediction = [0.25890674 0.92166427 0.92747158 0.29675557]\n",
      "iteration 26350, loss = 0.2019203625704116, prediction = [0.25869737 0.9217871  0.92758633 0.29652625]\n",
      "iteration 26400, loss = 0.20170464419489936, prediction = [0.25848854 0.92190952 0.92770069 0.2962975 ]\n",
      "iteration 26450, loss = 0.20148957529384454, prediction = [0.25828025 0.92203154 0.92781468 0.29606932]\n",
      "iteration 26500, loss = 0.20127515274038082, prediction = [0.25807249 0.92215316 0.92792829 0.2958417 ]\n",
      "iteration 26550, loss = 0.20106137342807476, prediction = [0.25786527 0.92227437 0.92804152 0.29561465]\n",
      "iteration 26600, loss = 0.20084823427075954, prediction = [0.25765857 0.92239519 0.92815438 0.29538817]\n",
      "iteration 26650, loss = 0.20063573220237, prediction = [0.2574524  0.92251561 0.92826687 0.29516224]\n",
      "iteration 26700, loss = 0.20042386417677713, prediction = [0.25724676 0.92263563 0.92837899 0.29493687]\n",
      "iteration 26750, loss = 0.20021262716762672, prediction = [0.25704164 0.92275527 0.92849074 0.29471206]\n",
      "iteration 26800, loss = 0.20000201816817867, prediction = [0.25683704 0.92287451 0.92860212 0.2944878 ]\n",
      "iteration 26850, loss = 0.19979203419114805, prediction = [0.25663296 0.92299337 0.92871314 0.29426409]\n",
      "iteration 26900, loss = 0.1995826722685469, prediction = [0.2564294  0.92311183 0.9288238  0.29404093]\n",
      "iteration 26950, loss = 0.19937392945152793, prediction = [0.25622635 0.92322992 0.9289341  0.29381831]\n",
      "iteration 27000, loss = 0.19916580281023039, prediction = [0.25602382 0.92334762 0.92904403 0.29359624]\n",
      "iteration 27050, loss = 0.19895828943362745, prediction = [0.25582179 0.92346493 0.92915361 0.29337472]\n",
      "iteration 27100, loss = 0.19875138642937337, prediction = [0.25562027 0.92358187 0.92926284 0.29315373]\n",
      "iteration 27150, loss = 0.1985450909236535, prediction = [0.25541926 0.92369844 0.92937171 0.29293328]\n",
      "iteration 27200, loss = 0.19833940006103734, prediction = [0.25521875 0.92381462 0.92948023 0.29271336]\n",
      "iteration 27250, loss = 0.19813431100432824, prediction = [0.25501875 0.92393043 0.92958839 0.29249398]\n",
      "iteration 27300, loss = 0.19792982093441946, prediction = [0.25481924 0.92404588 0.92969621 0.29227513]\n",
      "iteration 27350, loss = 0.19772592705014924, prediction = [0.25462023 0.92416095 0.92980368 0.29205681]\n",
      "iteration 27400, loss = 0.19752262656815867, prediction = [0.25442172 0.92427565 0.92991081 0.29183902]\n",
      "iteration 27450, loss = 0.19731991672274662, prediction = [0.2542237  0.92438999 0.93001759 0.29162175]\n",
      "iteration 27500, loss = 0.19711779476573338, prediction = [0.25402617 0.92450396 0.93012403 0.291405  ]\n",
      "iteration 27550, loss = 0.19691625796631906, prediction = [0.25382913 0.92461757 0.93023014 0.29118878]\n",
      "iteration 27600, loss = 0.19671530361094575, prediction = [0.25363257 0.92473081 0.9303359  0.29097307]\n",
      "iteration 27650, loss = 0.19651492900316275, prediction = [0.25343651 0.9248437  0.93044132 0.29075788]\n",
      "iteration 27700, loss = 0.19631513146349, prediction = [0.25324092 0.92495623 0.93054641 0.2905432 ]\n",
      "iteration 27750, loss = 0.19611590832928397, prediction = [0.25304582 0.92506841 0.93065117 0.29032904]\n",
      "iteration 27800, loss = 0.1959172569546056, prediction = [0.2528512  0.92518023 0.93075559 0.29011538]\n",
      "iteration 27850, loss = 0.19571917471008982, prediction = [0.25265705 0.92529169 0.93085968 0.28990224]\n",
      "iteration 27900, loss = 0.19552165898281282, prediction = [0.25246338 0.92540281 0.93096344 0.2896896 ]\n",
      "iteration 27950, loss = 0.19532470717616593, prediction = [0.25227018 0.92551358 0.93106688 0.28947746]\n",
      "iteration 28000, loss = 0.1951283167097253, prediction = [0.25207746 0.925624   0.93116999 0.28926583]\n",
      "iteration 28050, loss = 0.19493248501912808, prediction = [0.2518852  0.92573407 0.93127277 0.28905469]\n",
      "iteration 28100, loss = 0.1947372095559437, prediction = [0.25169342 0.9258438  0.93137524 0.28884406]\n",
      "iteration 28150, loss = 0.19454248778755337, prediction = [0.2515021  0.92595319 0.93147738 0.28863392]\n",
      "iteration 28200, loss = 0.19434831719702383, prediction = [0.25131124 0.92606223 0.9315792  0.28842428]\n",
      "iteration 28250, loss = 0.19415469528298712, prediction = [0.25112084 0.92617094 0.9316807  0.28821513]\n",
      "iteration 28300, loss = 0.19396161955952004, prediction = [0.25093091 0.92627931 0.93178189 0.28800646]\n",
      "iteration 28350, loss = 0.19376908755602373, prediction = [0.25074143 0.92638734 0.93188276 0.28779829]\n",
      "iteration 28400, loss = 0.19357709681710478, prediction = [0.25055241 0.92649504 0.93198332 0.28759061]\n",
      "iteration 28450, loss = 0.1933856449024584, prediction = [0.25036385 0.9266024  0.93208356 0.2873834 ]\n",
      "iteration 28500, loss = 0.19319472938675256, prediction = [0.25017573 0.92670944 0.93218349 0.28717669]\n",
      "iteration 28550, loss = 0.1930043478595116, prediction = [0.24998807 0.92681614 0.93228312 0.28697045]\n",
      "iteration 28600, loss = 0.19281449792500283, prediction = [0.24980086 0.92692251 0.93238243 0.28676469]\n",
      "iteration 28650, loss = 0.1926251772021229, prediction = [0.2496141  0.92702856 0.93248144 0.28655941]\n",
      "iteration 28700, loss = 0.1924363833242856, prediction = [0.24942778 0.92713429 0.93258015 0.2863546 ]\n",
      "iteration 28750, loss = 0.19224811393931082, prediction = [0.24924191 0.92723968 0.93267855 0.28615027]\n",
      "iteration 28800, loss = 0.19206036670931487, prediction = [0.24905647 0.92734476 0.93277665 0.28594641]\n",
      "iteration 28850, loss = 0.19187313931060146, prediction = [0.24887148 0.92744952 0.93287444 0.28574302]\n",
      "iteration 28900, loss = 0.19168642943355238, prediction = [0.24868693 0.92755395 0.93297194 0.2855401 ]\n",
      "iteration 28950, loss = 0.1915002347825209, prediction = [0.24850281 0.92765807 0.93306914 0.28533764]\n",
      "iteration 29000, loss = 0.1913145530757277, prediction = [0.24831913 0.92776187 0.93316604 0.28513565]\n",
      "iteration 29050, loss = 0.1911293820451519, prediction = [0.24813589 0.92786536 0.93326265 0.28493412]\n",
      "iteration 29100, loss = 0.19094471943643027, prediction = [0.24795307 0.92796853 0.93335896 0.28473306]\n",
      "iteration 29150, loss = 0.19076056300875174, prediction = [0.24777069 0.92807139 0.93345498 0.28453245]\n",
      "iteration 29200, loss = 0.19057691053475617, prediction = [0.24758873 0.92817394 0.93355071 0.2843323 ]\n",
      "iteration 29250, loss = 0.19039375980043238, prediction = [0.2474072  0.92827619 0.93364615 0.2841326 ]\n",
      "iteration 29300, loss = 0.1902111086050182, prediction = [0.2472261  0.92837812 0.9337413  0.28393336]\n",
      "iteration 29350, loss = 0.19002895476090023, prediction = [0.24704542 0.92847975 0.93383617 0.28373457]\n",
      "iteration 29400, loss = 0.18984729609351647, prediction = [0.24686516 0.92858107 0.93393074 0.28353623]\n",
      "iteration 29450, loss = 0.18966613044125585, prediction = [0.24668532 0.92868209 0.93402504 0.28333835]\n",
      "iteration 29500, loss = 0.18948545565536434, prediction = [0.2465059  0.9287828  0.93411905 0.2831409 ]\n",
      "iteration 29550, loss = 0.18930526959984628, prediction = [0.2463269  0.92888322 0.93421277 0.28294391]\n",
      "iteration 29600, loss = 0.18912557015137132, prediction = [0.24614831 0.92898334 0.93430622 0.28274735]\n",
      "iteration 29650, loss = 0.1889463551991784, prediction = [0.24597014 0.92908315 0.93439939 0.28255124]\n",
      "iteration 29700, loss = 0.18876762264498362, prediction = [0.24579238 0.92918268 0.93449227 0.28235557]\n",
      "iteration 29750, loss = 0.1885893704028867, prediction = [0.24561503 0.9292819  0.93458488 0.28216034]\n",
      "iteration 29800, loss = 0.1884115963992784, prediction = [0.24543809 0.92938083 0.93467722 0.28196555]\n",
      "iteration 29850, loss = 0.1882342985727512, prediction = [0.24526155 0.92947947 0.93476928 0.28177119]\n",
      "iteration 29900, loss = 0.18805747487400817, prediction = [0.24508542 0.92957782 0.93486107 0.28157726]\n",
      "iteration 29950, loss = 0.18788112326577291, prediction = [0.2449097  0.92967588 0.93495258 0.28138377]\n",
      "iteration 30000, loss = 0.18770524172270275, prediction = [0.24473438 0.92977364 0.93504383 0.28119071]\n",
      "iteration 30050, loss = 0.18752982823129827, prediction = [0.24455946 0.92987113 0.9351348  0.28099808]\n",
      "iteration 30100, loss = 0.18735488078981827, prediction = [0.24438494 0.92996832 0.93522551 0.28080587]\n",
      "iteration 30150, loss = 0.1871803974081926, prediction = [0.24421082 0.93006523 0.93531594 0.28061409]\n",
      "iteration 30200, loss = 0.18700637610793744, prediction = [0.24403709 0.93016185 0.93540612 0.28042273]\n",
      "iteration 30250, loss = 0.18683281492206952, prediction = [0.24386376 0.9302582  0.93549602 0.2802318 ]\n",
      "iteration 30300, loss = 0.18665971189502117, prediction = [0.24369083 0.93035426 0.93558567 0.28004129]\n",
      "iteration 30350, loss = 0.18648706508255977, prediction = [0.24351828 0.93045004 0.93567505 0.2798512 ]\n",
      "iteration 30400, loss = 0.18631487255170254, prediction = [0.24334613 0.93054554 0.93576417 0.27966153]\n",
      "iteration 30450, loss = 0.1861431323806364, prediction = [0.24317436 0.93064077 0.93585303 0.27947227]\n",
      "iteration 30500, loss = 0.18597184265863512, prediction = [0.24300299 0.93073572 0.93594163 0.27928343]\n",
      "iteration 30550, loss = 0.18580100148597994, prediction = [0.242832   0.93083039 0.93602997 0.279095  ]\n",
      "iteration 30600, loss = 0.1856306069738804, prediction = [0.24266139 0.93092479 0.93611806 0.27890698]\n",
      "iteration 30650, loss = 0.18546065724439403, prediction = [0.24249117 0.93101892 0.93620589 0.27871938]\n",
      "iteration 30700, loss = 0.18529115043034788, prediction = [0.24232133 0.93111278 0.93629346 0.27853218]\n",
      "iteration 30750, loss = 0.18512208467526226, prediction = [0.24215187 0.93120636 0.93638078 0.27834539]\n",
      "iteration 30800, loss = 0.18495345813327213, prediction = [0.24198279 0.93129968 0.93646785 0.278159  ]\n",
      "iteration 30850, loss = 0.18478526896905217, prediction = [0.24181409 0.93139273 0.93655467 0.27797303]\n",
      "iteration 30900, loss = 0.1846175153577393, prediction = [0.24164576 0.93148551 0.93664124 0.27778745]\n",
      "iteration 30950, loss = 0.18445019548485903, prediction = [0.24147781 0.93157803 0.93672756 0.27760227]\n",
      "iteration 31000, loss = 0.18428330754625089, prediction = [0.24131023 0.93167028 0.93681363 0.2774175 ]\n",
      "iteration 31050, loss = 0.1841168497479936, prediction = [0.24114303 0.93176227 0.93689945 0.27723312]\n",
      "iteration 31100, loss = 0.18395082030633397, prediction = [0.24097619 0.93185399 0.93698503 0.27704915]\n",
      "iteration 31150, loss = 0.18378521744761198, prediction = [0.24080973 0.93194546 0.93707036 0.27686556]\n",
      "iteration 31200, loss = 0.18362003940819044, prediction = [0.24064363 0.93203666 0.93715545 0.27668238]\n",
      "iteration 31250, loss = 0.18345528443438308, prediction = [0.2404779  0.93212761 0.9372403  0.27649958]\n",
      "iteration 31300, loss = 0.18329095078238455, prediction = [0.24031253 0.9322183  0.9373249  0.27631718]\n",
      "iteration 31350, loss = 0.18312703671819908, prediction = [0.24014753 0.93230873 0.93740927 0.27613516]\n",
      "iteration 31400, loss = 0.18296354051757252, prediction = [0.23998289 0.93239891 0.93749339 0.27595354]\n",
      "iteration 31450, loss = 0.18280046046592224, prediction = [0.23981861 0.93248883 0.93757728 0.2757723 ]\n",
      "iteration 31500, loss = 0.18263779485826923, prediction = [0.2396547  0.9325785  0.93766093 0.27559145]\n",
      "iteration 31550, loss = 0.18247554199917104, prediction = [0.23949114 0.93266792 0.93774434 0.27541098]\n",
      "iteration 31600, loss = 0.1823137002026536, prediction = [0.23932794 0.93275708 0.93782752 0.2752309 ]\n",
      "iteration 31650, loss = 0.18215226779214505, prediction = [0.23916509 0.932846   0.93791046 0.2750512 ]\n",
      "iteration 31700, loss = 0.18199124310040976, prediction = [0.2390026  0.93293467 0.93799317 0.27487187]\n",
      "iteration 31750, loss = 0.18183062446948273, prediction = [0.23884046 0.93302309 0.93807564 0.27469293]\n",
      "iteration 31800, loss = 0.18167041025060504, prediction = [0.23867868 0.93311126 0.93815789 0.27451437]\n",
      "iteration 31850, loss = 0.18151059880415843, prediction = [0.23851724 0.93319919 0.9382399  0.27433618]\n",
      "iteration 31900, loss = 0.18135118849960324, prediction = [0.23835616 0.93328687 0.93832169 0.27415837]\n",
      "iteration 31950, loss = 0.18119217771541338, prediction = [0.23819542 0.93337431 0.93840325 0.27398093]\n",
      "iteration 32000, loss = 0.18103356483901492, prediction = [0.23803503 0.9334615  0.93848457 0.27380386]\n",
      "iteration 32050, loss = 0.18087534826672264, prediction = [0.23787499 0.93354845 0.93856568 0.27362717]\n",
      "iteration 32100, loss = 0.18071752640367886, prediction = [0.23771529 0.93363517 0.93864655 0.27345084]\n",
      "iteration 32150, loss = 0.18056009766379255, prediction = [0.23755594 0.93372164 0.93872721 0.27327489]\n",
      "iteration 32200, loss = 0.18040306046967822, prediction = [0.23739693 0.93380788 0.93880763 0.2730993 ]\n",
      "iteration 32250, loss = 0.18024641325259605, prediction = [0.23723825 0.93389387 0.93888784 0.27292407]\n",
      "iteration 32300, loss = 0.18009015445239152, prediction = [0.23707992 0.93397963 0.93896782 0.27274922]\n",
      "iteration 32350, loss = 0.17993428251743665, prediction = [0.23692193 0.93406516 0.93904759 0.27257472]\n",
      "iteration 32400, loss = 0.17977879590457158, prediction = [0.23676427 0.93415045 0.93912713 0.27240059]\n",
      "iteration 32450, loss = 0.179623693079046, prediction = [0.23660695 0.9342355  0.93920645 0.27222682]\n",
      "iteration 32500, loss = 0.1794689725144612, prediction = [0.23644996 0.93432033 0.93928556 0.2720534 ]\n",
      "iteration 32550, loss = 0.17931463269271344, prediction = [0.23629331 0.93440492 0.93936444 0.27188035]\n",
      "iteration 32600, loss = 0.17916067210393644, prediction = [0.23613699 0.93448928 0.93944312 0.27170765]\n",
      "iteration 32650, loss = 0.1790070892464449, prediction = [0.23598101 0.93457341 0.93952157 0.27153531]\n",
      "iteration 32700, loss = 0.17885388262668028, prediction = [0.23582535 0.93465731 0.93959981 0.27136333]\n",
      "iteration 32750, loss = 0.17870105075915324, prediction = [0.23567002 0.93474099 0.93967784 0.2711917 ]\n",
      "iteration 32800, loss = 0.17854859216639013, prediction = [0.23551502 0.93482443 0.93975566 0.27102042]\n",
      "iteration 32850, loss = 0.17839650537887716, prediction = [0.23536034 0.93490766 0.93983326 0.27084949]\n",
      "iteration 32900, loss = 0.1782447889350089, prediction = [0.235206   0.93499065 0.93991065 0.27067891]\n",
      "iteration 32950, loss = 0.17809344138103092, prediction = [0.23505197 0.93507342 0.93998783 0.27050868]\n",
      "iteration 33000, loss = 0.17794246127098984, prediction = [0.23489827 0.93515597 0.9400648  0.2703388 ]\n",
      "iteration 33050, loss = 0.17779184716667878, prediction = [0.23474489 0.9352383  0.94014157 0.27016926]\n",
      "iteration 33100, loss = 0.1776415976375859, prediction = [0.23459183 0.9353204  0.94021812 0.27000007]\n",
      "iteration 33150, loss = 0.17749171126084196, prediction = [0.2344391  0.93540229 0.94029447 0.26983122]\n",
      "iteration 33200, loss = 0.17734218662116907, prediction = [0.23428668 0.93548395 0.94037062 0.26966272]\n",
      "iteration 33250, loss = 0.17719302231082937, prediction = [0.23413457 0.9355654  0.94044656 0.26949455]\n",
      "iteration 33300, loss = 0.17704421692957592, prediction = [0.23398279 0.93564662 0.94052229 0.26932673]\n",
      "iteration 33350, loss = 0.17689576908459922, prediction = [0.23383132 0.93572764 0.94059782 0.26915925]\n",
      "iteration 33400, loss = 0.1767476773904805, prediction = [0.23368016 0.93580843 0.94067315 0.2689921 ]\n",
      "iteration 33450, loss = 0.17659994046914107, prediction = [0.23352932 0.93588901 0.94074828 0.2688253 ]\n",
      "iteration 33500, loss = 0.1764525569497929, prediction = [0.23337879 0.93596938 0.9408232  0.26865882]\n",
      "iteration 33550, loss = 0.17630552546889083, prediction = [0.23322857 0.93604953 0.94089793 0.26849269]\n",
      "iteration 33600, loss = 0.1761588446700832, prediction = [0.23307866 0.93612947 0.94097245 0.26832688]\n",
      "iteration 33650, loss = 0.17601251320416517, prediction = [0.23292907 0.93620919 0.94104678 0.26816141]\n",
      "iteration 33700, loss = 0.17586652972903055, prediction = [0.23277977 0.93628871 0.94112091 0.26799627]\n",
      "iteration 33750, loss = 0.17572089290962467, prediction = [0.23263079 0.93636802 0.94119484 0.26783146]\n",
      "iteration 33800, loss = 0.17557560141789813, prediction = [0.23248211 0.93644711 0.94126858 0.26766698]\n",
      "iteration 33850, loss = 0.17543065393276017, prediction = [0.23233373 0.936526   0.94134212 0.26750283]\n",
      "iteration 33900, loss = 0.175286049140032, prediction = [0.23218566 0.93660468 0.94141547 0.267339  ]\n",
      "iteration 33950, loss = 0.17514178573240258, prediction = [0.23203789 0.93668316 0.94148862 0.2671755 ]\n",
      "iteration 34000, loss = 0.17499786240938195, prediction = [0.23189043 0.93676143 0.94156158 0.26701233]\n",
      "iteration 34050, loss = 0.17485427787725732, prediction = [0.23174326 0.93683949 0.94163435 0.26684948]\n",
      "iteration 34100, loss = 0.17471103084904804, prediction = [0.2315964  0.93691735 0.94170692 0.26668695]\n",
      "iteration 34150, loss = 0.17456812004446182, prediction = [0.23144983 0.936995   0.94177931 0.26652474]\n",
      "iteration 34200, loss = 0.17442554418984987, prediction = [0.23130356 0.93707246 0.94185151 0.26636285]\n",
      "iteration 34250, loss = 0.17428330201816478, prediction = [0.23115758 0.93714971 0.94192351 0.26620129]\n",
      "iteration 34300, loss = 0.17414139226891603, prediction = [0.2310119  0.93722676 0.94199533 0.26604004]\n",
      "iteration 34350, loss = 0.17399981368812842, prediction = [0.23086652 0.93730361 0.94206696 0.2658791 ]\n",
      "iteration 34400, loss = 0.17385856502829858, prediction = [0.23072143 0.93738026 0.94213841 0.26571849]\n",
      "iteration 34450, loss = 0.17371764504835302, prediction = [0.23057663 0.93745671 0.94220966 0.26555819]\n",
      "iteration 34500, loss = 0.17357705251360753, prediction = [0.23043213 0.93753296 0.94228073 0.2653982 ]\n",
      "iteration 34550, loss = 0.1734367861957231, prediction = [0.23028791 0.93760902 0.94235162 0.26523853]\n",
      "iteration 34600, loss = 0.17329684487266767, prediction = [0.23014398 0.93768488 0.94242232 0.26507917]\n",
      "iteration 34650, loss = 0.1731572273286728, prediction = [0.23000035 0.93776055 0.94249284 0.26492012]\n",
      "iteration 34700, loss = 0.17301793235419533, prediction = [0.229857   0.93783602 0.94256318 0.26476138]\n",
      "iteration 34750, loss = 0.172878958745875, prediction = [0.22971393 0.93791129 0.94263334 0.26460295]\n",
      "iteration 34800, loss = 0.1727403053064962, prediction = [0.22957115 0.93798638 0.94270331 0.26444482]\n",
      "iteration 34850, loss = 0.17260197084494766, prediction = [0.22942866 0.93806127 0.94277311 0.26428701]\n",
      "iteration 34900, loss = 0.17246395417618332, prediction = [0.22928645 0.93813597 0.94284272 0.2641295 ]\n",
      "iteration 34950, loss = 0.17232625412118335, prediction = [0.22914453 0.93821047 0.94291216 0.26397229]\n",
      "iteration 35000, loss = 0.17218886950691478, prediction = [0.22900288 0.93828479 0.94298141 0.26381539]\n",
      "iteration 35050, loss = 0.17205179916629493, prediction = [0.22886152 0.93835892 0.94305049 0.26365879]\n",
      "iteration 35100, loss = 0.17191504193815127, prediction = [0.22872044 0.93843286 0.94311939 0.2635025 ]\n",
      "iteration 35150, loss = 0.1717785966671846, prediction = [0.22857963 0.93850661 0.94318812 0.2633465 ]\n",
      "iteration 35200, loss = 0.17164246220393192, prediction = [0.22843911 0.93858017 0.94325667 0.26319081]\n",
      "iteration 35250, loss = 0.17150663740472827, prediction = [0.22829886 0.93865355 0.94332505 0.26303541]\n",
      "iteration 35300, loss = 0.17137112113167002, prediction = [0.22815888 0.93872674 0.94339325 0.26288032]\n",
      "iteration 35350, loss = 0.17123591225257945, prediction = [0.22801919 0.93879975 0.94346128 0.26272552]\n",
      "iteration 35400, loss = 0.17110100964096622, prediction = [0.22787977 0.93887257 0.94352913 0.26257101]\n",
      "iteration 35450, loss = 0.17096641217599345, prediction = [0.22774062 0.9389452  0.94359681 0.2624168 ]\n",
      "iteration 35500, loss = 0.17083211874244003, prediction = [0.22760174 0.93901766 0.94366433 0.26226289]\n",
      "iteration 35550, loss = 0.17069812823066652, prediction = [0.22746314 0.93908993 0.94373167 0.26210927]\n",
      "iteration 35600, loss = 0.17056443953657954, prediction = [0.2273248  0.93916202 0.94379884 0.26195594]\n",
      "iteration 35650, loss = 0.170431051561596, prediction = [0.22718674 0.93923393 0.94386584 0.2618029 ]\n",
      "iteration 35700, loss = 0.17029796321260862, prediction = [0.22704895 0.93930566 0.94393267 0.26165016]\n",
      "iteration 35750, loss = 0.17016517340195275, prediction = [0.22691142 0.93937721 0.94399933 0.2614977 ]\n",
      "iteration 35800, loss = 0.1700326810473703, prediction = [0.22677416 0.93944858 0.94406583 0.26134553]\n",
      "iteration 35850, loss = 0.16990048507197725, prediction = [0.22663717 0.93951978 0.94413216 0.26119365]\n",
      "iteration 35900, loss = 0.16976858440422804, prediction = [0.22650045 0.93959079 0.94419832 0.26104206]\n",
      "iteration 35950, loss = 0.16963697797788557, prediction = [0.22636399 0.93966163 0.94426432 0.26089075]\n",
      "iteration 36000, loss = 0.16950566473198395, prediction = [0.22622779 0.93973229 0.94433015 0.26073973]\n",
      "iteration 36050, loss = 0.1693746436107983, prediction = [0.22609186 0.93980278 0.94439582 0.26058899]\n",
      "iteration 36100, loss = 0.16924391356381124, prediction = [0.22595618 0.93987309 0.94446132 0.26043854]\n",
      "iteration 36150, loss = 0.16911347354567977, prediction = [0.22582077 0.93994323 0.94452666 0.26028837]\n",
      "iteration 36200, loss = 0.16898332251620518, prediction = [0.22568562 0.94001319 0.94459184 0.26013848]\n",
      "iteration 36250, loss = 0.16885345944029734, prediction = [0.22555074 0.94008298 0.94465686 0.25998887]\n",
      "iteration 36300, loss = 0.16872388328794746, prediction = [0.2254161  0.9401526  0.94472171 0.25983954]\n",
      "iteration 36350, loss = 0.16859459303419366, prediction = [0.22528173 0.94022205 0.9447864  0.25969049]\n",
      "iteration 36400, loss = 0.1684655876590893, prediction = [0.22514762 0.94029133 0.94485094 0.25954171]\n",
      "iteration 36450, loss = 0.1683368661476752, prediction = [0.22501376 0.94036043 0.94491531 0.25939322]\n",
      "iteration 36500, loss = 0.16820842748994527, prediction = [0.22488015 0.94042937 0.94497953 0.259245  ]\n",
      "iteration 36550, loss = 0.16808027068081746, prediction = [0.22474681 0.94049814 0.94504358 0.25909705]\n",
      "iteration 36600, loss = 0.16795239472010387, prediction = [0.22461371 0.94056674 0.94510748 0.25894938]\n",
      "iteration 36650, loss = 0.1678247986124807, prediction = [0.22448087 0.94063517 0.94517123 0.25880199]\n",
      "iteration 36700, loss = 0.16769748136745644, prediction = [0.22434828 0.94070343 0.94523481 0.25865486]\n",
      "iteration 36750, loss = 0.16757044199934495, prediction = [0.22421594 0.94077153 0.94529824 0.25850801]\n",
      "iteration 36800, loss = 0.16744367952723377, prediction = [0.22408386 0.94083946 0.94536151 0.25836143]\n",
      "iteration 36850, loss = 0.16731719297495606, prediction = [0.22395202 0.94090723 0.94542463 0.25821512]\n",
      "iteration 36900, loss = 0.16719098137106198, prediction = [0.22382043 0.94097483 0.9454876  0.25806908]\n",
      "iteration 36950, loss = 0.1670650437487884, prediction = [0.22368909 0.94104227 0.94555041 0.25792331]\n",
      "iteration 37000, loss = 0.16693937914603146, prediction = [0.223558   0.94110954 0.94561307 0.25777781]\n",
      "iteration 37050, loss = 0.16681398660531732, prediction = [0.22342716 0.94117665 0.94567557 0.25763257]\n",
      "iteration 37100, loss = 0.16668886517377596, prediction = [0.22329656 0.9412436  0.94573792 0.2574876 ]\n",
      "iteration 37150, loss = 0.16656401390311035, prediction = [0.22316621 0.94131039 0.94580013 0.25734289]\n",
      "iteration 37200, loss = 0.16643943184957063, prediction = [0.2230361  0.94137701 0.94586218 0.25719845]\n",
      "iteration 37250, loss = 0.1663151180739268, prediction = [0.22290623 0.94144348 0.94592408 0.25705427]\n",
      "iteration 37300, loss = 0.1661910716414389, prediction = [0.22277661 0.94150979 0.94598583 0.25691036]\n",
      "iteration 37350, loss = 0.1660672916218338, prediction = [0.22264723 0.94157593 0.94604743 0.25676671]\n",
      "iteration 37400, loss = 0.16594377708927446, prediction = [0.22251809 0.94164192 0.94610888 0.25662332]\n",
      "iteration 37450, loss = 0.1658205271223352, prediction = [0.22238919 0.94170775 0.94617019 0.25648018]\n",
      "iteration 37500, loss = 0.16569754080397459, prediction = [0.22226054 0.94177342 0.94623135 0.25633731]\n",
      "iteration 37550, loss = 0.16557481722150946, prediction = [0.22213212 0.94183894 0.94629236 0.2561947 ]\n",
      "iteration 37600, loss = 0.16545235546658804, prediction = [0.22200394 0.9419043  0.94635322 0.25605235]\n",
      "iteration 37650, loss = 0.1653301546351646, prediction = [0.22187599 0.9419695  0.94641394 0.25591025]\n",
      "iteration 37700, loss = 0.16520821382747333, prediction = [0.22174829 0.94203455 0.94647451 0.25576841]\n",
      "iteration 37750, loss = 0.1650865321480029, prediction = [0.22162082 0.94209944 0.94653494 0.25562683]\n",
      "iteration 37800, loss = 0.1649651087054703, prediction = [0.22149358 0.94216418 0.94659522 0.2554855 ]\n",
      "iteration 37850, loss = 0.164843942612797, prediction = [0.22136659 0.94222876 0.94665536 0.25534442]\n",
      "iteration 37900, loss = 0.16472303298708302, prediction = [0.22123982 0.9422932  0.94671536 0.2552036 ]\n",
      "iteration 37950, loss = 0.1646023789495809, prediction = [0.22111329 0.94235747 0.94677521 0.25506303]\n",
      "iteration 38000, loss = 0.16448197962567435, prediction = [0.22098699 0.9424216  0.94683492 0.25492271]\n",
      "iteration 38050, loss = 0.1643618341448492, prediction = [0.22086092 0.94248558 0.94689449 0.25478265]\n",
      "iteration 38100, loss = 0.1642419416406729, prediction = [0.22073509 0.9425494  0.94695392 0.25464283]\n",
      "iteration 38150, loss = 0.16412230125076854, prediction = [0.22060948 0.94261308 0.9470132  0.25450326]\n",
      "iteration 38200, loss = 0.16400291211679097, prediction = [0.22048411 0.9426766  0.94707235 0.25436395]\n",
      "iteration 38250, loss = 0.1638837733844028, prediction = [0.22035896 0.94273998 0.94713136 0.25422488]\n",
      "iteration 38300, loss = 0.16376488420325164, prediction = [0.22023404 0.94280321 0.94719023 0.25408606]\n",
      "iteration 38350, loss = 0.1636462437269452, prediction = [0.22010935 0.94286629 0.94724895 0.25394748]\n",
      "iteration 38400, loss = 0.1635278511130303, prediction = [0.21998489 0.94292922 0.94730755 0.25380915]\n",
      "iteration 38450, loss = 0.16340970552296608, prediction = [0.21986065 0.942992   0.947366   0.25367107]\n",
      "iteration 38500, loss = 0.16329180612210478, prediction = [0.21973664 0.94305464 0.94742431 0.25353323]\n",
      "iteration 38550, loss = 0.163174152079666, prediction = [0.21961285 0.94311713 0.94748249 0.25339563]\n",
      "iteration 38600, loss = 0.1630567425687161, prediction = [0.21948929 0.94317948 0.94754054 0.25325828]\n",
      "iteration 38650, loss = 0.1629395767661445, prediction = [0.21936595 0.94324168 0.94759844 0.25312117]\n",
      "iteration 38700, loss = 0.1628226538526414, prediction = [0.21924283 0.94330374 0.94765621 0.2529843 ]\n",
      "iteration 38750, loss = 0.16270597301267564, prediction = [0.21911994 0.94336565 0.94771385 0.25284767]\n",
      "iteration 38800, loss = 0.16258953343447335, prediction = [0.21899726 0.94342742 0.94777135 0.25271128]\n",
      "iteration 38850, loss = 0.162473334309995, prediction = [0.21887481 0.94348904 0.94782872 0.25257513]\n",
      "iteration 38900, loss = 0.16235737483491486, prediction = [0.21875258 0.94355053 0.94788596 0.25243922]\n",
      "iteration 38950, loss = 0.16224165420859754, prediction = [0.21863057 0.94361187 0.94794306 0.25230355]\n",
      "iteration 39000, loss = 0.16212617163407889, prediction = [0.21850877 0.94367307 0.94800003 0.25216812]\n",
      "iteration 39050, loss = 0.16201092631804254, prediction = [0.2183872  0.94373413 0.94805686 0.25203292]\n",
      "iteration 39100, loss = 0.1618959174708008, prediction = [0.21826584 0.94379505 0.94811357 0.25189796]\n",
      "iteration 39150, loss = 0.16178114430627194, prediction = [0.2181447  0.94385582 0.94817015 0.25176323]\n",
      "iteration 39200, loss = 0.16166660604196048, prediction = [0.21802378 0.94391646 0.94822659 0.25162874]\n",
      "iteration 39250, loss = 0.16155230189893535, prediction = [0.21790307 0.94397696 0.9482829  0.25149448]\n",
      "iteration 39300, loss = 0.16143823110181083, prediction = [0.21778257 0.94403732 0.94833909 0.25136045]\n",
      "iteration 39350, loss = 0.1613243928787254, prediction = [0.21766229 0.94409755 0.94839514 0.25122666]\n",
      "iteration 39400, loss = 0.16121078646132087, prediction = [0.21754223 0.94415763 0.94845107 0.2510931 ]\n",
      "iteration 39450, loss = 0.16109741108472292, prediction = [0.21742237 0.94421758 0.94850687 0.25095977]\n",
      "iteration 39500, loss = 0.1609842659875211, prediction = [0.21730273 0.94427739 0.94856254 0.25082667]\n",
      "iteration 39550, loss = 0.16087135041174946, prediction = [0.2171833  0.94433707 0.94861808 0.2506938 ]\n",
      "iteration 39600, loss = 0.1607586636028655, prediction = [0.21706409 0.94439661 0.9486735  0.25056116]\n",
      "iteration 39650, loss = 0.16064620480973127, prediction = [0.21694508 0.94445602 0.94872878 0.25042875]\n",
      "iteration 39700, loss = 0.16053397328459496, prediction = [0.21682628 0.94451529 0.94878395 0.25029656]\n",
      "iteration 39750, loss = 0.16042196828306973, prediction = [0.21670769 0.94457442 0.94883898 0.25016461]\n",
      "iteration 39800, loss = 0.1603101890641155, prediction = [0.21658931 0.94463342 0.9488939  0.25003288]\n",
      "iteration 39850, loss = 0.16019863489002062, prediction = [0.21647114 0.94469229 0.94894868 0.24990137]\n",
      "iteration 39900, loss = 0.1600873050263813, prediction = [0.21635318 0.94475103 0.94900335 0.24977009]\n",
      "iteration 39950, loss = 0.15997619874208444, prediction = [0.21623542 0.94480963 0.94905788 0.24963904]\n",
      "iteration 40000, loss = 0.15986531530928755, prediction = [0.21611787 0.9448681  0.9491123  0.24950821]\n",
      "iteration 40050, loss = 0.159754654003402, prediction = [0.21600052 0.94492644 0.94916659 0.2493776 ]\n",
      "iteration 40100, loss = 0.15964421410307172, prediction = [0.21588338 0.94498465 0.94922076 0.24924721]\n",
      "iteration 40150, loss = 0.15953399489015915, prediction = [0.21576645 0.94504273 0.94927481 0.24911705]\n",
      "iteration 40200, loss = 0.1594239956497226, prediction = [0.21564971 0.94510068 0.94932873 0.24898711]\n",
      "iteration 40250, loss = 0.1593142156700013, prediction = [0.21553318 0.9451585  0.94938254 0.24885738]\n",
      "iteration 40300, loss = 0.1592046542423966, prediction = [0.21541686 0.94521619 0.94943622 0.24872788]\n",
      "iteration 40350, loss = 0.15909531066145363, prediction = [0.21530073 0.94527375 0.94948978 0.2485986 ]\n",
      "iteration 40400, loss = 0.15898618422484428, prediction = [0.21518481 0.94533118 0.94954322 0.24846954]\n",
      "iteration 40450, loss = 0.15887727423334969, prediction = [0.21506909 0.94538848 0.94959655 0.24834069]\n",
      "iteration 40500, loss = 0.15876857999084237, prediction = [0.21495357 0.94544566 0.94964975 0.24821206]\n",
      "iteration 40550, loss = 0.1586601008042692, prediction = [0.21483824 0.94550271 0.94970283 0.24808365]\n",
      "iteration 40600, loss = 0.15855183598363365, prediction = [0.21472312 0.94555963 0.9497558  0.24795546]\n",
      "iteration 40650, loss = 0.15844378484198057, prediction = [0.2146082  0.94561643 0.94980865 0.24782748]\n",
      "iteration 40700, loss = 0.15833594669537637, prediction = [0.21449347 0.9456731  0.94986138 0.24769972]\n",
      "iteration 40750, loss = 0.15822832086289518, prediction = [0.21437894 0.94572964 0.94991399 0.24757217]\n",
      "iteration 40800, loss = 0.1581209066665995, prediction = [0.21426461 0.94578606 0.94996648 0.24744483]\n",
      "iteration 40850, loss = 0.158013703431525, prediction = [0.21415047 0.94584236 0.95001886 0.24731771]\n",
      "iteration 40900, loss = 0.15790671048566368, prediction = [0.21403653 0.94589853 0.95007112 0.2471908 ]\n",
      "iteration 40950, loss = 0.15779992715994792, prediction = [0.21392278 0.94595458 0.95012327 0.2470641 ]\n",
      "iteration 41000, loss = 0.1576933527882332, prediction = [0.21380923 0.9460105  0.9501753  0.24693762]\n",
      "iteration 41050, loss = 0.1575869867072834, prediction = [0.21369587 0.9460663  0.95022722 0.24681134]\n",
      "iteration 41100, loss = 0.15748082825675302, prediction = [0.21358271 0.94612198 0.95027902 0.24668528]\n",
      "iteration 41150, loss = 0.1573748767791724, prediction = [0.21346974 0.94617754 0.95033071 0.24655942]\n",
      "iteration 41200, loss = 0.15726913161993109, prediction = [0.21335696 0.94623297 0.95038228 0.24643378]\n",
      "iteration 41250, loss = 0.1571635921272634, prediction = [0.21324437 0.94628829 0.95043374 0.24630834]\n",
      "iteration 41300, loss = 0.15705825765223097, prediction = [0.21313198 0.94634348 0.95048509 0.24618311]\n",
      "iteration 41350, loss = 0.15695312754870883, prediction = [0.21301977 0.94639855 0.95053632 0.24605809]\n",
      "iteration 41400, loss = 0.15684820117336837, prediction = [0.21290776 0.94645351 0.95058744 0.24593327]\n",
      "iteration 41450, loss = 0.15674347788566406, prediction = [0.21279593 0.94650834 0.95063845 0.24580866]\n",
      "iteration 41500, loss = 0.1566389570478161, prediction = [0.2126843  0.94656305 0.95068935 0.24568426]\n",
      "iteration 41550, loss = 0.15653463802479645, prediction = [0.21257285 0.94661765 0.95074014 0.24556006]\n",
      "iteration 41600, loss = 0.15643052018431386, prediction = [0.21246159 0.94667212 0.95079082 0.24543607]\n",
      "iteration 41650, loss = 0.15632660289679817, prediction = [0.21235052 0.94672648 0.95084138 0.24531228]\n",
      "iteration 41700, loss = 0.15622288553538644, prediction = [0.21223963 0.94678072 0.95089184 0.24518869]\n",
      "iteration 41750, loss = 0.15611936747590738, prediction = [0.21212893 0.94683485 0.95094218 0.24506531]\n",
      "iteration 41800, loss = 0.1560160480968677, prediction = [0.21201842 0.94688885 0.95099242 0.24494213]\n",
      "iteration 41850, loss = 0.15591292677943575, prediction = [0.21190809 0.94694274 0.95104255 0.24481915]\n",
      "iteration 41900, loss = 0.15581000290742897, prediction = [0.21179795 0.94699652 0.95109257 0.24469637]\n",
      "iteration 41950, loss = 0.15570727586729863, prediction = [0.21168799 0.94705017 0.95114248 0.24457379]\n",
      "iteration 42000, loss = 0.15560474504811547, prediction = [0.21157822 0.94710372 0.95119228 0.24445141]\n",
      "iteration 42050, loss = 0.1555024098415568, prediction = [0.21146863 0.94715714 0.95124197 0.24432923]\n",
      "iteration 42100, loss = 0.15540026964188972, prediction = [0.21135922 0.94721046 0.95129156 0.24420725]\n",
      "iteration 42150, loss = 0.15529832384596085, prediction = [0.21124999 0.94726366 0.95134104 0.24408547]\n",
      "iteration 42200, loss = 0.1551965718531783, prediction = [0.21114095 0.94731674 0.95139042 0.24396389]\n",
      "iteration 42250, loss = 0.1550950130655015, prediction = [0.21103208 0.94736971 0.95143969 0.2438425 ]\n",
      "iteration 42300, loss = 0.15499364688742545, prediction = [0.2109234  0.94742257 0.95148885 0.24372131]\n",
      "iteration 42350, loss = 0.15489247272596723, prediction = [0.2108149  0.94747532 0.95153791 0.24360032]\n",
      "iteration 42400, loss = 0.15479148999065334, prediction = [0.21070657 0.94752795 0.95158686 0.24347952]\n",
      "iteration 42450, loss = 0.15469069809350555, prediction = [0.21059843 0.94758047 0.95163571 0.24335892]\n",
      "iteration 42500, loss = 0.15459009644902827, prediction = [0.21049047 0.94763288 0.95168445 0.24323851]\n",
      "iteration 42550, loss = 0.15448968447419348, prediction = [0.21038268 0.94768518 0.95173309 0.24311829]\n",
      "iteration 42600, loss = 0.15438946158842937, prediction = [0.21027507 0.94773736 0.95178162 0.24299827]\n",
      "iteration 42650, loss = 0.1542894272136075, prediction = [0.21016764 0.94778944 0.95183005 0.24287845]\n",
      "iteration 42700, loss = 0.1541895807740276, prediction = [0.21006038 0.94784141 0.95187838 0.24275881]\n",
      "iteration 42750, loss = 0.15408992169640734, prediction = [0.2099533  0.94789326 0.95192661 0.24263937]\n",
      "iteration 42800, loss = 0.1539904494098669, prediction = [0.2098464  0.94794501 0.95197473 0.24252012]\n",
      "iteration 42850, loss = 0.15389116334591782, prediction = [0.20973967 0.94799665 0.95202275 0.24240106]\n",
      "iteration 42900, loss = 0.15379206293845013, prediction = [0.20963312 0.94804818 0.95207067 0.24228219]\n",
      "iteration 42950, loss = 0.15369314762371888, prediction = [0.20952674 0.9480996  0.95211849 0.24216351]\n",
      "iteration 43000, loss = 0.15359441684033326, prediction = [0.20942054 0.94815091 0.9521662  0.24204502]\n",
      "iteration 43050, loss = 0.15349587002924192, prediction = [0.20931451 0.94820212 0.95221382 0.24192671]\n",
      "iteration 43100, loss = 0.15339750663372292, prediction = [0.20920865 0.94825321 0.95226133 0.2418086 ]\n",
      "iteration 43150, loss = 0.1532993260993694, prediction = [0.20910296 0.9483042  0.95230875 0.24169068]\n",
      "iteration 43200, loss = 0.15320132787407909, prediction = [0.20899745 0.94835509 0.95235606 0.24157294]\n",
      "iteration 43250, loss = 0.1531035114080408, prediction = [0.20889211 0.94840586 0.95240328 0.24145539]\n",
      "iteration 43300, loss = 0.1530058761537233, prediction = [0.20878693 0.94845653 0.9524504  0.24133802]\n",
      "iteration 43350, loss = 0.15290842156586282, prediction = [0.20868193 0.9485071  0.95249741 0.24122084]\n",
      "iteration 43400, loss = 0.152811147101451, prediction = [0.2085771  0.94855756 0.95254433 0.24110385]\n",
      "iteration 43450, loss = 0.15271405221972434, prediction = [0.20847244 0.94860791 0.95259115 0.24098704]\n",
      "iteration 43500, loss = 0.15261713638215046, prediction = [0.20836795 0.94865816 0.95263788 0.24087041]\n",
      "iteration 43550, loss = 0.15252039905241802, prediction = [0.20826363 0.94870831 0.9526845  0.24075397]\n",
      "iteration 43600, loss = 0.15242383969642426, prediction = [0.20815947 0.94875835 0.95273103 0.24063772]\n",
      "iteration 43650, loss = 0.1523274577822641, prediction = [0.20805548 0.94880829 0.95277746 0.24052164]\n",
      "iteration 43700, loss = 0.15223125278021785, prediction = [0.20795166 0.94885812 0.95282379 0.24040575]\n",
      "iteration 43750, loss = 0.15213522416274022, prediction = [0.20784801 0.94890785 0.95287003 0.24029004]\n",
      "iteration 43800, loss = 0.15203937140445045, prediction = [0.20774452 0.94895748 0.95291617 0.24017451]\n",
      "iteration 43850, loss = 0.15194369398211752, prediction = [0.2076412  0.94900701 0.95296222 0.24005917]\n",
      "iteration 43900, loss = 0.1518481913746531, prediction = [0.20753805 0.94905643 0.95300817 0.239944  ]\n",
      "iteration 43950, loss = 0.15175286306309746, prediction = [0.20743506 0.94910576 0.95305403 0.23982901]\n",
      "iteration 44000, loss = 0.1516577085306094, prediction = [0.20733223 0.94915498 0.95309979 0.23971421]\n",
      "iteration 44050, loss = 0.15156272726245662, prediction = [0.20722957 0.9492041  0.95314545 0.23959958]\n",
      "iteration 44100, loss = 0.15146791874600174, prediction = [0.20712707 0.94925312 0.95319102 0.23948513]\n",
      "iteration 44150, loss = 0.15137328247069465, prediction = [0.20702474 0.94930203 0.9532365  0.23937086]\n",
      "iteration 44200, loss = 0.15127881792805997, prediction = [0.20692257 0.94935085 0.95328188 0.23925676]\n",
      "iteration 44250, loss = 0.15118452461168658, prediction = [0.20682056 0.94939957 0.95332718 0.23914285]\n",
      "iteration 44300, loss = 0.15109040201721807, prediction = [0.20671871 0.94944819 0.95337237 0.23902911]\n",
      "iteration 44350, loss = 0.15099644964233983, prediction = [0.20661703 0.94949671 0.95341748 0.23891554]\n",
      "iteration 44400, loss = 0.1509026669867713, prediction = [0.2065155  0.94954513 0.95346249 0.23880216]\n",
      "iteration 44450, loss = 0.15080905355225385, prediction = [0.20641414 0.94959345 0.95350741 0.23868894]\n",
      "iteration 44500, loss = 0.15071560884254054, prediction = [0.20631294 0.94964167 0.95355224 0.23857591]\n",
      "iteration 44550, loss = 0.15062233236338668, prediction = [0.20621189 0.9496898  0.95359697 0.23846304]\n",
      "iteration 44600, loss = 0.15052922362253898, prediction = [0.20611101 0.94973783 0.95364162 0.23835036]\n",
      "iteration 44650, loss = 0.15043628212972487, prediction = [0.20601028 0.94978576 0.95368617 0.23823784]\n",
      "iteration 44700, loss = 0.1503435073966428, prediction = [0.20590972 0.94983359 0.95373064 0.2381255 ]\n",
      "iteration 44750, loss = 0.15025089893695392, prediction = [0.20580931 0.94988133 0.95377501 0.23801333]\n",
      "iteration 44800, loss = 0.15015845626626856, prediction = [0.20570906 0.94992897 0.95381929 0.23790133]\n",
      "iteration 44850, loss = 0.15006617890213944, prediction = [0.20560897 0.94997651 0.95386348 0.23778951]\n",
      "iteration 44900, loss = 0.1499740663640497, prediction = [0.20550903 0.95002396 0.95390758 0.23767785]\n",
      "iteration 44950, loss = 0.14988211817340513, prediction = [0.20540925 0.95007131 0.9539516  0.23756637]\n",
      "iteration 45000, loss = 0.14979033385352258, prediction = [0.20530963 0.95011857 0.95399552 0.23745506]\n",
      "iteration 45050, loss = 0.14969871292962134, prediction = [0.20521016 0.95016573 0.95403936 0.23734391]\n",
      "iteration 45100, loss = 0.14960725492881255, prediction = [0.20511085 0.9502128  0.9540831  0.23723294]\n",
      "iteration 45150, loss = 0.14951595938009116, prediction = [0.20501169 0.95025977 0.95412676 0.23712214]\n",
      "iteration 45200, loss = 0.14942482581432454, prediction = [0.20491269 0.95030665 0.95417033 0.2370115 ]\n",
      "iteration 45250, loss = 0.1493338537642446, prediction = [0.20481384 0.95035343 0.95421382 0.23690103]\n",
      "iteration 45300, loss = 0.14924304276443873, prediction = [0.20471515 0.95040012 0.95425721 0.23679073]\n",
      "iteration 45350, loss = 0.14915239235133726, prediction = [0.2046166  0.95044672 0.95430052 0.2366806 ]\n",
      "iteration 45400, loss = 0.1490619020632083, prediction = [0.20451821 0.95049323 0.95434374 0.23657064]\n",
      "iteration 45450, loss = 0.14897157144014614, prediction = [0.20441998 0.95053964 0.95438687 0.23646084]\n",
      "iteration 45500, loss = 0.14888140002406242, prediction = [0.20432189 0.95058596 0.95442992 0.23635121]\n",
      "iteration 45550, loss = 0.14879138735867753, prediction = [0.20422396 0.95063218 0.95447288 0.23624174]\n",
      "iteration 45600, loss = 0.1487015329895113, prediction = [0.20412618 0.95067832 0.95451576 0.23613244]\n",
      "iteration 45650, loss = 0.1486118364638735, prediction = [0.20402855 0.95072436 0.95455855 0.2360233 ]\n",
      "iteration 45700, loss = 0.14852229733085548, prediction = [0.20393107 0.95077031 0.95460125 0.23591433]\n",
      "iteration 45750, loss = 0.148432915141322, prediction = [0.20383374 0.95081618 0.95464387 0.23580552]\n",
      "iteration 45800, loss = 0.1483436894479002, prediction = [0.20373656 0.95086195 0.95468641 0.23569688]\n",
      "iteration 45850, loss = 0.1482546198049729, prediction = [0.20363953 0.95090763 0.95472886 0.2355884 ]\n",
      "iteration 45900, loss = 0.14816570576866855, prediction = [0.20354265 0.95095322 0.95477122 0.23548008]\n",
      "iteration 45950, loss = 0.14807694689685416, prediction = [0.20344591 0.95099872 0.9548135  0.23537192]\n",
      "iteration 46000, loss = 0.1479883427491246, prediction = [0.20334933 0.95104413 0.9548557  0.23526393]\n",
      "iteration 46050, loss = 0.14789989288679553, prediction = [0.20325289 0.95108945 0.95489782 0.23515609]\n",
      "iteration 46100, loss = 0.14781159687289436, prediction = [0.2031566  0.95113468 0.95493985 0.23504842]\n",
      "iteration 46150, loss = 0.14772345427215144, prediction = [0.20306046 0.95117982 0.95498179 0.23494091]\n",
      "iteration 46200, loss = 0.1476354646509929, prediction = [0.20296446 0.95122487 0.95502366 0.23483356]\n",
      "iteration 46250, loss = 0.14754762757753015, prediction = [0.20286861 0.95126984 0.95506544 0.23472637]\n",
      "iteration 46300, loss = 0.14745994262155365, prediction = [0.20277291 0.95131472 0.95510714 0.23461934]\n",
      "iteration 46350, loss = 0.1473724093545237, prediction = [0.20267735 0.95135951 0.95514876 0.23451247]\n",
      "iteration 46400, loss = 0.14728502734956195, prediction = [0.20258193 0.95140421 0.9551903  0.23440575]\n",
      "iteration 46450, loss = 0.14719779618144363, prediction = [0.20248667 0.95144882 0.95523175 0.2342992 ]\n",
      "iteration 46500, loss = 0.14711071542658988, prediction = [0.20239154 0.95149335 0.95527312 0.2341928 ]\n",
      "iteration 46550, loss = 0.14702378466305863, prediction = [0.20229656 0.95153779 0.95531442 0.23408656]\n",
      "iteration 46600, loss = 0.14693700347053684, prediction = [0.20220173 0.95158214 0.95535563 0.23398048]\n",
      "iteration 46650, loss = 0.14685037143033347, prediction = [0.20210703 0.95162641 0.95539676 0.23387455]\n",
      "iteration 46700, loss = 0.14676388812537072, prediction = [0.20201248 0.95167059 0.95543781 0.23376878]\n",
      "iteration 46750, loss = 0.14667755314017542, prediction = [0.20191807 0.95171469 0.95547878 0.23366317]\n",
      "iteration 46800, loss = 0.14659136606087358, prediction = [0.20182381 0.9517587  0.95551967 0.23355771]\n",
      "iteration 46850, loss = 0.14650532647517922, prediction = [0.20172969 0.95180263 0.95556048 0.23345241]\n",
      "iteration 46900, loss = 0.14641943397238982, prediction = [0.2016357  0.95184647 0.95560121 0.23334726]\n",
      "iteration 46950, loss = 0.14633368814337744, prediction = [0.20154186 0.95189022 0.95564186 0.23324227]\n",
      "iteration 47000, loss = 0.1462480885805793, prediction = [0.20144816 0.95193389 0.95568243 0.23313743]\n",
      "iteration 47050, loss = 0.1461626348779932, prediction = [0.2013546  0.95197748 0.95572293 0.23303274]\n",
      "iteration 47100, loss = 0.14607732663116768, prediction = [0.20126118 0.95202098 0.95576334 0.23292821]\n",
      "iteration 47150, loss = 0.145992163437196, prediction = [0.2011679  0.9520644  0.95580368 0.23282383]\n",
      "iteration 47200, loss = 0.14590714489470732, prediction = [0.20107476 0.95210774 0.95584394 0.2327196 ]\n",
      "iteration 47250, loss = 0.1458222706038602, prediction = [0.20098176 0.95215099 0.95588412 0.23261553]\n",
      "iteration 47300, loss = 0.14573754016633456, prediction = [0.2008889  0.95219416 0.95592422 0.2325116 ]\n",
      "iteration 47350, loss = 0.14565295318532534, prediction = [0.20079618 0.95223725 0.95596425 0.23240783]\n",
      "iteration 47400, loss = 0.14556850926553416, prediction = [0.20070359 0.95228025 0.9560042  0.23230421]\n",
      "iteration 47450, loss = 0.14548420801316192, prediction = [0.20061114 0.95232317 0.95604407 0.23220074]\n",
      "iteration 47500, loss = 0.14540004903590348, prediction = [0.20051883 0.95236601 0.95608387 0.23209742]\n",
      "iteration 47550, loss = 0.1453160319429373, prediction = [0.20042666 0.95240877 0.95612359 0.23199425]\n",
      "iteration 47600, loss = 0.14523215634492176, prediction = [0.20033462 0.95245144 0.95616323 0.23189123]\n",
      "iteration 47650, loss = 0.14514842185398574, prediction = [0.20024272 0.95249404 0.9562028  0.23178836]\n",
      "iteration 47700, loss = 0.14506482808372106, prediction = [0.20015095 0.95253655 0.95624229 0.23168563]\n",
      "iteration 47750, loss = 0.14498137464917937, prediction = [0.20005932 0.95257899 0.95628171 0.23158306]\n",
      "iteration 47800, loss = 0.14489806116685938, prediction = [0.19996783 0.95262134 0.95632105 0.23148063]\n",
      "iteration 47850, loss = 0.14481488725470537, prediction = [0.19987647 0.95266361 0.95636031 0.23137836]\n",
      "iteration 47900, loss = 0.14473185253209592, prediction = [0.19978524 0.9527058  0.9563995  0.23127622]\n",
      "iteration 47950, loss = 0.14464895661984079, prediction = [0.19969415 0.95274791 0.95643862 0.23117424]\n",
      "iteration 48000, loss = 0.14456619914017146, prediction = [0.1996032  0.95278995 0.95647766 0.2310724 ]\n",
      "iteration 48050, loss = 0.1444835797167353, prediction = [0.19951237 0.9528319  0.95651663 0.23097071]\n",
      "iteration 48100, loss = 0.1444010979745886, prediction = [0.19942168 0.95287377 0.95655552 0.23086917]\n",
      "iteration 48150, loss = 0.14431875354019105, prediction = [0.19933113 0.95291557 0.95659434 0.23076777]\n",
      "iteration 48200, loss = 0.14423654604139685, prediction = [0.1992407  0.95295728 0.95663308 0.23066651]\n",
      "iteration 48250, loss = 0.14415447510745044, prediction = [0.19915041 0.95299892 0.95667176 0.2305654 ]\n",
      "iteration 48300, loss = 0.1440725403689777, prediction = [0.19906025 0.95304048 0.95671036 0.23046444]\n",
      "iteration 48350, loss = 0.1439907414579813, prediction = [0.19897022 0.95308196 0.95674888 0.23036362]\n",
      "iteration 48400, loss = 0.14390907800783315, prediction = [0.19888032 0.95312336 0.95678734 0.23026294]\n",
      "iteration 48450, loss = 0.14382754965326844, prediction = [0.19879056 0.95316468 0.95682572 0.23016241]\n",
      "iteration 48500, loss = 0.14374615603037805, prediction = [0.19870092 0.95320593 0.95686402 0.23006202]\n",
      "iteration 48550, loss = 0.1436648967766046, prediction = [0.19861142 0.9532471  0.95690226 0.22996177]\n",
      "iteration 48600, loss = 0.14358377153073287, prediction = [0.19852204 0.95328819 0.95694043 0.22986166]\n",
      "iteration 48650, loss = 0.14350277993288696, prediction = [0.1984328  0.95332921 0.95697852 0.2297617 ]\n",
      "iteration 48700, loss = 0.14342192162452044, prediction = [0.19834368 0.95337015 0.95701654 0.22966188]\n",
      "iteration 48750, loss = 0.14334119624841302, prediction = [0.19825469 0.95341101 0.95705449 0.2295622 ]\n",
      "iteration 48800, loss = 0.14326060344866298, prediction = [0.19816584 0.9534518  0.95709237 0.22946266]\n",
      "iteration 48850, loss = 0.14318014287068065, prediction = [0.19807711 0.95349251 0.95713018 0.22936326]\n",
      "iteration 48900, loss = 0.14309981416118328, prediction = [0.1979885  0.95353315 0.95716791 0.22926401]\n",
      "iteration 48950, loss = 0.14301961696818816, prediction = [0.19790003 0.95357371 0.95720558 0.22916489]\n",
      "iteration 49000, loss = 0.1429395509410072, prediction = [0.19781168 0.95361419 0.95724318 0.22906591]\n",
      "iteration 49050, loss = 0.14285961573024025, prediction = [0.19772347 0.9536546  0.9572807  0.22896707]\n",
      "iteration 49100, loss = 0.14277981098776898, prediction = [0.19763537 0.95369494 0.95731816 0.22886837]\n",
      "iteration 49150, loss = 0.14270013636675247, prediction = [0.19754741 0.9537352  0.95735555 0.22876981]\n",
      "iteration 49200, loss = 0.1426205915216186, prediction = [0.19745957 0.95377538 0.95739286 0.22867139]\n",
      "iteration 49250, loss = 0.14254117610806094, prediction = [0.19737186 0.9538155  0.95743011 0.22857311]\n",
      "iteration 49300, loss = 0.1424618897830306, prediction = [0.19728427 0.95385553 0.95746729 0.22847496]\n",
      "iteration 49350, loss = 0.1423827322047319, prediction = [0.19719681 0.9538955  0.9575044  0.22837695]\n",
      "iteration 49400, loss = 0.1423037030326159, prediction = [0.19710947 0.95393539 0.95754144 0.22827908]\n",
      "iteration 49450, loss = 0.1422248019273743, prediction = [0.19702226 0.95397521 0.95757841 0.22818135]\n",
      "iteration 49500, loss = 0.14214602855093475, prediction = [0.19693517 0.95401495 0.95761532 0.22808375]\n",
      "iteration 49550, loss = 0.1420673825664542, prediction = [0.1968482  0.95405462 0.95765215 0.22798629]\n",
      "iteration 49600, loss = 0.14198886363831367, prediction = [0.19676137 0.95409422 0.95768892 0.22788896]\n",
      "iteration 49650, loss = 0.14191047143211236, prediction = [0.19667465 0.95413374 0.95772562 0.22779177]\n",
      "iteration 49700, loss = 0.14183220561466214, prediction = [0.19658806 0.9541732  0.95776226 0.22769471]\n",
      "iteration 49750, loss = 0.14175406585398181, prediction = [0.19650159 0.95421258 0.95779882 0.22759779]\n",
      "iteration 49800, loss = 0.1416760518192927, prediction = [0.19641524 0.95425189 0.95783532 0.22750101]\n",
      "iteration 49850, loss = 0.14159816318101065, prediction = [0.19632901 0.95429112 0.95787175 0.22740435]\n",
      "iteration 49900, loss = 0.14152039961074311, prediction = [0.19624291 0.95433029 0.95790812 0.22730784]\n",
      "iteration 49950, loss = 0.14144276078128204, prediction = [0.19615693 0.95436939 0.95794441 0.22721145]\n",
      "iteration 50000, loss = 0.14136524636659972, prediction = [0.19607107 0.95440841 0.95798064 0.2271152 ]\n",
      "iteration 50050, loss = 0.1412878560418418, prediction = [0.19598533 0.95444736 0.95801681 0.22701908]\n",
      "iteration 50100, loss = 0.14121058948332316, prediction = [0.19589972 0.95448624 0.95805291 0.2269231 ]\n",
      "iteration 50150, loss = 0.1411334463685229, prediction = [0.19581422 0.95452505 0.95808894 0.22682724]\n",
      "iteration 50200, loss = 0.1410564263760769, prediction = [0.19572884 0.95456379 0.95812491 0.22673152]\n",
      "iteration 50250, loss = 0.1409795291857749, prediction = [0.19564359 0.95460246 0.95816081 0.22663593]\n",
      "iteration 50300, loss = 0.14090275447855422, prediction = [0.19555845 0.95464106 0.95819665 0.22654048]\n",
      "iteration 50350, loss = 0.14082610193649453, prediction = [0.19547343 0.9546796  0.95823242 0.22644515]\n",
      "iteration 50400, loss = 0.14074957124281295, prediction = [0.19538854 0.95471806 0.95826813 0.22634995]\n",
      "iteration 50450, loss = 0.14067316208185843, prediction = [0.19530376 0.95475645 0.95830377 0.22625489]\n",
      "iteration 50500, loss = 0.14059687413910693, prediction = [0.1952191  0.95479477 0.95833935 0.22615995]\n",
      "iteration 50550, loss = 0.1405207071011565, prediction = [0.19513456 0.95483302 0.95837486 0.22606515]\n",
      "iteration 50600, loss = 0.1404446606557211, prediction = [0.19505013 0.95487121 0.95841031 0.22597047]\n",
      "iteration 50650, loss = 0.1403687344916273, prediction = [0.19496583 0.95490933 0.95844569 0.22587593]\n",
      "iteration 50700, loss = 0.1402929282988084, prediction = [0.19488164 0.95494737 0.95848101 0.22578151]\n",
      "iteration 50750, loss = 0.14021724176829886, prediction = [0.19479757 0.95498535 0.95851627 0.22568722]\n",
      "iteration 50800, loss = 0.14014167459222984, prediction = [0.19471362 0.95502326 0.95855146 0.22559306]\n",
      "iteration 50850, loss = 0.14006622646382477, prediction = [0.19462978 0.95506111 0.95858659 0.22549903]\n",
      "iteration 50900, loss = 0.1399908970773933, prediction = [0.19454606 0.95509888 0.95862166 0.22540512]\n",
      "iteration 50950, loss = 0.1399156861283279, prediction = [0.19446246 0.95513659 0.95865666 0.22531135]\n",
      "iteration 51000, loss = 0.1398405933130965, prediction = [0.19437897 0.95517423 0.9586916  0.2252177 ]\n",
      "iteration 51050, loss = 0.1397656183292409, prediction = [0.1942956  0.95521181 0.95872648 0.22512418]\n",
      "iteration 51100, loss = 0.13969076087536997, prediction = [0.19421234 0.95524931 0.9587613  0.22503078]\n",
      "iteration 51150, loss = 0.13961602065115486, prediction = [0.1941292  0.95528675 0.95879605 0.22493751]\n",
      "iteration 51200, loss = 0.13954139735732443, prediction = [0.19404617 0.95532412 0.95883074 0.22484437]\n",
      "iteration 51250, loss = 0.1394668906956618, prediction = [0.19396326 0.95536143 0.95886537 0.22475135]\n",
      "iteration 51300, loss = 0.13939250036899692, prediction = [0.19388046 0.95539867 0.95889994 0.22465846]\n",
      "iteration 51350, loss = 0.13931822608120492, prediction = [0.19379778 0.95543585 0.95893444 0.22456569]\n",
      "iteration 51400, loss = 0.13924406753719887, prediction = [0.1937152  0.95547296 0.95896889 0.22447305]\n",
      "iteration 51450, loss = 0.13917002444292764, prediction = [0.19363275 0.95551    0.95900327 0.22438054]\n",
      "iteration 51500, loss = 0.13909609650536903, prediction = [0.1935504  0.95554698 0.95903759 0.22428814]\n",
      "iteration 51550, loss = 0.13902228343252615, prediction = [0.19346817 0.95558389 0.95907185 0.22419587]\n",
      "iteration 51600, loss = 0.13894858493342307, prediction = [0.19338606 0.95562074 0.95910605 0.22410373]\n",
      "iteration 51650, loss = 0.1388750007181001, prediction = [0.19330405 0.95565752 0.95914019 0.22401171]\n",
      "iteration 51700, loss = 0.13880153049760924, prediction = [0.19322216 0.95569423 0.95917427 0.22391981]\n",
      "iteration 51750, loss = 0.13872817398401, prediction = [0.19314038 0.95573089 0.95920829 0.22382804]\n",
      "iteration 51800, loss = 0.1386549308903635, prediction = [0.19305871 0.95576747 0.95924225 0.22373638]\n",
      "iteration 51850, loss = 0.13858180093073044, prediction = [0.19297715 0.955804   0.95927614 0.22364486]\n",
      "iteration 51900, loss = 0.13850878382016418, prediction = [0.1928957  0.95584046 0.95930998 0.22355345]\n",
      "iteration 51950, loss = 0.1384358792747091, prediction = [0.19281437 0.95587685 0.95934376 0.22346216]\n",
      "iteration 52000, loss = 0.13836308701139358, prediction = [0.19273314 0.95591318 0.95937748 0.223371  ]\n",
      "iteration 52050, loss = 0.13829040674822723, prediction = [0.19265203 0.95594945 0.95941114 0.22327996]\n",
      "iteration 52100, loss = 0.13821783820419584, prediction = [0.19257102 0.95598566 0.95944474 0.22318903]\n",
      "iteration 52150, loss = 0.13814538109925767, prediction = [0.19249013 0.9560218  0.95947828 0.22309823]\n",
      "iteration 52200, loss = 0.13807303515433894, prediction = [0.19240934 0.95605788 0.95951176 0.22300755]\n",
      "iteration 52250, loss = 0.13800080009132898, prediction = [0.19232867 0.95609389 0.95954519 0.22291699]\n",
      "iteration 52300, loss = 0.13792867563307734, prediction = [0.1922481  0.95612985 0.95957855 0.22282655]\n",
      "iteration 52350, loss = 0.13785666150338782, prediction = [0.19216764 0.95616574 0.95961186 0.22273623]\n",
      "iteration 52400, loss = 0.1377847574270165, prediction = [0.19208729 0.95620156 0.95964511 0.22264603]\n",
      "iteration 52450, loss = 0.1377129631296657, prediction = [0.19200705 0.95623733 0.9596783  0.22255595]\n",
      "iteration 52500, loss = 0.13764127833797973, prediction = [0.19192692 0.95627303 0.95971143 0.22246599]\n",
      "iteration 52550, loss = 0.1375697027795424, prediction = [0.1918469  0.95630867 0.9597445  0.22237614]\n",
      "iteration 52600, loss = 0.13749823618287216, prediction = [0.19176698 0.95634425 0.95977752 0.22228642]\n",
      "iteration 52650, loss = 0.1374268782774175, prediction = [0.19168717 0.95637977 0.95981048 0.22219681]\n",
      "iteration 52700, loss = 0.13735562879355367, prediction = [0.19160747 0.95641523 0.95984338 0.22210732]\n",
      "iteration 52750, loss = 0.13728448746257743, prediction = [0.19152787 0.95645062 0.95987623 0.22201795]\n",
      "iteration 52800, loss = 0.13721345401670504, prediction = [0.19144839 0.95648596 0.95990901 0.22192869]\n",
      "iteration 52850, loss = 0.13714252818906644, prediction = [0.191369   0.95652123 0.95994175 0.22183956]\n",
      "iteration 52900, loss = 0.1370717097137022, prediction = [0.19128973 0.95655644 0.95997442 0.22175053]\n",
      "iteration 52950, loss = 0.1370009983255591, prediction = [0.19121056 0.95659159 0.96000704 0.22166163]\n",
      "iteration 53000, loss = 0.13693039376048688, prediction = [0.1911315  0.95662668 0.9600396  0.22157284]\n",
      "iteration 53050, loss = 0.13685989575523333, prediction = [0.19105254 0.95666171 0.9600721  0.22148417]\n",
      "iteration 53100, loss = 0.1367895040474415, prediction = [0.19097368 0.95669668 0.96010455 0.22139561]\n",
      "iteration 53150, loss = 0.136719218375645, prediction = [0.19089494 0.9567316  0.96013695 0.22130717]\n",
      "iteration 53200, loss = 0.13664903847926482, prediction = [0.19081629 0.95676645 0.96016928 0.22121885]\n",
      "iteration 53250, loss = 0.13657896409860432, prediction = [0.19073776 0.95680124 0.96020156 0.22113064]\n",
      "iteration 53300, loss = 0.13650899497484734, prediction = [0.19065932 0.95683597 0.96023379 0.22104254]\n",
      "iteration 53350, loss = 0.1364391308500528, prediction = [0.19058099 0.95687064 0.96026596 0.22095456]\n",
      "iteration 53400, loss = 0.13636937146715128, prediction = [0.19050276 0.95690525 0.96029807 0.22086669]\n",
      "iteration 53450, loss = 0.13629971656994186, prediction = [0.19042464 0.95693981 0.96033013 0.22077894]\n",
      "iteration 53500, loss = 0.1362301659030875, prediction = [0.19034662 0.9569743  0.96036214 0.2206913 ]\n",
      "iteration 53550, loss = 0.13616071921211273, prediction = [0.19026871 0.95700874 0.96039409 0.22060377]\n",
      "iteration 53600, loss = 0.13609137624339804, prediction = [0.19019089 0.95704312 0.96042598 0.22051636]\n",
      "iteration 53650, loss = 0.13602213674417796, prediction = [0.19011318 0.95707744 0.96045783 0.22042906]\n",
      "iteration 53700, loss = 0.1359530004625361, prediction = [0.19003558 0.9571117  0.96048961 0.22034187]\n",
      "iteration 53750, loss = 0.1358839671474028, prediction = [0.18995807 0.9571459  0.96052134 0.2202548 ]\n",
      "iteration 53800, loss = 0.13581503654854993, prediction = [0.18988067 0.95718004 0.96055302 0.22016784]\n",
      "iteration 53850, loss = 0.1357462084165896, prediction = [0.18980337 0.95721413 0.96058465 0.22008099]\n",
      "iteration 53900, loss = 0.13567748250296718, prediction = [0.18972617 0.95724816 0.96061622 0.21999425]\n",
      "iteration 53950, loss = 0.13560885855996138, prediction = [0.18964907 0.95728213 0.96064773 0.21990762]\n",
      "iteration 54000, loss = 0.13554033634067858, prediction = [0.18957207 0.95731604 0.9606792  0.21982111]\n",
      "iteration 54050, loss = 0.13547191559904947, prediction = [0.18949517 0.9573499  0.96071061 0.2197347 ]\n",
      "iteration 54100, loss = 0.13540359608982624, prediction = [0.18941838 0.9573837  0.96074196 0.21964841]\n",
      "iteration 54150, loss = 0.13533537756857913, prediction = [0.18934168 0.95741744 0.96077327 0.21956223]\n",
      "iteration 54200, loss = 0.13526725979169177, prediction = [0.18926509 0.95745113 0.96080452 0.21947615]\n",
      "iteration 54250, loss = 0.1351992425163586, prediction = [0.18918859 0.95748476 0.96083571 0.21939019]\n",
      "iteration 54300, loss = 0.13513132550058216, prediction = [0.1891122  0.95751833 0.96086686 0.21930434]\n",
      "iteration 54350, loss = 0.13506350850316792, prediction = [0.1890359  0.95755185 0.96089795 0.2192186 ]\n",
      "iteration 54400, loss = 0.13499579128372283, prediction = [0.1889597  0.95758531 0.96092899 0.21913296]\n",
      "iteration 54450, loss = 0.13492817360264991, prediction = [0.18888361 0.95761871 0.96095998 0.21904744]\n",
      "iteration 54500, loss = 0.13486065522114693, prediction = [0.18880761 0.95765206 0.96099091 0.21896202]\n",
      "iteration 54550, loss = 0.1347932359012019, prediction = [0.18873171 0.95768535 0.96102179 0.21887672]\n",
      "iteration 54600, loss = 0.13472591540558923, prediction = [0.18865591 0.95771859 0.96105262 0.21879152]\n",
      "iteration 54650, loss = 0.13465869349786785, prediction = [0.18858021 0.95775177 0.9610834  0.21870643]\n",
      "iteration 54700, loss = 0.13459156994237653, prediction = [0.1885046  0.95778489 0.96111413 0.21862144]\n",
      "iteration 54750, loss = 0.13452454450423224, prediction = [0.18842909 0.95781796 0.96114481 0.21853657]\n",
      "iteration 54800, loss = 0.13445761694932512, prediction = [0.18835369 0.95785098 0.96117543 0.2184518 ]\n",
      "iteration 54850, loss = 0.13439078704431623, prediction = [0.18827837 0.95788394 0.96120601 0.21836714]\n",
      "iteration 54900, loss = 0.1343240545566344, prediction = [0.18820316 0.95791685 0.96123653 0.21828259]\n",
      "iteration 54950, loss = 0.13425741925447224, prediction = [0.18812804 0.9579497  0.961267   0.21819815]\n",
      "iteration 55000, loss = 0.1341908809067838, prediction = [0.18805302 0.95798249 0.96129742 0.21811381]\n",
      "iteration 55050, loss = 0.13412443928328108, prediction = [0.1879781  0.95801523 0.96132779 0.21802957]\n",
      "iteration 55100, loss = 0.13405809415443096, prediction = [0.18790327 0.95804792 0.96135811 0.21794545]\n",
      "iteration 55150, loss = 0.13399184529145156, prediction = [0.18782854 0.95808056 0.96138838 0.21786143]\n",
      "iteration 55200, loss = 0.13392569246630917, prediction = [0.1877539  0.95811313 0.9614186  0.21777751]\n",
      "iteration 55250, loss = 0.13385963545171678, prediction = [0.18767936 0.95814566 0.96144876 0.2176937 ]\n",
      "iteration 55300, loss = 0.13379367402112846, prediction = [0.18760492 0.95817813 0.96147888 0.21761   ]\n",
      "iteration 55350, loss = 0.13372780794873762, prediction = [0.18753057 0.95821055 0.96150895 0.2175264 ]\n",
      "iteration 55400, loss = 0.1336620370094741, prediction = [0.18745632 0.95824292 0.96153897 0.2174429 ]\n",
      "iteration 55450, loss = 0.13359636097900074, prediction = [0.18738216 0.95827523 0.96156894 0.21735951]\n",
      "iteration 55500, loss = 0.1335307796337102, prediction = [0.18730809 0.95830749 0.96159885 0.21727623]\n",
      "iteration 55550, loss = 0.13346529275072244, prediction = [0.18723412 0.95833969 0.96162872 0.21719305]\n",
      "iteration 55600, loss = 0.1333999001078816, prediction = [0.18716025 0.95837184 0.96165854 0.21710997]\n",
      "iteration 55650, loss = 0.133334601483752, prediction = [0.18708647 0.95840394 0.96168831 0.21702699]\n",
      "iteration 55700, loss = 0.1332693966576171, prediction = [0.18701278 0.95843599 0.96171804 0.21694412]\n",
      "iteration 55750, loss = 0.1332042854094748, prediction = [0.18693919 0.95846798 0.96174771 0.21686136]\n",
      "iteration 55800, loss = 0.1331392675200352, prediction = [0.18686569 0.95849993 0.96177733 0.21677869]\n",
      "iteration 55850, loss = 0.13307434277071742, prediction = [0.18679228 0.95853182 0.96180691 0.21669613]\n",
      "iteration 55900, loss = 0.13300951094364727, prediction = [0.18671897 0.95856365 0.96183643 0.21661367]\n",
      "iteration 55950, loss = 0.1329447718216538, prediction = [0.18664575 0.95859544 0.96186591 0.21653132]\n",
      "iteration 56000, loss = 0.13288012518826656, prediction = [0.18657262 0.95862717 0.96189534 0.21644906]\n",
      "iteration 56050, loss = 0.132815570827713, prediction = [0.18649958 0.95865886 0.96192472 0.21636691]\n",
      "iteration 56100, loss = 0.13275110852491445, prediction = [0.18642664 0.95869049 0.96195406 0.21628486]\n",
      "iteration 56150, loss = 0.13268673806548548, prediction = [0.18635379 0.95872207 0.96198334 0.21620291]\n",
      "iteration 56200, loss = 0.13262245923572852, prediction = [0.18628103 0.95875359 0.96201258 0.21612106]\n",
      "iteration 56250, loss = 0.1325582718226329, prediction = [0.18620836 0.95878507 0.96204177 0.21603932]\n",
      "iteration 56300, loss = 0.13249417561387145, prediction = [0.18613579 0.95881649 0.96207091 0.21595767]\n",
      "iteration 56350, loss = 0.13243017039779736, prediction = [0.1860633  0.95884787 0.96210001 0.21587613]\n",
      "iteration 56400, loss = 0.13236625596344193, prediction = [0.18599091 0.95887919 0.96212905 0.21579468]\n",
      "iteration 56450, loss = 0.1323024321005118, prediction = [0.18591861 0.95891047 0.96215805 0.21571334]\n",
      "iteration 56500, loss = 0.13223869859938533, prediction = [0.1858464  0.95894169 0.96218701 0.21563209]\n",
      "iteration 56550, loss = 0.13217505525111164, prediction = [0.18577428 0.95897286 0.96221591 0.21555095]\n",
      "iteration 56600, loss = 0.132111501847406, prediction = [0.18570225 0.95900398 0.96224477 0.2154699 ]\n",
      "iteration 56650, loss = 0.13204803818064814, prediction = [0.1856303  0.95903505 0.96227358 0.21538896]\n",
      "iteration 56700, loss = 0.13198466404387924, prediction = [0.18555845 0.95906607 0.96230235 0.21530811]\n",
      "iteration 56750, loss = 0.13192137923079933, prediction = [0.18548669 0.95909704 0.96233107 0.21522737]\n",
      "iteration 56800, loss = 0.13185818353576445, prediction = [0.18541502 0.95912797 0.96235974 0.21514672]\n",
      "iteration 56850, loss = 0.1317950767537849, prediction = [0.18534344 0.95915884 0.96238836 0.21506617]\n",
      "iteration 56900, loss = 0.1317320586805211, prediction = [0.18527195 0.95918966 0.96241694 0.21498572]\n",
      "iteration 56950, loss = 0.1316691291122823, prediction = [0.18520055 0.95922043 0.96244548 0.21490537]\n",
      "iteration 57000, loss = 0.13160628784602263, prediction = [0.18512923 0.95925116 0.96247396 0.21482511]\n",
      "iteration 57050, loss = 0.13154353467934085, prediction = [0.18505801 0.95928183 0.96250241 0.21474495]\n",
      "iteration 57100, loss = 0.1314808694104743, prediction = [0.18498687 0.95931245 0.9625308  0.21466489]\n",
      "iteration 57150, loss = 0.13141829183829914, prediction = [0.18491582 0.95934303 0.96255915 0.21458493]\n",
      "iteration 57200, loss = 0.13135580176232664, prediction = [0.18484486 0.95937356 0.96258746 0.21450507]\n",
      "iteration 57250, loss = 0.13129339898270143, prediction = [0.18477399 0.95940404 0.96261572 0.2144253 ]\n",
      "iteration 57300, loss = 0.1312310833001975, prediction = [0.1847032  0.95943447 0.96264393 0.21434563]\n",
      "iteration 57350, loss = 0.13116885451621707, prediction = [0.18463251 0.95946485 0.9626721  0.21426606]\n",
      "iteration 57400, loss = 0.13110671243278704, prediction = [0.1845619  0.95949518 0.96270022 0.21418658]\n",
      "iteration 57450, loss = 0.13104465685255762, prediction = [0.18449137 0.95952546 0.9627283  0.2141072 ]\n",
      "iteration 57500, loss = 0.13098268757879933, prediction = [0.18442094 0.9595557  0.96275633 0.21402791]\n",
      "iteration 57550, loss = 0.13092080441539922, prediction = [0.18435059 0.95958589 0.96278432 0.21394872]\n",
      "iteration 57600, loss = 0.13085900716686055, prediction = [0.18428033 0.95961603 0.96281227 0.21386963]\n",
      "iteration 57650, loss = 0.13079729563829914, prediction = [0.18421015 0.95964612 0.96284016 0.21379063]\n",
      "iteration 57700, loss = 0.13073566963544192, prediction = [0.18414006 0.95967617 0.96286802 0.21371172]\n",
      "iteration 57750, loss = 0.13067412896462227, prediction = [0.18407006 0.95970616 0.96289583 0.21363291]\n",
      "iteration 57800, loss = 0.1306126734327799, prediction = [0.18400014 0.95973611 0.9629236  0.2135542 ]\n",
      "iteration 57850, loss = 0.13055130284745775, prediction = [0.18393031 0.95976601 0.96295132 0.21347558]\n",
      "iteration 57900, loss = 0.13049001701679863, prediction = [0.18386056 0.95979587 0.962979   0.21339706]\n",
      "iteration 57950, loss = 0.13042881574954487, prediction = [0.1837909  0.95982568 0.96300663 0.21331863]\n",
      "iteration 58000, loss = 0.13036769885503394, prediction = [0.18372133 0.95985544 0.96303422 0.21324029]\n",
      "iteration 58050, loss = 0.13030666614319641, prediction = [0.18365184 0.95988515 0.96306177 0.21316205]\n",
      "iteration 58100, loss = 0.13024571742455537, prediction = [0.18358243 0.95991482 0.96308927 0.2130839 ]\n",
      "iteration 58150, loss = 0.13018485251022188, prediction = [0.18351311 0.95994444 0.96311673 0.21300584]\n",
      "iteration 58200, loss = 0.13012407121189376, prediction = [0.18344388 0.95997401 0.96314415 0.21292788]\n",
      "iteration 58250, loss = 0.13006337334185294, prediction = [0.18337472 0.96000354 0.96317152 0.21285001]\n",
      "iteration 58300, loss = 0.1300027587129634, prediction = [0.18330566 0.96003302 0.96319885 0.21277224]\n",
      "iteration 58350, loss = 0.1299422271386692, prediction = [0.18323667 0.96006246 0.96322614 0.21269455]\n",
      "iteration 58400, loss = 0.12988177843299087, prediction = [0.18316777 0.96009185 0.96325338 0.21261696]\n",
      "iteration 58450, loss = 0.12982141241052447, prediction = [0.18309896 0.96012119 0.96328058 0.21253947]\n",
      "iteration 58500, loss = 0.12976112888643848, prediction = [0.18303023 0.96015049 0.96330774 0.21246206]\n",
      "iteration 58550, loss = 0.1297009276764733, prediction = [0.18296158 0.96017974 0.96333485 0.21238475]\n",
      "iteration 58600, loss = 0.12964080859693572, prediction = [0.18289301 0.96020894 0.96336193 0.21230752]\n",
      "iteration 58650, loss = 0.12958077146469976, prediction = [0.18282453 0.9602381  0.96338896 0.21223039]\n",
      "iteration 58700, loss = 0.12952081609720373, prediction = [0.18275613 0.96026722 0.96341595 0.21215335]\n",
      "iteration 58750, loss = 0.12946094231244593, prediction = [0.18268781 0.96029629 0.96344289 0.21207641]\n",
      "iteration 58800, loss = 0.12940114992898585, prediction = [0.18261958 0.96032531 0.9634698  0.21199955]\n",
      "iteration 58850, loss = 0.12934143876593934, prediction = [0.18255142 0.96035429 0.96349666 0.21192278]\n",
      "iteration 58900, loss = 0.1292818086429777, prediction = [0.18248335 0.96038322 0.96352348 0.21184611]\n",
      "iteration 58950, loss = 0.12922225938032475, prediction = [0.18241536 0.96041211 0.96355025 0.21176952]\n",
      "iteration 59000, loss = 0.12916279079875564, prediction = [0.18234746 0.96044096 0.96357699 0.21169303]\n",
      "iteration 59050, loss = 0.12910340271959408, prediction = [0.18227963 0.96046976 0.96360368 0.21161663]\n",
      "iteration 59100, loss = 0.12904409496470987, prediction = [0.18221189 0.96049851 0.96363034 0.21154031]\n",
      "iteration 59150, loss = 0.1289848673565171, prediction = [0.18214423 0.96052722 0.96365695 0.21146409]\n",
      "iteration 59200, loss = 0.12892571971797234, prediction = [0.18207665 0.96055589 0.96368352 0.21138795]\n",
      "iteration 59250, loss = 0.12886665187257257, prediction = [0.18200915 0.96058451 0.96371005 0.21131191]\n",
      "iteration 59300, loss = 0.1288076636443528, prediction = [0.18194173 0.96061309 0.96373653 0.21123595]\n",
      "iteration 59350, loss = 0.12874875485788334, prediction = [0.18187439 0.96064162 0.96376298 0.21116008]\n",
      "iteration 59400, loss = 0.12868992533826923, prediction = [0.18180713 0.96067011 0.96378939 0.21108431]\n",
      "iteration 59450, loss = 0.1286311749111463, prediction = [0.18173995 0.96069855 0.96381575 0.21100862]\n",
      "iteration 59500, loss = 0.12857250340268128, prediction = [0.18167286 0.96072696 0.96384207 0.21093302]\n",
      "iteration 59550, loss = 0.12851391063956807, prediction = [0.18160584 0.96075531 0.96386836 0.21085751]\n",
      "iteration 59600, loss = 0.1284553964490256, prediction = [0.1815389  0.96078363 0.9638946  0.21078208]\n",
      "iteration 59650, loss = 0.12839696065879752, prediction = [0.18147204 0.9608119  0.9639208  0.21070675]\n",
      "iteration 59700, loss = 0.12833860309714884, prediction = [0.18140527 0.96084013 0.96394696 0.2106315 ]\n",
      "iteration 59750, loss = 0.1282803235928636, prediction = [0.18133857 0.96086831 0.96397308 0.21055634]\n",
      "iteration 59800, loss = 0.1282221219752438, prediction = [0.18127195 0.96089645 0.96399917 0.21048126]\n",
      "iteration 59850, loss = 0.1281639980741075, prediction = [0.18120541 0.96092455 0.96402521 0.21040628]\n",
      "iteration 59900, loss = 0.1281059517197855, prediction = [0.18113895 0.96095261 0.96405121 0.21033138]\n",
      "iteration 59950, loss = 0.12804798274312085, prediction = [0.18107256 0.96098062 0.96407717 0.21025657]\n",
      "iteration 60000, loss = 0.1279900909754665, prediction = [0.18100626 0.96100859 0.96410309 0.21018185]\n",
      "iteration 60050, loss = 0.1279322762486828, prediction = [0.18094003 0.96103652 0.96412897 0.21010721]\n",
      "iteration 60100, loss = 0.12787453839513557, prediction = [0.18087389 0.9610644  0.96415481 0.21003266]\n",
      "iteration 60150, loss = 0.12781687724769517, prediction = [0.18080782 0.96109224 0.96418062 0.20995819]\n",
      "iteration 60200, loss = 0.12775929263973315, prediction = [0.18074182 0.96112004 0.96420638 0.20988381]\n",
      "iteration 60250, loss = 0.12770178440512223, prediction = [0.18067591 0.9611478  0.9642321  0.20980952]\n",
      "iteration 60300, loss = 0.127644352378232, prediction = [0.18061007 0.96117551 0.96425779 0.20973531]\n",
      "iteration 60350, loss = 0.1275869963939293, prediction = [0.18054432 0.96120319 0.96428343 0.20966119]\n",
      "iteration 60400, loss = 0.12752971628757376, prediction = [0.18047863 0.96123082 0.96430904 0.20958716]\n",
      "iteration 60450, loss = 0.12747251189501937, prediction = [0.18041303 0.96125841 0.9643346  0.20951321]\n",
      "iteration 60500, loss = 0.12741538305260955, prediction = [0.1803475  0.96128596 0.96436013 0.20943934]\n",
      "iteration 60550, loss = 0.12735832959717622, prediction = [0.18028205 0.96131346 0.96438562 0.20936556]\n",
      "iteration 60600, loss = 0.12730135136603898, prediction = [0.18021668 0.96134093 0.96441107 0.20929187]\n",
      "iteration 60650, loss = 0.12724444819700198, prediction = [0.18015138 0.96136835 0.96443648 0.20921826]\n",
      "iteration 60700, loss = 0.12718761992835234, prediction = [0.18008616 0.96139573 0.96446186 0.20914473]\n",
      "iteration 60750, loss = 0.12713086639885865, prediction = [0.18002102 0.96142307 0.96448719 0.20907129]\n",
      "iteration 60800, loss = 0.12707418744776897, prediction = [0.17995595 0.96145037 0.96451249 0.20899793]\n",
      "iteration 60850, loss = 0.12701758291480908, prediction = [0.17989096 0.96147763 0.96453775 0.20892466]\n",
      "iteration 60900, loss = 0.12696105264018084, prediction = [0.17982604 0.96150485 0.96456297 0.20885147]\n",
      "iteration 60950, loss = 0.1269045964645597, prediction = [0.1797612  0.96153202 0.96458815 0.20877836]\n",
      "iteration 61000, loss = 0.12684821422909384, prediction = [0.17969644 0.96155916 0.96461329 0.20870534]\n",
      "iteration 61050, loss = 0.1267919057754015, prediction = [0.17963175 0.96158625 0.9646384  0.2086324 ]\n",
      "iteration 61100, loss = 0.12673567094557037, prediction = [0.17956714 0.9616133  0.96466347 0.20855954]\n",
      "iteration 61150, loss = 0.1266795095821542, prediction = [0.1795026  0.96164032 0.9646885  0.20848677]\n",
      "iteration 61200, loss = 0.12662342152817269, prediction = [0.17943813 0.96166729 0.96471349 0.20841408]\n",
      "iteration 61250, loss = 0.12656740662710825, prediction = [0.17937374 0.96169422 0.96473844 0.20834147]\n",
      "iteration 61300, loss = 0.12651146472290606, prediction = [0.17930943 0.96172112 0.96476336 0.20826895]\n",
      "iteration 61350, loss = 0.12645559565997017, prediction = [0.17924519 0.96174797 0.96478824 0.20819651]\n",
      "iteration 61400, loss = 0.12639979928316342, prediction = [0.17918102 0.96177478 0.96481308 0.20812415]\n",
      "iteration 61450, loss = 0.1263440754378053, prediction = [0.17911693 0.96180155 0.96483789 0.20805187]\n",
      "iteration 61500, loss = 0.12628842396966997, prediction = [0.17905291 0.96182828 0.96486266 0.20797967]\n",
      "iteration 61550, loss = 0.12623284472498486, prediction = [0.17898897 0.96185498 0.96488739 0.20790756]\n",
      "iteration 61600, loss = 0.1261773375504282, prediction = [0.1789251  0.96188163 0.96491208 0.20783552]\n",
      "iteration 61650, loss = 0.12612190229312886, prediction = [0.1788613  0.96190824 0.96493674 0.20776357]\n",
      "iteration 61700, loss = 0.126066538800663, prediction = [0.17879758 0.96193482 0.96496136 0.2076917 ]\n",
      "iteration 61750, loss = 0.1260112469210542, prediction = [0.17873393 0.96196135 0.96498595 0.20761991]\n",
      "iteration 61800, loss = 0.1259560265027692, prediction = [0.17867036 0.96198784 0.96501049 0.20754821]\n",
      "iteration 61850, loss = 0.12590087739471925, prediction = [0.17860685 0.9620143  0.965035   0.20747658]\n",
      "iteration 61900, loss = 0.12584579944625615, prediction = [0.17854342 0.96204072 0.96505948 0.20740503]\n",
      "iteration 61950, loss = 0.12579079250717273, prediction = [0.17848007 0.96206709 0.96508392 0.20733357]\n",
      "iteration 62000, loss = 0.12573585642769777, prediction = [0.17841678 0.96209343 0.96510832 0.20726218]\n",
      "iteration 62050, loss = 0.12568099105849861, prediction = [0.17835357 0.96211973 0.96513268 0.20719088]\n",
      "iteration 62100, loss = 0.12562619625067634, prediction = [0.17829043 0.96214599 0.96515701 0.20711965]\n",
      "iteration 62150, loss = 0.12557147185576578, prediction = [0.17822736 0.96217221 0.9651813  0.20704851]\n",
      "iteration 62200, loss = 0.1255168177257339, prediction = [0.17816437 0.9621984  0.96520556 0.20697744]\n",
      "iteration 62250, loss = 0.1254622337129764, prediction = [0.17810144 0.96222454 0.96522978 0.20690645]\n",
      "iteration 62300, loss = 0.1254077196703187, prediction = [0.17803859 0.96225065 0.96525397 0.20683555]\n",
      "iteration 62350, loss = 0.12535327545101305, prediction = [0.17797581 0.96227671 0.96527812 0.20676472]\n",
      "iteration 62400, loss = 0.12529890090873638, prediction = [0.1779131  0.96230274 0.96530223 0.20669397]\n",
      "iteration 62450, loss = 0.12524459589758935, prediction = [0.17785046 0.96232873 0.96532631 0.2066233 ]\n",
      "iteration 62500, loss = 0.1251903602720958, prediction = [0.1777879  0.96235469 0.96535035 0.20655271]\n",
      "iteration 62550, loss = 0.12513619388719935, prediction = [0.1777254  0.9623806  0.96537436 0.2064822 ]\n",
      "iteration 62600, loss = 0.12508209659826355, prediction = [0.17766298 0.96240648 0.96539833 0.20641177]\n",
      "iteration 62650, loss = 0.12502806826106794, prediction = [0.17760063 0.96243232 0.96542227 0.20634141]\n",
      "iteration 62700, loss = 0.12497410873180967, prediction = [0.17753835 0.96245812 0.96544617 0.20627114]\n",
      "iteration 62750, loss = 0.12492021786709968, prediction = [0.17747613 0.96248388 0.96547004 0.20620094]\n",
      "iteration 62800, loss = 0.12486639552396218, prediction = [0.17741399 0.96250961 0.96549387 0.20613082]\n",
      "iteration 62850, loss = 0.12481264155983218, prediction = [0.17735192 0.9625353  0.96551767 0.20606078]\n",
      "iteration 62900, loss = 0.12475895583255542, prediction = [0.17728992 0.96256095 0.96554143 0.20599081]\n",
      "iteration 62950, loss = 0.12470533820038573, prediction = [0.17722799 0.96258656 0.96556515 0.20592093]\n",
      "iteration 63000, loss = 0.12465178852198418, prediction = [0.17716613 0.96261214 0.96558885 0.20585112]\n",
      "iteration 63050, loss = 0.12459830665641689, prediction = [0.17710434 0.96263768 0.9656125  0.20578139]\n",
      "iteration 63100, loss = 0.12454489246315395, prediction = [0.17704262 0.96266318 0.96563613 0.20571173]\n",
      "iteration 63150, loss = 0.12449154580206873, prediction = [0.17698097 0.96268865 0.96565972 0.20564215]\n",
      "iteration 63200, loss = 0.12443826653343451, prediction = [0.17691938 0.96271408 0.96568327 0.20557265]\n",
      "iteration 63250, loss = 0.12438505451792528, prediction = [0.17685787 0.96273947 0.96570679 0.20550323]\n",
      "iteration 63300, loss = 0.12433190961661221, prediction = [0.17679643 0.96276482 0.96573027 0.20543388]\n",
      "iteration 63350, loss = 0.12427883169096376, prediction = [0.17673505 0.96279014 0.96575373 0.20536461]\n",
      "iteration 63400, loss = 0.12422582060284343, prediction = [0.17667374 0.96281542 0.96577714 0.20529541]\n",
      "iteration 63450, loss = 0.12417287621450848, prediction = [0.17661251 0.96284066 0.96580053 0.20522629]\n",
      "iteration 63500, loss = 0.12411999838860857, prediction = [0.17655134 0.96286587 0.96582388 0.20515725]\n",
      "iteration 63550, loss = 0.12406718698818454, prediction = [0.17649024 0.96289104 0.96584719 0.20508828]\n",
      "iteration 63600, loss = 0.12401444187666652, prediction = [0.17642921 0.96291618 0.96587047 0.20501939]\n",
      "iteration 63650, loss = 0.12396176291787259, prediction = [0.17636824 0.96294128 0.96589372 0.20495057]\n",
      "iteration 63700, loss = 0.12390914997600791, prediction = [0.17630735 0.96296634 0.96591693 0.20488183]\n",
      "iteration 63750, loss = 0.12385660291566278, prediction = [0.17624652 0.96299137 0.96594012 0.20481316]\n",
      "iteration 63800, loss = 0.12380412160181153, prediction = [0.17618576 0.96301636 0.96596326 0.20474457]\n",
      "iteration 63850, loss = 0.12375170589981072, prediction = [0.17612507 0.96304132 0.96598638 0.20467606]\n",
      "iteration 63900, loss = 0.12369935567539861, prediction = [0.17606444 0.96306624 0.96600946 0.20460762]\n",
      "iteration 63950, loss = 0.12364707079469286, prediction = [0.17600388 0.96309112 0.9660325  0.20453925]\n",
      "iteration 64000, loss = 0.12359485112418993, prediction = [0.17594339 0.96311597 0.96605552 0.20447096]\n",
      "iteration 64050, loss = 0.12354269653076275, prediction = [0.17588297 0.96314078 0.9660785  0.20440274]\n",
      "iteration 64100, loss = 0.12349060688166033, prediction = [0.17582261 0.96316556 0.96610145 0.2043346 ]\n",
      "iteration 64150, loss = 0.12343858204450583, prediction = [0.17576233 0.9631903  0.96612436 0.20426653]\n",
      "iteration 64200, loss = 0.12338662188729588, prediction = [0.1757021  0.963215   0.96614724 0.20419853]\n",
      "iteration 64250, loss = 0.12333472627839837, prediction = [0.17564195 0.96323967 0.96617009 0.20413061]\n",
      "iteration 64300, loss = 0.12328289508655128, prediction = [0.17558186 0.96326431 0.96619291 0.20406277]\n",
      "iteration 64350, loss = 0.12323112818086271, prediction = [0.17552184 0.96328891 0.96621569 0.20399499]\n",
      "iteration 64400, loss = 0.12317942543080712, prediction = [0.17546188 0.96331348 0.96623844 0.20392729]\n",
      "iteration 64450, loss = 0.12312778670622594, prediction = [0.17540199 0.96333801 0.96626116 0.20385966]\n",
      "iteration 64500, loss = 0.12307621187732617, prediction = [0.17534217 0.9633625  0.96628385 0.20379211]\n",
      "iteration 64550, loss = 0.12302470081467717, prediction = [0.17528241 0.96338696 0.9663065  0.20372463]\n",
      "iteration 64600, loss = 0.12297325338921236, prediction = [0.17522272 0.96341139 0.96632912 0.20365722]\n",
      "iteration 64650, loss = 0.12292186947222491, prediction = [0.17516309 0.96343578 0.96635171 0.20358989]\n",
      "iteration 64700, loss = 0.12287054893536933, prediction = [0.17510353 0.96346014 0.96637427 0.20352263]\n",
      "iteration 64750, loss = 0.12281929165065755, prediction = [0.17504404 0.96348446 0.96639679 0.20345544]\n",
      "iteration 64800, loss = 0.12276809749045936, prediction = [0.17498461 0.96350875 0.96641928 0.20338832]\n",
      "iteration 64850, loss = 0.12271696632750048, prediction = [0.17492524 0.963533   0.96644174 0.20332127]\n",
      "iteration 64900, loss = 0.12266589803486129, prediction = [0.17486594 0.96355722 0.96646417 0.2032543 ]\n",
      "iteration 64950, loss = 0.12261489248597651, prediction = [0.17480671 0.9635814  0.96648657 0.2031874 ]\n",
      "iteration 65000, loss = 0.12256394955463179, prediction = [0.17474754 0.96360555 0.96650893 0.20312057]\n",
      "iteration 65050, loss = 0.12251306911496472, prediction = [0.17468843 0.96362967 0.96653127 0.20305381]\n",
      "iteration 65100, loss = 0.12246225104146258, prediction = [0.17462939 0.96365375 0.96655357 0.20298713]\n",
      "iteration 65150, loss = 0.12241149520896097, prediction = [0.17457041 0.9636778  0.96657584 0.20292052]\n",
      "iteration 65200, loss = 0.12236080149264299, prediction = [0.1745115  0.96370182 0.96659808 0.20285397]\n",
      "iteration 65250, loss = 0.12231016976803795, prediction = [0.17445266 0.9637258  0.96662028 0.2027875 ]\n",
      "iteration 65300, loss = 0.122259599911019, prediction = [0.17439387 0.96374975 0.96664246 0.2027211 ]\n",
      "iteration 65350, loss = 0.12220909179780468, prediction = [0.17433515 0.96377366 0.9666646  0.20265477]\n",
      "iteration 65400, loss = 0.12215864530495474, prediction = [0.1742765  0.96379754 0.96668672 0.20258851]\n",
      "iteration 65450, loss = 0.12210826030937055, prediction = [0.17421791 0.96382139 0.9667088  0.20252232]\n",
      "iteration 65500, loss = 0.12205793668829318, prediction = [0.17415938 0.9638452  0.96673085 0.20245621]\n",
      "iteration 65550, loss = 0.12200767431930298, prediction = [0.17410091 0.96386898 0.96675287 0.20239016]\n",
      "iteration 65600, loss = 0.12195747308031787, prediction = [0.17404251 0.96389273 0.96677486 0.20232418]\n",
      "iteration 65650, loss = 0.12190733284959174, prediction = [0.17398417 0.96391644 0.96679681 0.20225828]\n",
      "iteration 65700, loss = 0.12185725350571475, prediction = [0.1739259  0.96394012 0.96681874 0.20219244]\n",
      "iteration 65750, loss = 0.12180723492761004, prediction = [0.17386769 0.96396377 0.96684064 0.20212668]\n",
      "iteration 65800, loss = 0.12175727699453456, prediction = [0.17380954 0.96398738 0.9668625  0.20206098]\n",
      "iteration 65850, loss = 0.12170737958607596, prediction = [0.17375146 0.96401096 0.96688434 0.20199535]\n",
      "iteration 65900, loss = 0.12165754258215401, prediction = [0.17369343 0.96403451 0.96690614 0.20192979]\n",
      "iteration 65950, loss = 0.12160776586301664, prediction = [0.17363547 0.96405803 0.96692792 0.20186431]\n",
      "iteration 66000, loss = 0.12155804930924065, prediction = [0.17357757 0.96408151 0.96694966 0.20179889]\n",
      "iteration 66050, loss = 0.12150839280173009, prediction = [0.17351974 0.96410496 0.96697137 0.20173354]\n",
      "iteration 66100, loss = 0.12145879622171404, prediction = [0.17346197 0.96412838 0.96699306 0.20166826]\n",
      "iteration 66150, loss = 0.12140925945074812, prediction = [0.17340426 0.96415176 0.96701471 0.20160305]\n",
      "iteration 66200, loss = 0.1213597823707099, prediction = [0.17334661 0.96417512 0.96703633 0.20153791]\n",
      "iteration 66250, loss = 0.12131036486380099, prediction = [0.17328902 0.96419844 0.96705792 0.20147283]\n",
      "iteration 66300, loss = 0.12126100681254393, prediction = [0.1732315  0.96422172 0.96707948 0.20140783]\n",
      "iteration 66350, loss = 0.12121170809978174, prediction = [0.17317403 0.96424498 0.96710102 0.20134289]\n",
      "iteration 66400, loss = 0.12116246860867605, prediction = [0.17311663 0.9642682  0.96712252 0.20127802]\n",
      "iteration 66450, loss = 0.12111328822270777, prediction = [0.17305929 0.9642914  0.96714399 0.20121322]\n",
      "iteration 66500, loss = 0.12106416682567395, prediction = [0.17300201 0.96431455 0.96716543 0.20114849]\n",
      "iteration 66550, loss = 0.12101510430168838, prediction = [0.17294479 0.96433768 0.96718685 0.20108383]\n",
      "iteration 66600, loss = 0.12096610053517828, prediction = [0.17288764 0.96436078 0.96720823 0.20101923]\n",
      "iteration 66650, loss = 0.12091715541088655, prediction = [0.17283054 0.96438384 0.96722958 0.2009547 ]\n",
      "iteration 66700, loss = 0.12086826881386757, prediction = [0.17277351 0.96440687 0.96725091 0.20089024]\n",
      "iteration 66750, loss = 0.12081944062948737, prediction = [0.17271654 0.96442988 0.9672722  0.20082585]\n",
      "iteration 66800, loss = 0.12077067074342243, prediction = [0.17265962 0.96445284 0.96729347 0.20076152]\n",
      "iteration 66850, loss = 0.12072195904165943, prediction = [0.17260277 0.96447578 0.9673147  0.20069726]\n",
      "iteration 66900, loss = 0.12067330541049298, prediction = [0.17254598 0.96449869 0.96733591 0.20063307]\n",
      "iteration 66950, loss = 0.12062470973652455, prediction = [0.17248925 0.96452156 0.96735708 0.20056895]\n",
      "iteration 67000, loss = 0.12057617190666277, prediction = [0.17243258 0.9645444  0.96737823 0.20050489]\n",
      "iteration 67050, loss = 0.12052769180812084, prediction = [0.17237597 0.96456722 0.96739935 0.2004409 ]\n",
      "iteration 67100, loss = 0.12047926932841617, prediction = [0.17231942 0.96459    0.96742044 0.20037697]\n",
      "iteration 67150, loss = 0.12043090435536932, prediction = [0.17226292 0.96461275 0.9674415  0.20031312]\n",
      "iteration 67200, loss = 0.12038259677710325, prediction = [0.17220649 0.96463546 0.96746253 0.20024933]\n",
      "iteration 67250, loss = 0.12033434648204212, prediction = [0.17215012 0.96465815 0.96748354 0.2001856 ]\n",
      "iteration 67300, loss = 0.1202861533589089, prediction = [0.17209381 0.96468081 0.96750451 0.20012194]\n",
      "iteration 67350, loss = 0.12023801729672697, prediction = [0.17203756 0.96470343 0.96752546 0.20005835]\n",
      "iteration 67400, loss = 0.12018993818481695, prediction = [0.17198137 0.96472603 0.96754637 0.19999482]\n",
      "iteration 67450, loss = 0.12014191591279635, prediction = [0.17192523 0.96474859 0.96756726 0.19993136]\n",
      "iteration 67500, loss = 0.12009395037057878, prediction = [0.17186916 0.96477112 0.96758812 0.19986797]\n",
      "iteration 67550, loss = 0.12004604144837278, prediction = [0.17181315 0.96479363 0.96760895 0.19980464]\n",
      "iteration 67600, loss = 0.1199981890366808, prediction = [0.17175719 0.9648161  0.96762975 0.19974137]\n",
      "iteration 67650, loss = 0.11995039302629792, prediction = [0.17170129 0.96483854 0.96765053 0.19967817]\n",
      "iteration 67700, loss = 0.11990265330831149, prediction = [0.17164546 0.96486095 0.96767127 0.19961504]\n",
      "iteration 67750, loss = 0.11985496977409985, prediction = [0.17158968 0.96488333 0.96769199 0.19955197]\n",
      "iteration 67800, loss = 0.11980734231533119, prediction = [0.17153396 0.96490568 0.96771268 0.19948897]\n",
      "iteration 67850, loss = 0.11975977082396191, prediction = [0.17147829 0.964928   0.96773334 0.19942603]\n",
      "iteration 67900, loss = 0.11971225519223744, prediction = [0.17142269 0.96495029 0.96775397 0.19936316]\n",
      "iteration 67950, loss = 0.11966479531268912, prediction = [0.17136714 0.96497255 0.96777458 0.19930035]\n",
      "iteration 68000, loss = 0.11961739107813557, prediction = [0.17131166 0.96499478 0.96779516 0.19923761]\n",
      "iteration 68050, loss = 0.11957004238167909, prediction = [0.17125623 0.96501698 0.96781571 0.19917493]\n",
      "iteration 68100, loss = 0.11952274911670736, prediction = [0.17120086 0.96503915 0.96783623 0.19911231]\n",
      "iteration 68150, loss = 0.1194755111768899, prediction = [0.17114554 0.96506129 0.96785672 0.19904976]\n",
      "iteration 68200, loss = 0.11942832845617941, prediction = [0.17109029 0.9650834  0.96787719 0.19898727]\n",
      "iteration 68250, loss = 0.119381200848809, prediction = [0.17103509 0.96510548 0.96789762 0.19892485]\n",
      "iteration 68300, loss = 0.11933412824929307, prediction = [0.17097995 0.96512753 0.96791803 0.19886249]\n",
      "iteration 68350, loss = 0.11928711055242386, prediction = [0.17092486 0.96514955 0.96793842 0.1988002 ]\n",
      "iteration 68400, loss = 0.11924014765327307, prediction = [0.17086984 0.96517154 0.96795877 0.19873796]\n",
      "iteration 68450, loss = 0.11919323944718913, prediction = [0.17081487 0.9651935  0.9679791  0.1986758 ]\n",
      "iteration 68500, loss = 0.11914638582979803, prediction = [0.17075996 0.96521543 0.9679994  0.19861369]\n",
      "iteration 68550, loss = 0.11909958669700037, prediction = [0.17070511 0.96523734 0.96801967 0.19855165]\n",
      "iteration 68600, loss = 0.11905284194497134, prediction = [0.17065031 0.96525921 0.96803992 0.19848967]\n",
      "iteration 68650, loss = 0.11900615147016008, prediction = [0.17059557 0.96528105 0.96806014 0.19842776]\n",
      "iteration 68700, loss = 0.11895951516928886, prediction = [0.17054088 0.96530287 0.96808033 0.19836591]\n",
      "iteration 68750, loss = 0.11891293293935093, prediction = [0.17048626 0.96532465 0.96810049 0.19830412]\n",
      "iteration 68800, loss = 0.11886640467761153, prediction = [0.17043169 0.96534641 0.96812063 0.19824239]\n",
      "iteration 68850, loss = 0.11881993028160577, prediction = [0.17037717 0.96536814 0.96814074 0.19818073]\n",
      "iteration 68900, loss = 0.11877350964913638, prediction = [0.17032272 0.96538984 0.96816082 0.19811913]\n",
      "iteration 68950, loss = 0.11872714267827686, prediction = [0.17026831 0.96541151 0.96818088 0.19805759]\n",
      "iteration 69000, loss = 0.11868082926736638, prediction = [0.17021397 0.96543315 0.96820091 0.19799612]\n",
      "iteration 69050, loss = 0.1186345693150109, prediction = [0.17015968 0.96545476 0.96822091 0.1979347 ]\n",
      "iteration 69100, loss = 0.118588362720082, prediction = [0.17010545 0.96547634 0.96824089 0.19787335]\n",
      "iteration 69150, loss = 0.11854220938171581, prediction = [0.17005127 0.9654979  0.96826084 0.19781206]\n",
      "iteration 69200, loss = 0.11849610919931243, prediction = [0.16999715 0.96551942 0.96828076 0.19775083]\n",
      "iteration 69250, loss = 0.1184500620725346, prediction = [0.16994308 0.96554092 0.96830065 0.19768967]\n",
      "iteration 69300, loss = 0.11840406790130781, prediction = [0.16988907 0.96556239 0.96832052 0.19762857]\n",
      "iteration 69350, loss = 0.11835812658581751, prediction = [0.16983512 0.96558383 0.96834037 0.19756752]\n",
      "iteration 69400, loss = 0.11831223802651059, prediction = [0.16978122 0.96560524 0.96836018 0.19750654]\n",
      "iteration 69450, loss = 0.118266402124093, prediction = [0.16972738 0.96562662 0.96837997 0.19744563]\n",
      "iteration 69500, loss = 0.11822061877952914, prediction = [0.16967359 0.96564798 0.96839974 0.19738477]\n",
      "iteration 69550, loss = 0.11817488789404101, prediction = [0.16961985 0.96566931 0.96841948 0.19732397]\n",
      "iteration 69600, loss = 0.11812920936910823, prediction = [0.16956617 0.96569061 0.96843919 0.19726324]\n",
      "iteration 69650, loss = 0.11808358310646544, prediction = [0.16951255 0.96571188 0.96845887 0.19720256]\n",
      "iteration 69700, loss = 0.11803800900810243, prediction = [0.16945898 0.96573312 0.96847853 0.19714195]\n",
      "iteration 69750, loss = 0.11799248697626437, prediction = [0.16940547 0.96575433 0.96849816 0.1970814 ]\n",
      "iteration 69800, loss = 0.11794701691344904, prediction = [0.16935201 0.96577552 0.96851777 0.1970209 ]\n",
      "iteration 69850, loss = 0.11790159872240732, prediction = [0.1692986  0.96579668 0.96853735 0.19696047]\n",
      "iteration 69900, loss = 0.11785623230614116, prediction = [0.16924525 0.96581781 0.96855691 0.1969001 ]\n",
      "iteration 69950, loss = 0.11781091756790457, prediction = [0.16919196 0.96583891 0.96857644 0.19683979]\n",
      "iteration 70000, loss = 0.11776565441120035, prediction = [0.16913872 0.96585999 0.96859594 0.19677954]\n",
      "iteration 70050, loss = 0.11772044273978206, prediction = [0.16908553 0.96588104 0.96861542 0.19671935]\n",
      "iteration 70100, loss = 0.11767528245765088, prediction = [0.16903239 0.96590206 0.96863487 0.19665922]\n",
      "iteration 70150, loss = 0.1176301734690559, prediction = [0.16897931 0.96592305 0.9686543  0.19659915]\n",
      "iteration 70200, loss = 0.11758511567849272, prediction = [0.16892629 0.96594402 0.9686737  0.19653914]\n",
      "iteration 70250, loss = 0.11754010899070433, prediction = [0.16887332 0.96596495 0.96869307 0.19647919]\n",
      "iteration 70300, loss = 0.11749515331067706, prediction = [0.1688204  0.96598586 0.96871243 0.1964193 ]\n",
      "iteration 70350, loss = 0.11745024854364298, prediction = [0.16876753 0.96600675 0.96873175 0.19635947]\n",
      "iteration 70400, loss = 0.11740539459507689, prediction = [0.16871472 0.9660276  0.96875105 0.1962997 ]\n",
      "iteration 70450, loss = 0.11736059137069813, prediction = [0.16866196 0.96604843 0.96877032 0.19623999]\n",
      "iteration 70500, loss = 0.11731583877646587, prediction = [0.16860926 0.96606923 0.96878957 0.19618034]\n",
      "iteration 70550, loss = 0.11727113671858207, prediction = [0.16855661 0.96609001 0.9688088  0.19612074]\n",
      "iteration 70600, loss = 0.11722648510348874, prediction = [0.16850401 0.96611075 0.96882799 0.19606121]\n",
      "iteration 70650, loss = 0.11718188383786735, prediction = [0.16845147 0.96613147 0.96884717 0.19600173]\n",
      "iteration 70700, loss = 0.11713733282863856, prediction = [0.16839897 0.96615217 0.96886632 0.19594232]\n",
      "iteration 70750, loss = 0.1170928319829613, prediction = [0.16834654 0.96617283 0.96888544 0.19588296]\n",
      "iteration 70800, loss = 0.11704838120823108, prediction = [0.16829415 0.96619347 0.96890454 0.19582366]\n",
      "iteration 70850, loss = 0.11700398041208067, prediction = [0.16824182 0.96621409 0.96892361 0.19576442]\n",
      "iteration 70900, loss = 0.11695962950237881, prediction = [0.16818953 0.96623467 0.96894266 0.19570524]\n",
      "iteration 70950, loss = 0.11691532838722882, prediction = [0.16813731 0.96625523 0.96896168 0.19564612]\n",
      "iteration 71000, loss = 0.11687107697496828, prediction = [0.16808513 0.96627576 0.96898068 0.19558705]\n",
      "iteration 71050, loss = 0.11682687517416829, prediction = [0.16803301 0.96629627 0.96899965 0.19552804]\n",
      "iteration 71100, loss = 0.11678272289363316, prediction = [0.16798093 0.96631675 0.9690186  0.1954691 ]\n",
      "iteration 71150, loss = 0.11673862004239882, prediction = [0.16792892 0.9663372  0.96903753 0.1954102 ]\n",
      "iteration 71200, loss = 0.11669456652973205, prediction = [0.16787695 0.96635763 0.96905643 0.19535137]\n",
      "iteration 71250, loss = 0.11665056226513121, prediction = [0.16782503 0.96637803 0.96907531 0.1952926 ]\n",
      "iteration 71300, loss = 0.11660660715832359, prediction = [0.16777317 0.9663984  0.96909416 0.19523388]\n",
      "iteration 71350, loss = 0.11656270111926564, prediction = [0.16772136 0.96641875 0.96911298 0.19517522]\n",
      "iteration 71400, loss = 0.11651884405814193, prediction = [0.1676696  0.96643907 0.96913179 0.19511662]\n",
      "iteration 71450, loss = 0.11647503588536479, prediction = [0.16761789 0.96645937 0.96915056 0.19505807]\n",
      "iteration 71500, loss = 0.11643127651157348, prediction = [0.16756623 0.96647964 0.96916932 0.19499959]\n",
      "iteration 71550, loss = 0.11638756584763289, prediction = [0.16751463 0.96649988 0.96918805 0.19494116]\n",
      "iteration 71600, loss = 0.11634390380463373, prediction = [0.16746308 0.9665201  0.96920675 0.19488278]\n",
      "iteration 71650, loss = 0.116300290293891, prediction = [0.16741157 0.96654029 0.96922543 0.19482447]\n",
      "iteration 71700, loss = 0.11625672522694364, prediction = [0.16736012 0.96656045 0.96924409 0.19476621]\n",
      "iteration 71750, loss = 0.11621320851555356, prediction = [0.16730872 0.96658059 0.96926272 0.19470801]\n",
      "iteration 71800, loss = 0.11616974007170532, prediction = [0.16725737 0.9666007  0.96928133 0.19464986]\n",
      "iteration 71850, loss = 0.11612631980760538, prediction = [0.16720608 0.96662079 0.96929992 0.19459177]\n",
      "iteration 71900, loss = 0.11608294763568139, prediction = [0.16715483 0.96664085 0.96931848 0.19453374]\n",
      "iteration 71950, loss = 0.11603962346858054, prediction = [0.16710363 0.96666089 0.96933702 0.19447576]\n",
      "iteration 72000, loss = 0.11599634721917057, prediction = [0.16705249 0.9666809  0.96935553 0.19441784]\n",
      "iteration 72050, loss = 0.11595311880053677, prediction = [0.16700139 0.96670089 0.96937402 0.19435998]\n",
      "iteration 72100, loss = 0.11590993812598373, prediction = [0.16695035 0.96672085 0.96939249 0.19430217]\n",
      "iteration 72150, loss = 0.11586680510903412, prediction = [0.16689935 0.96674078 0.96941093 0.19424442]\n",
      "iteration 72200, loss = 0.11582371966342567, prediction = [0.16684841 0.96676069 0.96942935 0.19418673]\n",
      "iteration 72250, loss = 0.11578068170311319, prediction = [0.16679752 0.96678057 0.96944775 0.19412909]\n",
      "iteration 72300, loss = 0.11573769114226756, prediction = [0.16674667 0.96680043 0.96946612 0.19407151]\n",
      "iteration 72350, loss = 0.11569474789527326, prediction = [0.16669588 0.96682026 0.96948447 0.19401398]\n",
      "iteration 72400, loss = 0.11565185187672895, prediction = [0.16664514 0.96684007 0.96950279 0.19395651]\n",
      "iteration 72450, loss = 0.11560900300144736, prediction = [0.16659444 0.96685986 0.96952109 0.19389909]\n",
      "iteration 72500, loss = 0.11556620118445332, prediction = [0.1665438  0.96687961 0.96953937 0.19384173]\n",
      "iteration 72550, loss = 0.11552344634098421, prediction = [0.16649321 0.96689935 0.96955762 0.19378443]\n",
      "iteration 72600, loss = 0.11548073838648781, prediction = [0.16644266 0.96691905 0.96957586 0.19372718]\n",
      "iteration 72650, loss = 0.11543807723662326, prediction = [0.16639217 0.96693874 0.96959407 0.19366998]\n",
      "iteration 72700, loss = 0.11539546280726057, prediction = [0.16634172 0.96695839 0.96961225 0.19361284]\n",
      "iteration 72750, loss = 0.11535289501447682, prediction = [0.16629133 0.96697803 0.96963041 0.19355576]\n",
      "iteration 72800, loss = 0.11531037377455952, prediction = [0.16624098 0.96699763 0.96964855 0.19349873]\n",
      "iteration 72850, loss = 0.11526789900400393, prediction = [0.16619068 0.96701722 0.96966667 0.19344176]\n",
      "iteration 72900, loss = 0.11522547061951281, prediction = [0.16614044 0.96703677 0.96968476 0.19338484]\n",
      "iteration 72950, loss = 0.11518308853799417, prediction = [0.16609024 0.96705631 0.96970283 0.19332797]\n",
      "iteration 73000, loss = 0.11514075267656396, prediction = [0.16604009 0.96707582 0.96972088 0.19327116]\n",
      "iteration 73050, loss = 0.11509846295254236, prediction = [0.16598999 0.9670953  0.9697389  0.1932144 ]\n",
      "iteration 73100, loss = 0.115056219283455, prediction = [0.16593994 0.96711476 0.9697569  0.1931577 ]\n",
      "iteration 73150, loss = 0.1150140215870307, prediction = [0.16588994 0.9671342  0.96977488 0.19310106]\n",
      "iteration 73200, loss = 0.11497186978120222, prediction = [0.16583998 0.96715361 0.96979284 0.19304446]\n",
      "iteration 73250, loss = 0.11492976378410534, prediction = [0.16579008 0.96717299 0.96981077 0.19298792]\n",
      "iteration 73300, loss = 0.11488770351407801, prediction = [0.16574022 0.96719236 0.96982868 0.19293144]\n",
      "iteration 73350, loss = 0.11484568888965924, prediction = [0.16569041 0.96721169 0.96984657 0.19287501]\n",
      "iteration 73400, loss = 0.11480371982958933, prediction = [0.16564065 0.96723101 0.96986444 0.19281863]\n",
      "iteration 73450, loss = 0.1147617962528086, prediction = [0.16559094 0.9672503  0.96988228 0.19276231]\n",
      "iteration 73500, loss = 0.11471991807845718, prediction = [0.16554128 0.96726956 0.9699001  0.19270604]\n",
      "iteration 73550, loss = 0.11467808522587414, prediction = [0.16549166 0.9672888  0.9699179  0.19264982]\n",
      "iteration 73600, loss = 0.11463629761459734, prediction = [0.16544209 0.96730802 0.96993568 0.19259366]\n",
      "iteration 73650, loss = 0.11459455516436146, prediction = [0.16539258 0.96732722 0.96995343 0.19253755]\n",
      "iteration 73700, loss = 0.11455285779509913, prediction = [0.1653431  0.96734638 0.96997116 0.19248149]\n",
      "iteration 73750, loss = 0.11451120542693916, prediction = [0.16529368 0.96736553 0.96998887 0.19242549]\n",
      "iteration 73800, loss = 0.1144695979802064, prediction = [0.16524431 0.96738465 0.97000656 0.19236954]\n",
      "iteration 73850, loss = 0.11442803537542127, prediction = [0.16519498 0.96740375 0.97002422 0.19231365]\n",
      "iteration 73900, loss = 0.11438651753329838, prediction = [0.1651457  0.96742282 0.97004186 0.1922578 ]\n",
      "iteration 73950, loss = 0.11434504437474605, prediction = [0.16509646 0.96744187 0.97005948 0.19220201]\n",
      "iteration 74000, loss = 0.11430361582086751, prediction = [0.16504728 0.9674609  0.97007708 0.19214628]\n",
      "iteration 74050, loss = 0.11426223179295736, prediction = [0.16499814 0.9674799  0.97009466 0.19209059]\n",
      "iteration 74100, loss = 0.11422089221250302, prediction = [0.16494905 0.96749888 0.97011221 0.19203496]\n",
      "iteration 74150, loss = 0.11417959700118442, prediction = [0.16490001 0.96751784 0.97012975 0.19197938]\n",
      "iteration 74200, loss = 0.11413834608087117, prediction = [0.16485101 0.96753677 0.97014726 0.19192385]\n",
      "iteration 74250, loss = 0.11409713937362403, prediction = [0.16480206 0.96755568 0.97016475 0.19186838]\n",
      "iteration 74300, loss = 0.11405597680169399, prediction = [0.16475316 0.96757456 0.97018221 0.19181296]\n",
      "iteration 74350, loss = 0.11401485828752048, prediction = [0.16470431 0.96759342 0.97019966 0.19175759]\n",
      "iteration 74400, loss = 0.11397378375373256, prediction = [0.1646555  0.96761226 0.97021708 0.19170227]\n",
      "iteration 74450, loss = 0.1139327531231465, prediction = [0.16460674 0.96763108 0.97023449 0.191647  ]\n",
      "iteration 74500, loss = 0.11389176631876713, prediction = [0.16455802 0.96764987 0.97025187 0.19159179]\n",
      "iteration 74550, loss = 0.11385082326378548, prediction = [0.16450935 0.96766864 0.97026923 0.19153663]\n",
      "iteration 74600, loss = 0.11380992388157876, prediction = [0.16446073 0.96768738 0.97028656 0.19148152]\n",
      "iteration 74650, loss = 0.11376906809571072, prediction = [0.16441216 0.9677061  0.97030388 0.19142646]\n",
      "iteration 74700, loss = 0.11372825582993032, prediction = [0.16436363 0.9677248  0.97032117 0.19137145]\n",
      "iteration 74750, loss = 0.1136874870081706, prediction = [0.16431514 0.96774348 0.97033845 0.1913165 ]\n",
      "iteration 74800, loss = 0.11364676155454873, prediction = [0.16426671 0.96776213 0.9703557  0.1912616 ]\n",
      "iteration 74850, loss = 0.11360607939336606, prediction = [0.16421832 0.96778076 0.97037293 0.19120674]\n",
      "iteration 74900, loss = 0.11356544044910666, prediction = [0.16416997 0.96779937 0.97039014 0.19115194]\n",
      "iteration 74950, loss = 0.11352484464643622, prediction = [0.16412168 0.96781795 0.97040733 0.19109719]\n",
      "iteration 75000, loss = 0.11348429191020343, prediction = [0.16407342 0.96783652 0.97042449 0.1910425 ]\n",
      "iteration 75050, loss = 0.1134437821654373, prediction = [0.16402522 0.96785505 0.97044164 0.19098785]\n",
      "iteration 75100, loss = 0.11340331533734821, prediction = [0.16397706 0.96787357 0.97045876 0.19093325]\n",
      "iteration 75150, loss = 0.11336289135132606, prediction = [0.16392894 0.96789206 0.97047587 0.19087871]\n",
      "iteration 75200, loss = 0.1133225101329409, prediction = [0.16388087 0.96791053 0.97049295 0.19082421]\n",
      "iteration 75250, loss = 0.11328217160794155, prediction = [0.16383285 0.96792898 0.97051001 0.19076977]\n",
      "iteration 75300, loss = 0.11324187570225566, prediction = [0.16378487 0.96794741 0.97052705 0.19071538]\n",
      "iteration 75350, loss = 0.11320162234198866, prediction = [0.16373694 0.96796581 0.97054407 0.19066104]\n",
      "iteration 75400, loss = 0.11316141145342298, prediction = [0.16368905 0.96798419 0.97056107 0.19060674]\n",
      "iteration 75450, loss = 0.11312124296301815, prediction = [0.16364121 0.96800255 0.97057805 0.1905525 ]\n",
      "iteration 75500, loss = 0.1130811167974106, prediction = [0.16359341 0.96802089 0.970595   0.19049831]\n",
      "iteration 75550, loss = 0.11304103288341216, prediction = [0.16354566 0.9680392  0.97061194 0.19044417]\n",
      "iteration 75600, loss = 0.11300099114800954, prediction = [0.16349796 0.96805749 0.97062885 0.19039008]\n",
      "iteration 75650, loss = 0.11296099151836442, prediction = [0.16345029 0.96807576 0.97064575 0.19033604]\n",
      "iteration 75700, loss = 0.11292103392181288, prediction = [0.16340268 0.968094   0.97066262 0.19028205]\n",
      "iteration 75750, loss = 0.11288111828586489, prediction = [0.16335511 0.96811223 0.97067948 0.19022811]\n",
      "iteration 75800, loss = 0.11284124453820235, prediction = [0.16330758 0.96813043 0.97069631 0.19017422]\n",
      "iteration 75850, loss = 0.11280141260668095, prediction = [0.1632601  0.96814861 0.97071312 0.19012038]\n",
      "iteration 75900, loss = 0.11276162241932777, prediction = [0.16321266 0.96816677 0.97072991 0.19006659]\n",
      "iteration 75950, loss = 0.11272187390434239, prediction = [0.16316527 0.96818491 0.97074668 0.19001285]\n",
      "iteration 76000, loss = 0.11268216699009403, prediction = [0.16311792 0.96820302 0.97076343 0.18995916]\n",
      "iteration 76050, loss = 0.1126425016051237, prediction = [0.16307061 0.96822111 0.97078016 0.18990552]\n",
      "iteration 76100, loss = 0.11260287767814144, prediction = [0.16302336 0.96823918 0.97079687 0.18985193]\n",
      "iteration 76150, loss = 0.11256329513802721, prediction = [0.16297614 0.96825723 0.97081356 0.18979838]\n",
      "iteration 76200, loss = 0.11252375391383045, prediction = [0.16292897 0.96827526 0.97083023 0.18974489]\n",
      "iteration 76250, loss = 0.11248425393476769, prediction = [0.16288184 0.96829326 0.97084688 0.18969144]\n",
      "iteration 76300, loss = 0.11244479513022436, prediction = [0.16283476 0.96831124 0.97086351 0.18963805]\n",
      "iteration 76350, loss = 0.11240537742975346, prediction = [0.16278772 0.96832921 0.97088012 0.1895847 ]\n",
      "iteration 76400, loss = 0.11236600076307424, prediction = [0.16274072 0.96834715 0.97089671 0.18953141]\n",
      "iteration 76450, loss = 0.11232666506007259, prediction = [0.16269377 0.96836506 0.97091328 0.18947816]\n",
      "iteration 76500, loss = 0.11228737025080074, prediction = [0.16264687 0.96838296 0.97092983 0.18942496]\n",
      "iteration 76550, loss = 0.11224811626547654, prediction = [0.1626     0.96840083 0.97094636 0.18937181]\n",
      "iteration 76600, loss = 0.11220890303448144, prediction = [0.16255318 0.96841869 0.97096286 0.18931871]\n",
      "iteration 76650, loss = 0.11216973048836218, prediction = [0.16250641 0.96843652 0.97097935 0.18926565]\n",
      "iteration 76700, loss = 0.11213059855783031, prediction = [0.16245967 0.96845433 0.97099582 0.18921265]\n",
      "iteration 76750, loss = 0.11209150717375876, prediction = [0.16241298 0.96847212 0.97101227 0.18915969]\n",
      "iteration 76800, loss = 0.11205245626718517, prediction = [0.16236634 0.96848989 0.9710287  0.18910678]\n",
      "iteration 76850, loss = 0.11201344576930869, prediction = [0.16231974 0.96850763 0.97104511 0.18905392]\n",
      "iteration 76900, loss = 0.11197447561149096, prediction = [0.16227318 0.96852536 0.9710615  0.18900111]\n",
      "iteration 76950, loss = 0.11193554572525466, prediction = [0.16222666 0.96854306 0.97107787 0.18894835]\n",
      "iteration 77000, loss = 0.11189665604228388, prediction = [0.16218019 0.96856075 0.97109422 0.18889563]\n",
      "iteration 77050, loss = 0.11185780649442267, prediction = [0.16213376 0.96857841 0.97111055 0.18884296]\n",
      "iteration 77100, loss = 0.11181899701367548, prediction = [0.16208737 0.96859605 0.97112686 0.18879034]\n",
      "iteration 77150, loss = 0.11178022753220657, prediction = [0.16204103 0.96861367 0.97114315 0.18873777]\n",
      "iteration 77200, loss = 0.11174149798233891, prediction = [0.16199473 0.96863127 0.97115942 0.18868525]\n",
      "iteration 77250, loss = 0.11170280829655393, prediction = [0.16194847 0.96864884 0.97117568 0.18863277]\n",
      "iteration 77300, loss = 0.11166415840749147, prediction = [0.16190225 0.9686664  0.97119191 0.18858034]\n",
      "iteration 77350, loss = 0.11162554824794896, prediction = [0.16185608 0.96868393 0.97120812 0.18852796]\n",
      "iteration 77400, loss = 0.11158697775088086, prediction = [0.16180995 0.96870145 0.97122432 0.18847563]\n",
      "iteration 77450, loss = 0.11154844684939885, prediction = [0.16176386 0.96871894 0.97124049 0.18842334]\n",
      "iteration 77500, loss = 0.11150995547676998, prediction = [0.16171781 0.96873642 0.97125665 0.18837111]\n",
      "iteration 77550, loss = 0.11147150356641751, prediction = [0.16167181 0.96875387 0.97127278 0.18831891]\n",
      "iteration 77600, loss = 0.11143309105192105, prediction = [0.16162585 0.9687713  0.9712889  0.18826677]\n",
      "iteration 77650, loss = 0.11139471786701272, prediction = [0.16157993 0.96878871 0.971305   0.18821467]\n",
      "iteration 77700, loss = 0.1113563839455812, prediction = [0.16153405 0.9688061  0.97132108 0.18816262]\n",
      "iteration 77750, loss = 0.11131808922166785, prediction = [0.16148822 0.96882347 0.97133713 0.18811062]\n",
      "iteration 77800, loss = 0.11127983362946842, prediction = [0.16144243 0.96884082 0.97135317 0.18805866]\n",
      "iteration 77850, loss = 0.11124161710333061, prediction = [0.16139668 0.96885815 0.9713692  0.18800675]\n",
      "iteration 77900, loss = 0.11120343957775612, prediction = [0.16135097 0.96887546 0.9713852  0.18795489]\n",
      "iteration 77950, loss = 0.11116530098739752, prediction = [0.1613053  0.96889274 0.97140118 0.18790307]\n",
      "iteration 78000, loss = 0.11112720126705924, prediction = [0.16125968 0.96891001 0.97141714 0.18785131]\n",
      "iteration 78050, loss = 0.11108914035169705, prediction = [0.1612141  0.96892726 0.97143309 0.18779958]\n",
      "iteration 78100, loss = 0.11105111817641822, prediction = [0.16116855 0.96894448 0.97144902 0.18774791]\n",
      "iteration 78150, loss = 0.11101313467647883, prediction = [0.16112306 0.96896169 0.97146492 0.18769628]\n",
      "iteration 78200, loss = 0.11097518978728654, prediction = [0.1610776  0.96897887 0.97148081 0.18764469]\n",
      "iteration 78250, loss = 0.1109372834443972, prediction = [0.16103218 0.96899604 0.97149668 0.18759316]\n",
      "iteration 78300, loss = 0.11089941558351624, prediction = [0.16098681 0.96901319 0.97151253 0.18754166]\n",
      "iteration 78350, loss = 0.11086158614049729, prediction = [0.16094147 0.96903031 0.97152836 0.18749022]\n",
      "iteration 78400, loss = 0.1108237950513422, prediction = [0.16089618 0.96904742 0.97154418 0.18743882]\n",
      "iteration 78450, loss = 0.11078604225220073, prediction = [0.16085093 0.9690645  0.97155997 0.18738747]\n",
      "iteration 78500, loss = 0.11074832767937007, prediction = [0.16080572 0.96908156 0.97157575 0.18733616]\n",
      "iteration 78550, loss = 0.11071065126929339, prediction = [0.16076055 0.96909861 0.97159151 0.1872849 ]\n",
      "iteration 78600, loss = 0.11067301295856083, prediction = [0.16071542 0.96911563 0.97160725 0.18723368]\n",
      "iteration 78650, loss = 0.11063541268390836, prediction = [0.16067033 0.96913264 0.97162297 0.18718252]\n",
      "iteration 78700, loss = 0.11059785038221803, prediction = [0.16062529 0.96914962 0.97163867 0.18713139]\n",
      "iteration 78750, loss = 0.11056032599051567, prediction = [0.16058028 0.96916659 0.97165435 0.18708031]\n",
      "iteration 78800, loss = 0.11052283944597317, prediction = [0.16053532 0.96918353 0.97167002 0.18702928]\n",
      "iteration 78850, loss = 0.11048539068590599, prediction = [0.16049039 0.96920046 0.97168566 0.18697829]\n",
      "iteration 78900, loss = 0.11044797964777348, prediction = [0.16044551 0.96921736 0.97170129 0.18692735]\n",
      "iteration 78950, loss = 0.11041060626917824, prediction = [0.16040067 0.96923425 0.9717169  0.18687645]\n",
      "iteration 79000, loss = 0.11037327048786612, prediction = [0.16035586 0.96925112 0.97173249 0.1868256 ]\n",
      "iteration 79050, loss = 0.11033597224172532, prediction = [0.1603111  0.96926796 0.97174807 0.1867748 ]\n",
      "iteration 79100, loss = 0.11029871146878653, prediction = [0.16026638 0.96928479 0.97176362 0.18672403]\n",
      "iteration 79150, loss = 0.11026148810722197, prediction = [0.1602217  0.9693016  0.97177916 0.18667332]\n",
      "iteration 79200, loss = 0.11022430209534514, prediction = [0.16017706 0.96931838 0.97179468 0.18662265]\n",
      "iteration 79250, loss = 0.11018715337161034, prediction = [0.16013246 0.96933515 0.97181018 0.18657202]\n",
      "iteration 79300, loss = 0.1101500418746128, prediction = [0.1600879  0.9693519  0.97182566 0.18652144]\n",
      "iteration 79350, loss = 0.1101129675430871, prediction = [0.16004338 0.96936863 0.97184112 0.1864709 ]\n",
      "iteration 79400, loss = 0.1100759303159084, prediction = [0.1599989  0.96938534 0.97185657 0.18642041]\n",
      "iteration 79450, loss = 0.1100389301320904, prediction = [0.15995446 0.96940203 0.971872   0.18636996]\n",
      "iteration 79500, loss = 0.11000196693078637, prediction = [0.15991006 0.9694187  0.97188741 0.18631956]\n",
      "iteration 79550, loss = 0.1099650406512872, prediction = [0.1598657  0.96943535 0.9719028  0.1862692 ]\n",
      "iteration 79600, loss = 0.1099281512330226, prediction = [0.15982138 0.96945199 0.97191818 0.18621889]\n",
      "iteration 79650, loss = 0.10989129861555957, prediction = [0.1597771  0.9694686  0.97193353 0.18616862]\n",
      "iteration 79700, loss = 0.10985448273860234, prediction = [0.15973286 0.96948519 0.97194887 0.18611839]\n",
      "iteration 79750, loss = 0.1098177035419923, prediction = [0.15968865 0.96950177 0.97196419 0.18606821]\n",
      "iteration 79800, loss = 0.10978096096570675, prediction = [0.15964449 0.96951832 0.9719795  0.18601807]\n",
      "iteration 79850, loss = 0.10974425494985969, prediction = [0.15960037 0.96953486 0.97199478 0.18596798]\n",
      "iteration 79900, loss = 0.10970758543470058, prediction = [0.15955629 0.96955138 0.97201005 0.18591793]\n",
      "iteration 79950, loss = 0.1096709523606142, prediction = [0.15951224 0.96956787 0.9720253  0.18586793]\n",
      "iteration 80000, loss = 0.10963435566811995, prediction = [0.15946824 0.96958435 0.97204053 0.18581796]\n",
      "iteration 80050, loss = 0.10959779529787245, prediction = [0.15942427 0.96960081 0.97205575 0.18576805]\n",
      "iteration 80100, loss = 0.10956127119065971, prediction = [0.15938035 0.96961726 0.97207094 0.18571817]\n",
      "iteration 80150, loss = 0.10952478328740342, prediction = [0.15933646 0.96963368 0.97208612 0.18566834]\n",
      "iteration 80200, loss = 0.10948833152915902, prediction = [0.15929261 0.96965008 0.97210128 0.18561856]\n",
      "iteration 80250, loss = 0.10945191585711535, prediction = [0.15924881 0.96966647 0.97211643 0.18556881]\n",
      "iteration 80300, loss = 0.10941553621259331, prediction = [0.15920504 0.96968283 0.97213156 0.18551912]\n",
      "iteration 80350, loss = 0.10937919253704517, prediction = [0.1591613  0.96969918 0.97214667 0.18546946]\n",
      "iteration 80400, loss = 0.1093428847720567, prediction = [0.15911761 0.96971551 0.97216176 0.18541985]\n",
      "iteration 80450, loss = 0.10930661285934418, prediction = [0.15907396 0.96973182 0.97217683 0.18537028]\n",
      "iteration 80500, loss = 0.10927037674075482, prediction = [0.15903035 0.96974811 0.97219189 0.18532075]\n",
      "iteration 80550, loss = 0.10923417635826688, prediction = [0.15898677 0.96976438 0.97220693 0.18527127]\n",
      "iteration 80600, loss = 0.10919801165398887, prediction = [0.15894323 0.96978063 0.97222195 0.18522183]\n",
      "iteration 80650, loss = 0.10916188257015921, prediction = [0.15889973 0.96979687 0.97223696 0.18517243]\n",
      "iteration 80700, loss = 0.10912578904914584, prediction = [0.15885627 0.96981308 0.97225195 0.18512308]\n",
      "iteration 80750, loss = 0.10908973103344577, prediction = [0.15881285 0.96982928 0.97226692 0.18507377]\n",
      "iteration 80800, loss = 0.10905370846568516, prediction = [0.15876947 0.96984546 0.97228187 0.1850245 ]\n",
      "iteration 80850, loss = 0.10901772128861886, prediction = [0.15872612 0.96986162 0.97229681 0.18497528]\n",
      "iteration 80900, loss = 0.10898176944512883, prediction = [0.15868282 0.96987777 0.97231173 0.1849261 ]\n",
      "iteration 80950, loss = 0.10894585287822602, prediction = [0.15863955 0.96989389 0.97232663 0.18487696]\n",
      "iteration 81000, loss = 0.10890997153104781, prediction = [0.15859632 0.96991    0.97234152 0.18482786]\n",
      "iteration 81050, loss = 0.10887412534685859, prediction = [0.15855313 0.96992608 0.97235639 0.1847788 ]\n",
      "iteration 81100, loss = 0.10883831426905022, prediction = [0.15850997 0.96994215 0.97237124 0.18472979]\n",
      "iteration 81150, loss = 0.10880253824113961, prediction = [0.15846686 0.9699582  0.97238607 0.18468082]\n",
      "iteration 81200, loss = 0.1087667972067711, prediction = [0.15842378 0.96997424 0.97240089 0.1846319 ]\n",
      "iteration 81250, loss = 0.10873109110971332, prediction = [0.15838074 0.96999025 0.97241569 0.18458301]\n",
      "iteration 81300, loss = 0.1086954198938607, prediction = [0.15833774 0.97000625 0.97243048 0.18453417]\n",
      "iteration 81350, loss = 0.10865978350323205, prediction = [0.15829477 0.97002222 0.97244524 0.18448537]\n",
      "iteration 81400, loss = 0.10862418188197173, prediction = [0.15825184 0.97003818 0.97245999 0.18443661]\n",
      "iteration 81450, loss = 0.10858861497434717, prediction = [0.15820895 0.97005413 0.97247473 0.18438789]\n",
      "iteration 81500, loss = 0.10855308272474978, prediction = [0.1581661  0.97007005 0.97248944 0.18433922]\n",
      "iteration 81550, loss = 0.10851758507769468, prediction = [0.15812329 0.97008596 0.97250414 0.18429059]\n",
      "iteration 81600, loss = 0.10848212197782053, prediction = [0.15808051 0.97010184 0.97251883 0.184242  ]\n",
      "iteration 81650, loss = 0.10844669336988721, prediction = [0.15803777 0.97011771 0.97253349 0.18419345]\n",
      "iteration 81700, loss = 0.10841129919877861, prediction = [0.15799507 0.97013357 0.97254814 0.18414494]\n",
      "iteration 81750, loss = 0.10837593940949988, prediction = [0.15795241 0.9701494  0.97256278 0.18409648]\n",
      "iteration 81800, loss = 0.10834061394717809, prediction = [0.15790978 0.97016522 0.97257739 0.18404805]\n",
      "iteration 81850, loss = 0.10830532275706176, prediction = [0.15786719 0.97018101 0.97259199 0.18399967]\n",
      "iteration 81900, loss = 0.10827006578452011, prediction = [0.15782463 0.97019679 0.97260658 0.18395133]\n",
      "iteration 81950, loss = 0.10823484297504304, prediction = [0.15778212 0.97021256 0.97262114 0.18390303]\n",
      "iteration 82000, loss = 0.1081996542742413, prediction = [0.15773964 0.9702283  0.9726357  0.18385478]\n",
      "iteration 82050, loss = 0.1081644996278453, prediction = [0.1576972  0.97024403 0.97265023 0.18380656]\n",
      "iteration 82100, loss = 0.10812937898170458, prediction = [0.15765479 0.97025974 0.97266475 0.18375838]\n",
      "iteration 82150, loss = 0.10809429228178899, prediction = [0.15761242 0.97027543 0.97267925 0.18371025]\n",
      "iteration 82200, loss = 0.10805923947418707, prediction = [0.15757009 0.97029111 0.97269373 0.18366216]\n",
      "iteration 82250, loss = 0.10802422050510524, prediction = [0.1575278  0.97030676 0.9727082  0.18361411]\n",
      "iteration 82300, loss = 0.10798923532086951, prediction = [0.15748554 0.9703224  0.97272266 0.1835661 ]\n",
      "iteration 82350, loss = 0.10795428386792327, prediction = [0.15744332 0.97033802 0.97273709 0.18351813]\n",
      "iteration 82400, loss = 0.10791936609282687, prediction = [0.15740114 0.97035363 0.97275151 0.1834702 ]\n",
      "iteration 82450, loss = 0.10788448194225911, prediction = [0.15735899 0.97036922 0.97276592 0.18342231]\n",
      "iteration 82500, loss = 0.10784963136301536, prediction = [0.15731688 0.97038479 0.9727803  0.18337446]\n",
      "iteration 82550, loss = 0.10781481430200732, prediction = [0.1572748  0.97040034 0.97279468 0.18332666]\n",
      "iteration 82600, loss = 0.10778003070626357, prediction = [0.15723276 0.97041587 0.97280903 0.18327889]\n",
      "iteration 82650, loss = 0.10774528052292869, prediction = [0.15719076 0.97043139 0.97282337 0.18323117]\n",
      "iteration 82700, loss = 0.10771056369926249, prediction = [0.15714879 0.97044689 0.97283769 0.18318348]\n",
      "iteration 82750, loss = 0.10767588018264057, prediction = [0.15710686 0.97046237 0.972852   0.18313584]\n",
      "iteration 82800, loss = 0.10764122992055306, prediction = [0.15706497 0.97047784 0.97286629 0.18308823]\n",
      "iteration 82850, loss = 0.10760661286060544, prediction = [0.15702311 0.97049329 0.97288057 0.18304067]\n",
      "iteration 82900, loss = 0.10757202895051736, prediction = [0.15698129 0.97050872 0.97289483 0.18299315]\n",
      "iteration 82950, loss = 0.1075374781381227, prediction = [0.15693951 0.97052413 0.97290907 0.18294567]\n",
      "iteration 83000, loss = 0.10750296037136885, prediction = [0.15689776 0.97053953 0.9729233  0.18289822]\n",
      "iteration 83050, loss = 0.10746847559831671, prediction = [0.15685604 0.97055491 0.97293751 0.18285082]\n",
      "iteration 83100, loss = 0.10743402376714059, prediction = [0.15681437 0.97057027 0.9729517  0.18280346]\n",
      "iteration 83150, loss = 0.10739960482612702, prediction = [0.15677272 0.97058561 0.97296588 0.18275614]\n",
      "iteration 83200, loss = 0.10736521872367574, prediction = [0.15673112 0.97060094 0.97298005 0.18270886]\n",
      "iteration 83250, loss = 0.10733086540829848, prediction = [0.15668955 0.97061625 0.9729942  0.18266161]\n",
      "iteration 83300, loss = 0.10729654482861861, prediction = [0.15664801 0.97063155 0.97300833 0.18261441]\n",
      "iteration 83350, loss = 0.10726225693337155, prediction = [0.15660651 0.97064683 0.97302245 0.18256725]\n",
      "iteration 83400, loss = 0.10722800167140323, prediction = [0.15656505 0.97066209 0.97303655 0.18252013]\n",
      "iteration 83450, loss = 0.10719377899167135, prediction = [0.15652362 0.97067733 0.97305063 0.18247304]\n",
      "iteration 83500, loss = 0.1071595888432437, prediction = [0.15648223 0.97069256 0.9730647  0.182426  ]\n",
      "iteration 83550, loss = 0.1071254311752989, prediction = [0.15644087 0.97070777 0.97307876 0.182379  ]\n",
      "iteration 83600, loss = 0.10709130593712558, prediction = [0.15639955 0.97072296 0.97309279 0.18233203]\n",
      "iteration 83650, loss = 0.10705721307812124, prediction = [0.15635826 0.97073814 0.97310682 0.18228511]\n",
      "iteration 83700, loss = 0.10702315254779371, prediction = [0.15631701 0.9707533  0.97312083 0.18223822]\n",
      "iteration 83750, loss = 0.10698912429576009, prediction = [0.1562758  0.97076844 0.97313482 0.18219137]\n",
      "iteration 83800, loss = 0.1069551282717452, prediction = [0.15623462 0.97078357 0.97314879 0.18214457]\n",
      "iteration 83850, loss = 0.1069211644255838, prediction = [0.15619347 0.97079868 0.97316275 0.1820978 ]\n",
      "iteration 83900, loss = 0.10688723270721819, prediction = [0.15615236 0.97081377 0.9731767  0.18205107]\n",
      "iteration 83950, loss = 0.10685333306669831, prediction = [0.15611128 0.97082885 0.97319063 0.18200438]\n",
      "iteration 84000, loss = 0.10681946545418254, prediction = [0.15607024 0.97084391 0.97320455 0.18195773]\n",
      "iteration 84050, loss = 0.10678562981993557, prediction = [0.15602924 0.97085895 0.97321844 0.18191112]\n",
      "iteration 84100, loss = 0.10675182611433023, prediction = [0.15598827 0.97087398 0.97323233 0.18186455]\n",
      "iteration 84150, loss = 0.106718054287845, prediction = [0.15594733 0.97088899 0.9732462  0.18181802]\n",
      "iteration 84200, loss = 0.10668431429106584, prediction = [0.15590643 0.97090398 0.97326005 0.18177152]\n",
      "iteration 84250, loss = 0.10665060607468453, prediction = [0.15586556 0.97091896 0.97327389 0.18172507]\n",
      "iteration 84300, loss = 0.10661692958949842, prediction = [0.15582473 0.97093392 0.97328771 0.18167865]\n",
      "iteration 84350, loss = 0.10658328478641053, prediction = [0.15578393 0.97094886 0.97330152 0.18163227]\n",
      "iteration 84400, loss = 0.10654967161642959, prediction = [0.15574317 0.97096379 0.97331531 0.18158593]\n",
      "iteration 84450, loss = 0.10651609003066903, prediction = [0.15570244 0.97097871 0.97332909 0.18153963]\n",
      "iteration 84500, loss = 0.10648253998034712, prediction = [0.15566174 0.9709936  0.97334285 0.18149337]\n",
      "iteration 84550, loss = 0.10644902141678626, prediction = [0.15562109 0.97100848 0.9733566  0.18144714]\n",
      "iteration 84600, loss = 0.10641553429141347, prediction = [0.15558046 0.97102334 0.97337033 0.18140096]\n",
      "iteration 84650, loss = 0.10638207855575922, prediction = [0.15553987 0.97103819 0.97338405 0.18135481]\n",
      "iteration 84700, loss = 0.10634865416145808, prediction = [0.15549931 0.97105302 0.97339775 0.1813087 ]\n",
      "iteration 84750, loss = 0.1063152610602476, prediction = [0.15545879 0.97106784 0.97341144 0.18126263]\n",
      "iteration 84800, loss = 0.10628189920396773, prediction = [0.1554183  0.97108264 0.97342511 0.1812166 ]\n",
      "iteration 84850, loss = 0.10624856854456277, prediction = [0.15537785 0.97109742 0.97343877 0.1811706 ]\n",
      "iteration 84900, loss = 0.10621526903407752, prediction = [0.15533743 0.97111218 0.97345241 0.18112465]\n",
      "iteration 84950, loss = 0.10618200062466086, prediction = [0.15529704 0.97112693 0.97346604 0.18107873]\n",
      "iteration 85000, loss = 0.10614876326856207, prediction = [0.15525669 0.97114167 0.97347965 0.18103285]\n",
      "iteration 85050, loss = 0.10611555691813299, prediction = [0.15521637 0.97115639 0.97349325 0.18098701]\n",
      "iteration 85100, loss = 0.10608238152582594, prediction = [0.15517608 0.97117109 0.97350683 0.1809412 ]\n",
      "iteration 85150, loss = 0.1060492370441952, prediction = [0.15513583 0.97118578 0.9735204  0.18089544]\n",
      "iteration 85200, loss = 0.10601612342589557, prediction = [0.15509562 0.97120045 0.97353395 0.18084971]\n",
      "iteration 85250, loss = 0.10598304062368208, prediction = [0.15505543 0.9712151  0.97354749 0.18080402]\n",
      "iteration 85300, loss = 0.10594998859041085, prediction = [0.15501528 0.97122974 0.97356102 0.18075836]\n",
      "iteration 85350, loss = 0.10591696727903638, prediction = [0.15497517 0.97124436 0.97357453 0.18071275]\n",
      "iteration 85400, loss = 0.10588397664261467, prediction = [0.15493509 0.97125897 0.97358802 0.18066717]\n",
      "iteration 85450, loss = 0.10585101663430016, prediction = [0.15489504 0.97127356 0.9736015  0.18062163]\n",
      "iteration 85500, loss = 0.10581808720734685, prediction = [0.15485502 0.97128814 0.97361497 0.18057613]\n",
      "iteration 85550, loss = 0.10578518831510714, prediction = [0.15481504 0.9713027  0.97362842 0.18053066]\n",
      "iteration 85600, loss = 0.1057523199110326, prediction = [0.15477509 0.97131724 0.97364185 0.18048523]\n",
      "iteration 85650, loss = 0.10571948194867305, prediction = [0.15473518 0.97133177 0.97365527 0.18043984]\n",
      "iteration 85700, loss = 0.10568667438167616, prediction = [0.15469529 0.97134628 0.97366868 0.18039449]\n",
      "iteration 85750, loss = 0.10565389716378801, prediction = [0.15465544 0.97136078 0.97368207 0.18034917]\n",
      "iteration 85800, loss = 0.10562115024885163, prediction = [0.15461563 0.97137526 0.97369545 0.18030389]\n",
      "iteration 85850, loss = 0.1055884335908076, prediction = [0.15457585 0.97138973 0.97370882 0.18025865]\n",
      "iteration 85900, loss = 0.10555574714369376, prediction = [0.1545361  0.97140418 0.97372217 0.18021344]\n",
      "iteration 85950, loss = 0.1055230908616446, prediction = [0.15449638 0.97141861 0.9737355  0.18016827]\n",
      "iteration 86000, loss = 0.10549046469889106, prediction = [0.1544567  0.97143303 0.97374882 0.18012314]\n",
      "iteration 86050, loss = 0.10545786860976045, prediction = [0.15441705 0.97144744 0.97376213 0.18007805]\n",
      "iteration 86100, loss = 0.10542530254867585, prediction = [0.15437743 0.97146182 0.97377542 0.18003299]\n",
      "iteration 86150, loss = 0.10539276647015697, prediction = [0.15433785 0.9714762  0.9737887  0.17998797]\n",
      "iteration 86200, loss = 0.10536026032881775, prediction = [0.15429829 0.97149055 0.97380196 0.17994298]\n",
      "iteration 86250, loss = 0.10532778407936824, prediction = [0.15425878 0.9715049  0.97381521 0.17989804]\n",
      "iteration 86300, loss = 0.1052953376766129, prediction = [0.15421929 0.97151922 0.97382844 0.17985313]\n",
      "iteration 86350, loss = 0.1052629210754521, prediction = [0.15417984 0.97153354 0.97384166 0.17980825]\n",
      "iteration 86400, loss = 0.10523053423087904, prediction = [0.15414042 0.97154783 0.97385487 0.17976341]\n",
      "iteration 86450, loss = 0.10519817709798236, prediction = [0.15410103 0.97156211 0.97386806 0.17971861]\n",
      "iteration 86500, loss = 0.1051658496319447, prediction = [0.15406167 0.97157638 0.97388124 0.17967385]\n",
      "iteration 86550, loss = 0.10513355178804125, prediction = [0.15402235 0.97159063 0.9738944  0.17962912]\n",
      "iteration 86600, loss = 0.10510128352164204, prediction = [0.15398306 0.97160486 0.97390755 0.17958443]\n",
      "iteration 86650, loss = 0.10506904478820915, prediction = [0.1539438  0.97161908 0.97392069 0.17953977]\n",
      "iteration 86700, loss = 0.10503683554329898, prediction = [0.15390457 0.97163329 0.97393381 0.17949515]\n",
      "iteration 86750, loss = 0.10500465574255938, prediction = [0.15386538 0.97164748 0.97394692 0.17945057]\n",
      "iteration 86800, loss = 0.10497250534173183, prediction = [0.15382622 0.97166165 0.97396001 0.17940602]\n",
      "iteration 86850, loss = 0.1049403842966489, prediction = [0.15378709 0.97167581 0.97397309 0.17936151]\n",
      "iteration 86900, loss = 0.10490829256323583, prediction = [0.15374799 0.97168996 0.97398616 0.17931703]\n",
      "iteration 86950, loss = 0.10487623009750958, prediction = [0.15370893 0.97170409 0.97399921 0.17927259]\n",
      "iteration 87000, loss = 0.10484419685557855, prediction = [0.1536699  0.9717182  0.97401224 0.17922819]\n",
      "iteration 87050, loss = 0.10481219279364268, prediction = [0.1536309  0.9717323  0.97402527 0.17918382]\n",
      "iteration 87100, loss = 0.10478021786799216, prediction = [0.15359193 0.97174639 0.97403828 0.17913949]\n",
      "iteration 87150, loss = 0.10474827203500878, prediction = [0.15355299 0.97176046 0.97405127 0.1790952 ]\n",
      "iteration 87200, loss = 0.10471635525116435, prediction = [0.15351409 0.97177451 0.97406426 0.17905094]\n",
      "iteration 87250, loss = 0.10468446747302128, prediction = [0.15347521 0.97178855 0.97407723 0.17900671]\n",
      "iteration 87300, loss = 0.10465260865723186, prediction = [0.15343637 0.97180258 0.97409018 0.17896252]\n",
      "iteration 87350, loss = 0.10462077876053816, prediction = [0.15339756 0.97181659 0.97410312 0.17891837]\n",
      "iteration 87400, loss = 0.10458897773977216, prediction = [0.15335879 0.97183058 0.97411605 0.17887425]\n",
      "iteration 87450, loss = 0.10455720555185463, prediction = [0.15332004 0.97184456 0.97412896 0.17883017]\n",
      "iteration 87500, loss = 0.1045254621537963, prediction = [0.15328133 0.97185853 0.97414186 0.17878612]\n",
      "iteration 87550, loss = 0.10449374750269598, prediction = [0.15324264 0.97187248 0.97415475 0.17874211]\n",
      "iteration 87600, loss = 0.1044620615557417, prediction = [0.15320399 0.97188642 0.97416762 0.17869814]\n",
      "iteration 87650, loss = 0.10443040427020868, prediction = [0.15316537 0.97190034 0.97418048 0.17865419]\n",
      "iteration 87700, loss = 0.10439877560346231, prediction = [0.15312679 0.97191425 0.97419332 0.17861029]\n",
      "iteration 87750, loss = 0.10436717551295438, prediction = [0.15308823 0.97192814 0.97420615 0.17856642]\n",
      "iteration 87800, loss = 0.10433560395622526, prediction = [0.15304971 0.97194202 0.97421897 0.17852258]\n",
      "iteration 87850, loss = 0.10430406089090184, prediction = [0.15301121 0.97195588 0.97423178 0.17847878]\n",
      "iteration 87900, loss = 0.10427254627469922, prediction = [0.15297275 0.97196973 0.97424457 0.17843502]\n",
      "iteration 87950, loss = 0.10424106006541908, prediction = [0.15293432 0.97198356 0.97425735 0.17839129]\n",
      "iteration 88000, loss = 0.10420960222094991, prediction = [0.15289592 0.97199738 0.97427011 0.17834759]\n",
      "iteration 88050, loss = 0.1041781726992668, prediction = [0.15285755 0.97201119 0.97428286 0.17830393]\n",
      "iteration 88100, loss = 0.10414677145843135, prediction = [0.15281921 0.97202498 0.9742956  0.17826031]\n",
      "iteration 88150, loss = 0.10411539845659093, prediction = [0.15278091 0.97203875 0.97430832 0.17821672]\n",
      "iteration 88200, loss = 0.10408405365197906, prediction = [0.15274263 0.97205252 0.97432103 0.17817316]\n",
      "iteration 88250, loss = 0.1040527370029146, prediction = [0.15270439 0.97206626 0.97433373 0.17812964]\n",
      "iteration 88300, loss = 0.10402144846780223, prediction = [0.15266617 0.97208    0.97434641 0.17808616]\n",
      "iteration 88350, loss = 0.10399018800513204, prediction = [0.15262799 0.97209372 0.97435908 0.17804271]\n",
      "iteration 88400, loss = 0.10395895557347826, prediction = [0.15258984 0.97210742 0.97437174 0.17799929]\n",
      "iteration 88450, loss = 0.10392775113150099, prediction = [0.15255172 0.97212111 0.97438438 0.17795591]\n",
      "iteration 88500, loss = 0.10389657463794352, prediction = [0.15251363 0.97213479 0.97439701 0.17791256]\n",
      "iteration 88550, loss = 0.1038654260516348, prediction = [0.15247557 0.97214845 0.97440963 0.17786925]\n",
      "iteration 88600, loss = 0.10383430533148719, prediction = [0.15243754 0.97216209 0.97442223 0.17782597]\n",
      "iteration 88650, loss = 0.10380321243649698, prediction = [0.15239954 0.97217573 0.97443483 0.17778272]\n",
      "iteration 88700, loss = 0.10377214732574419, prediction = [0.15236158 0.97218935 0.9744474  0.17773951]\n",
      "iteration 88750, loss = 0.10374110995839256, prediction = [0.15232364 0.97220295 0.97445997 0.17769634]\n",
      "iteration 88800, loss = 0.10371010029368835, prediction = [0.15228574 0.97221654 0.97447252 0.17765319]\n",
      "iteration 88850, loss = 0.1036791182909618, prediction = [0.15224786 0.97223012 0.97448506 0.17761009]\n",
      "iteration 88900, loss = 0.10364816390962484, prediction = [0.15221002 0.97224368 0.97449758 0.17756701]\n",
      "iteration 88950, loss = 0.10361723710917331, prediction = [0.1521722  0.97225723 0.97451009 0.17752397]\n",
      "iteration 89000, loss = 0.10358633784918442, prediction = [0.15213442 0.97227076 0.97452259 0.17748097]\n",
      "iteration 89050, loss = 0.10355546608931804, prediction = [0.15209666 0.97228428 0.97453508 0.177438  ]\n",
      "iteration 89100, loss = 0.10352462178931542, prediction = [0.15205894 0.97229779 0.97454755 0.17739506]\n",
      "iteration 89150, loss = 0.1034938049089999, prediction = [0.15202125 0.97231128 0.97456001 0.17735216]\n",
      "iteration 89200, loss = 0.10346301540827638, prediction = [0.15198358 0.97232476 0.97457246 0.17730929]\n",
      "iteration 89250, loss = 0.10343225324713112, prediction = [0.15194595 0.97233822 0.97458489 0.17726645]\n",
      "iteration 89300, loss = 0.10340151838563105, prediction = [0.15190835 0.97235167 0.97459731 0.17722365]\n",
      "iteration 89350, loss = 0.10337081078392502, prediction = [0.15187078 0.97236511 0.97460972 0.17718088]\n",
      "iteration 89400, loss = 0.10334013040224148, prediction = [0.15183323 0.97237853 0.97462212 0.17713814]\n",
      "iteration 89450, loss = 0.10330947720088968, prediction = [0.15179572 0.97239194 0.9746345  0.17709544]\n",
      "iteration 89500, loss = 0.10327885114025923, prediction = [0.15175824 0.97240533 0.97464687 0.17705278]\n",
      "iteration 89550, loss = 0.10324825218081977, prediction = [0.15172079 0.97241871 0.97465923 0.17701014]\n",
      "iteration 89600, loss = 0.10321768028312076, prediction = [0.15168336 0.97243208 0.97467157 0.17696754]\n",
      "iteration 89650, loss = 0.10318713540779115, prediction = [0.15164597 0.97244543 0.9746839  0.17692497]\n",
      "iteration 89700, loss = 0.10315661751553987, prediction = [0.15160861 0.97245877 0.97469622 0.17688244]\n",
      "iteration 89750, loss = 0.10312612656715471, prediction = [0.15157127 0.9724721  0.97470853 0.17683994]\n",
      "iteration 89800, loss = 0.10309566252350184, prediction = [0.15153397 0.97248541 0.97472082 0.17679747]\n",
      "iteration 89850, loss = 0.1030652253455277, prediction = [0.1514967  0.97249871 0.9747331  0.17675504]\n",
      "iteration 89900, loss = 0.10303481499425626, prediction = [0.15145945 0.97251199 0.97474537 0.17671264]\n",
      "iteration 89950, loss = 0.10300443143079005, prediction = [0.15142224 0.97252526 0.97475762 0.17667027]\n",
      "iteration 90000, loss = 0.10297407461631003, prediction = [0.15138505 0.97253852 0.97476987 0.17662794]\n",
      "iteration 90050, loss = 0.10294374451207537, prediction = [0.1513479  0.97255176 0.9747821  0.17658564]\n",
      "iteration 90100, loss = 0.10291344107942241, prediction = [0.15131077 0.97256499 0.97479431 0.17654337]\n",
      "iteration 90150, loss = 0.10288316427976568, prediction = [0.15127367 0.97257821 0.97480652 0.17650113]\n",
      "iteration 90200, loss = 0.10285291407459729, prediction = [0.15123661 0.97259141 0.97481871 0.17645893]\n",
      "iteration 90250, loss = 0.10282269042548567, prediction = [0.15119957 0.9726046  0.97483089 0.17641676]\n",
      "iteration 90300, loss = 0.10279249329407752, prediction = [0.15116256 0.97261778 0.97484306 0.17637463]\n",
      "iteration 90350, loss = 0.10276232264209517, prediction = [0.15112558 0.97263094 0.97485521 0.17633252]\n",
      "iteration 90400, loss = 0.10273217843133839, prediction = [0.15108863 0.97264409 0.97486735 0.17629045]\n",
      "iteration 90450, loss = 0.10270206062368295, prediction = [0.15105171 0.97265723 0.97487948 0.17624842]\n",
      "iteration 90500, loss = 0.10267196918108101, prediction = [0.15101482 0.97267035 0.9748916  0.17620641]\n",
      "iteration 90550, loss = 0.10264190406556109, prediction = [0.15097796 0.97268346 0.9749037  0.17616444]\n",
      "iteration 90600, loss = 0.1026118652392268, prediction = [0.15094113 0.97269655 0.9749158  0.1761225 ]\n",
      "iteration 90650, loss = 0.10258185266425834, prediction = [0.15090432 0.97270963 0.97492788 0.17608059]\n",
      "iteration 90700, loss = 0.10255186630291067, prediction = [0.15086755 0.9727227  0.97493994 0.17603872]\n",
      "iteration 90750, loss = 0.10252190611751402, prediction = [0.1508308  0.97273576 0.974952   0.17599687]\n",
      "iteration 90800, loss = 0.10249197207047432, prediction = [0.15079408 0.9727488  0.97496404 0.17595506]\n",
      "iteration 90850, loss = 0.10246206412427182, prediction = [0.15075739 0.97276183 0.97497607 0.17591329]\n",
      "iteration 90900, loss = 0.10243218224146151, prediction = [0.15072074 0.97277484 0.97498809 0.17587154]\n",
      "iteration 90950, loss = 0.10240232638467332, prediction = [0.15068411 0.97278785 0.9750001  0.17582983]\n",
      "iteration 91000, loss = 0.10237249651661123, prediction = [0.1506475  0.97280084 0.97501209 0.17578815]\n",
      "iteration 91050, loss = 0.10234269260005338, prediction = [0.15061093 0.97281381 0.97502407 0.1757465 ]\n",
      "iteration 91100, loss = 0.1023129145978515, prediction = [0.15057439 0.97282677 0.97503604 0.17570488]\n",
      "iteration 91150, loss = 0.10228316247293151, prediction = [0.15053787 0.97283972 0.975048   0.1756633 ]\n",
      "iteration 91200, loss = 0.10225343618829305, prediction = [0.15050138 0.97285266 0.97505994 0.17562175]\n",
      "iteration 91250, loss = 0.1022237357070084, prediction = [0.15046493 0.97286558 0.97507188 0.17558023]\n",
      "iteration 91300, loss = 0.10219406099222382, prediction = [0.1504285  0.97287849 0.9750838  0.17553874]\n",
      "iteration 91350, loss = 0.1021644120071582, prediction = [0.15039209 0.97289139 0.9750957  0.17549729]\n",
      "iteration 91400, loss = 0.10213478871510359, prediction = [0.15035572 0.97290428 0.9751076  0.17545586]\n",
      "iteration 91450, loss = 0.1021051910794239, prediction = [0.15031938 0.97291715 0.97511949 0.17541447]\n",
      "iteration 91500, loss = 0.10207561906355653, prediction = [0.15028306 0.97293001 0.97513136 0.17537311]\n",
      "iteration 91550, loss = 0.10204607263101032, prediction = [0.15024677 0.97294285 0.97514322 0.17533178]\n",
      "iteration 91600, loss = 0.10201655174536658, prediction = [0.15021051 0.97295568 0.97515507 0.17529049]\n",
      "iteration 91650, loss = 0.10198705637027905, prediction = [0.15017428 0.9729685  0.9751669  0.17524922]\n",
      "iteration 91700, loss = 0.10195758646947145, prediction = [0.15013808 0.97298131 0.97517873 0.17520799]\n",
      "iteration 91750, loss = 0.10192814200674091, prediction = [0.15010191 0.9729941  0.97519054 0.17516679]\n",
      "iteration 91800, loss = 0.10189872294595559, prediction = [0.15006576 0.97300688 0.97520234 0.17512562]\n",
      "iteration 91850, loss = 0.10186932925105391, prediction = [0.15002964 0.97301965 0.97521413 0.17508448]\n",
      "iteration 91900, loss = 0.10183996088604659, prediction = [0.14999355 0.9730324  0.9752259  0.17504337]\n",
      "iteration 91950, loss = 0.10181061781501419, prediction = [0.14995749 0.97304515 0.97523767 0.1750023 ]\n",
      "iteration 92000, loss = 0.10178130000210797, prediction = [0.14992146 0.97305788 0.97524942 0.17496125]\n",
      "iteration 92050, loss = 0.10175200741154981, prediction = [0.14988545 0.97307059 0.97526116 0.17492024]\n",
      "iteration 92100, loss = 0.10172274000763244, prediction = [0.14984947 0.9730833  0.97527289 0.17487926]\n",
      "iteration 92150, loss = 0.10169349775471817, prediction = [0.14981352 0.97309599 0.97528461 0.17483831]\n",
      "iteration 92200, loss = 0.10166428061723942, prediction = [0.1497776  0.97310866 0.97529631 0.17479739]\n",
      "iteration 92250, loss = 0.10163508855969827, prediction = [0.14974171 0.97312133 0.97530801 0.17475651]\n",
      "iteration 92300, loss = 0.10160592154666628, prediction = [0.14970584 0.97313398 0.97531969 0.17471565]\n",
      "iteration 92350, loss = 0.10157677954278489, prediction = [0.14967    0.97314662 0.97533136 0.17467482]\n",
      "iteration 92400, loss = 0.1015476625127643, prediction = [0.14963419 0.97315925 0.97534302 0.17463403]\n",
      "iteration 92450, loss = 0.10151857042138437, prediction = [0.14959841 0.97317187 0.97535466 0.17459327]\n",
      "iteration 92500, loss = 0.10148950323349323, prediction = [0.14956265 0.97318447 0.9753663  0.17455254]\n",
      "iteration 92550, loss = 0.1014604609140077, prediction = [0.14952693 0.97319706 0.97537792 0.17451184]\n",
      "iteration 92600, loss = 0.10143144342791403, prediction = [0.14949123 0.97320963 0.97538953 0.17447117]\n",
      "iteration 92650, loss = 0.10140245074026633, prediction = [0.14945555 0.9732222  0.97540113 0.17443053]\n",
      "iteration 92700, loss = 0.10137348281618679, prediction = [0.14941991 0.97323475 0.97541272 0.17438992]\n",
      "iteration 92750, loss = 0.10134453962086537, prediction = [0.14938429 0.97324729 0.9754243  0.17434934]\n",
      "iteration 92800, loss = 0.10131562111956036, prediction = [0.1493487  0.97325982 0.97543586 0.1743088 ]\n",
      "iteration 92850, loss = 0.10128672727759835, prediction = [0.14931314 0.97327233 0.97544742 0.17426828]\n",
      "iteration 92900, loss = 0.1012578580603723, prediction = [0.1492776  0.97328483 0.97545896 0.1742278 ]\n",
      "iteration 92950, loss = 0.10122901343334308, prediction = [0.14924209 0.97329732 0.97547049 0.17418734]\n",
      "iteration 93000, loss = 0.10120019336203895, prediction = [0.14920661 0.9733098  0.97548201 0.17414692]\n",
      "iteration 93050, loss = 0.10117139781205484, prediction = [0.14917116 0.97332226 0.97549352 0.17410653]\n",
      "iteration 93100, loss = 0.10114262674905289, prediction = [0.14913573 0.97333472 0.97550501 0.17406617]\n",
      "iteration 93150, loss = 0.10111388013876153, prediction = [0.14910034 0.97334716 0.9755165  0.17402583]\n",
      "iteration 93200, loss = 0.10108515794697624, prediction = [0.14906496 0.97335958 0.97552797 0.17398553]\n",
      "iteration 93250, loss = 0.10105646013955892, prediction = [0.14902962 0.973372   0.97553943 0.17394526]\n",
      "iteration 93300, loss = 0.10102778668243664, prediction = [0.1489943  0.9733844  0.97555088 0.17390502]\n",
      "iteration 93350, loss = 0.10099913754160356, prediction = [0.14895901 0.97339679 0.97556232 0.17386481]\n",
      "iteration 93400, loss = 0.10097051268311982, prediction = [0.14892375 0.97340917 0.97557375 0.17382463]\n",
      "iteration 93450, loss = 0.1009419120731106, prediction = [0.14888851 0.97342154 0.97558517 0.17378448]\n",
      "iteration 93500, loss = 0.10091333567776731, prediction = [0.1488533  0.97343389 0.97559657 0.17374436]\n",
      "iteration 93550, loss = 0.1008847834633458, prediction = [0.14881812 0.97344623 0.97560796 0.17370428]\n",
      "iteration 93600, loss = 0.10085625539616802, prediction = [0.14878297 0.97345856 0.97561935 0.17366422]\n",
      "iteration 93650, loss = 0.10082775144262077, prediction = [0.14874784 0.97347088 0.97563072 0.17362419]\n",
      "iteration 93700, loss = 0.10079927156915551, prediction = [0.14871273 0.97348319 0.97564208 0.17358419]\n",
      "iteration 93750, loss = 0.100770815742289, prediction = [0.14867766 0.97349548 0.97565342 0.17354422]\n",
      "iteration 93800, loss = 0.10074238392860188, prediction = [0.14864261 0.97350776 0.97566476 0.17350428]\n",
      "iteration 93850, loss = 0.10071397609473981, prediction = [0.14860759 0.97352003 0.97567609 0.17346437]\n",
      "iteration 93900, loss = 0.10068559220741281, prediction = [0.14857259 0.97353229 0.9756874  0.1734245 ]\n",
      "iteration 93950, loss = 0.1006572322333941, prediction = [0.14853763 0.97354453 0.97569871 0.17338465]\n",
      "iteration 94000, loss = 0.10062889613952186, prediction = [0.14850269 0.97355676 0.97571    0.17334483]\n",
      "iteration 94050, loss = 0.10060058389269763, prediction = [0.14846777 0.97356898 0.97572128 0.17330504]\n",
      "iteration 94100, loss = 0.10057229545988701, prediction = [0.14843288 0.97358119 0.97573255 0.17326528]\n",
      "iteration 94150, loss = 0.100544030808118, prediction = [0.14839802 0.97359339 0.97574381 0.17322555]\n",
      "iteration 94200, loss = 0.10051578990448354, prediction = [0.14836318 0.97360557 0.97575506 0.17318585]\n",
      "iteration 94250, loss = 0.10048757271613831, prediction = [0.14832838 0.97361775 0.97576629 0.17314618]\n",
      "iteration 94300, loss = 0.10045937921030043, prediction = [0.14829359 0.97362991 0.97577752 0.17310654]\n",
      "iteration 94350, loss = 0.10043120935425194, prediction = [0.14825884 0.97364206 0.97578873 0.17306693]\n",
      "iteration 94400, loss = 0.10040306311533566, prediction = [0.14822411 0.97365419 0.97579994 0.17302735]\n",
      "iteration 94450, loss = 0.1003749404609589, prediction = [0.1481894  0.97366632 0.97581113 0.1729878 ]\n",
      "iteration 94500, loss = 0.10034684135858984, prediction = [0.14815473 0.97367843 0.97582231 0.17294828]\n",
      "iteration 94550, loss = 0.1003187657757606, prediction = [0.14812007 0.97369053 0.97583348 0.17290879]\n",
      "iteration 94600, loss = 0.10029071368006325, prediction = [0.14808545 0.97370262 0.97584464 0.17286932]\n",
      "iteration 94650, loss = 0.10026268503915331, prediction = [0.14805085 0.9737147  0.97585579 0.17282989]\n",
      "iteration 94700, loss = 0.10023467982074769, prediction = [0.14801628 0.97372676 0.97586693 0.17279048]\n",
      "iteration 94750, loss = 0.10020669799262555, prediction = [0.14798173 0.97373882 0.97587806 0.17275111]\n",
      "iteration 94800, loss = 0.10017873952262635, prediction = [0.14794721 0.97375086 0.97588917 0.17271176]\n",
      "iteration 94850, loss = 0.10015080437865181, prediction = [0.14791272 0.97376289 0.97590028 0.17267245]\n",
      "iteration 94900, loss = 0.10012289252866463, prediction = [0.14787825 0.97377491 0.97591137 0.17263316]\n",
      "iteration 94950, loss = 0.10009500394068882, prediction = [0.14784381 0.97378692 0.97592245 0.1725939 ]\n",
      "iteration 95000, loss = 0.10006713858280904, prediction = [0.14780939 0.97379891 0.97593353 0.17255467]\n",
      "iteration 95050, loss = 0.10003929642317011, prediction = [0.147775   0.9738109  0.97594459 0.17251547]\n",
      "iteration 95100, loss = 0.10001147742997837, prediction = [0.14774064 0.97382287 0.97595564 0.1724763 ]\n",
      "iteration 95150, loss = 0.09998368157150003, prediction = [0.1477063  0.97383483 0.97596668 0.17243716]\n",
      "iteration 95200, loss = 0.09995590881606253, prediction = [0.14767198 0.97384678 0.97597771 0.17239805]\n",
      "iteration 95250, loss = 0.09992815913205211, prediction = [0.1476377  0.97385872 0.97598873 0.17235896]\n",
      "iteration 95300, loss = 0.09990043248791614, prediction = [0.14760344 0.97387064 0.97599973 0.17231991]\n",
      "iteration 95350, loss = 0.09987272885216125, prediction = [0.1475692  0.97388255 0.97601073 0.17228088]\n",
      "iteration 95400, loss = 0.09984504819335424, prediction = [0.14753499 0.97389446 0.97602172 0.17224188]\n",
      "iteration 95450, loss = 0.09981739048012105, prediction = [0.14750081 0.97390635 0.97603269 0.17220291]\n",
      "iteration 95500, loss = 0.09978975568114709, prediction = [0.14746665 0.97391823 0.97604366 0.17216397]\n",
      "iteration 95550, loss = 0.09976214376517792, prediction = [0.14743251 0.9739301  0.97605461 0.17212506]\n",
      "iteration 95600, loss = 0.09973455470101711, prediction = [0.14739841 0.97394195 0.97606556 0.17208618]\n",
      "iteration 95650, loss = 0.09970698845752779, prediction = [0.14736432 0.9739538  0.97607649 0.17204733]\n",
      "iteration 95700, loss = 0.09967944500363199, prediction = [0.14733027 0.97396563 0.97608741 0.1720085 ]\n",
      "iteration 95750, loss = 0.09965192430830996, prediction = [0.14729624 0.97397745 0.97609832 0.1719697 ]\n",
      "iteration 95800, loss = 0.09962442634060178, prediction = [0.14726223 0.97398926 0.97610923 0.17193094]\n",
      "iteration 95850, loss = 0.0995969510696047, prediction = [0.14722825 0.97400106 0.97612012 0.1718922 ]\n",
      "iteration 95900, loss = 0.09956949846447458, prediction = [0.1471943  0.97401285 0.976131   0.17185349]\n",
      "iteration 95950, loss = 0.09954206849442594, prediction = [0.14716037 0.97402463 0.97614187 0.1718148 ]\n",
      "iteration 96000, loss = 0.09951466112873077, prediction = [0.14712646 0.97403639 0.97615272 0.17177615]\n",
      "iteration 96050, loss = 0.09948727633671896, prediction = [0.14709258 0.97404815 0.97616357 0.17173752]\n",
      "iteration 96100, loss = 0.09945991408777874, prediction = [0.14705873 0.97405989 0.97617441 0.17169892]\n",
      "iteration 96150, loss = 0.09943257435135494, prediction = [0.1470249  0.97407162 0.97618524 0.17166036]\n",
      "iteration 96200, loss = 0.09940525709695112, prediction = [0.1469911  0.97408334 0.97619606 0.17162181]\n",
      "iteration 96250, loss = 0.09937796229412717, prediction = [0.14695732 0.97409505 0.97620686 0.1715833 ]\n",
      "iteration 96300, loss = 0.09935068991250061, prediction = [0.14692357 0.97410675 0.97621766 0.17154482]\n",
      "iteration 96350, loss = 0.09932343992174558, prediction = [0.14688984 0.97411843 0.97622844 0.17150636]\n",
      "iteration 96400, loss = 0.09929621229159398, prediction = [0.14685614 0.97413011 0.97623922 0.17146793]\n",
      "iteration 96450, loss = 0.09926900699183369, prediction = [0.14682246 0.97414177 0.97624999 0.17142953]\n",
      "iteration 96500, loss = 0.09924182399230921, prediction = [0.1467888  0.97415342 0.97626074 0.17139116]\n",
      "iteration 96550, loss = 0.09921466326292283, prediction = [0.14675518 0.97416506 0.97627148 0.17135282]\n",
      "iteration 96600, loss = 0.09918752477363088, prediction = [0.14672157 0.97417669 0.97628222 0.1713145 ]\n",
      "iteration 96650, loss = 0.09916040849444863, prediction = [0.14668799 0.97418831 0.97629294 0.17127621]\n",
      "iteration 96700, loss = 0.09913331439544534, prediction = [0.14665444 0.97419992 0.97630366 0.17123795]\n",
      "iteration 96750, loss = 0.099106242446747, prediction = [0.14662091 0.97421152 0.97631436 0.17119972]\n",
      "iteration 96800, loss = 0.09907919261853582, prediction = [0.14658741 0.9742231  0.97632505 0.17116151]\n",
      "iteration 96850, loss = 0.09905216488104912, prediction = [0.14655393 0.97423468 0.97633573 0.17112334]\n",
      "iteration 96900, loss = 0.09902515920458019, prediction = [0.14652048 0.97424624 0.97634641 0.17108519]\n",
      "iteration 96950, loss = 0.09899817555947724, prediction = [0.14648705 0.9742578  0.97635707 0.17104707]\n",
      "iteration 97000, loss = 0.09897121391614461, prediction = [0.14645364 0.97426934 0.97636772 0.17100897]\n",
      "iteration 97050, loss = 0.09894427424504137, prediction = [0.14642026 0.97428087 0.97637836 0.17097091]\n",
      "iteration 97100, loss = 0.09891735651668093, prediction = [0.1463869  0.97429239 0.97638899 0.17093287]\n",
      "iteration 97150, loss = 0.09889046070163282, prediction = [0.14635357 0.9743039  0.97639962 0.17089486]\n",
      "iteration 97200, loss = 0.09886358677052046, prediction = [0.14632027 0.97431539 0.97641023 0.17085687]\n",
      "iteration 97250, loss = 0.09883673469402263, prediction = [0.14628698 0.97432688 0.97642083 0.17081892]\n",
      "iteration 97300, loss = 0.09880990444287237, prediction = [0.14625373 0.97433836 0.97643142 0.17078099]\n",
      "iteration 97350, loss = 0.09878309598785691, prediction = [0.14622049 0.97434982 0.976442   0.17074309]\n",
      "iteration 97400, loss = 0.09875630929981746, prediction = [0.14618729 0.97436128 0.97645257 0.17070522]\n",
      "iteration 97450, loss = 0.09872954434965014, prediction = [0.1461541  0.97437272 0.97646313 0.17066737]\n",
      "iteration 97500, loss = 0.09870280110830484, prediction = [0.14612094 0.97438415 0.97647368 0.17062955]\n",
      "iteration 97550, loss = 0.0986760795467849, prediction = [0.14608781 0.97439557 0.97648422 0.17059176]\n",
      "iteration 97600, loss = 0.09864937963614774, prediction = [0.14605469 0.97440698 0.97649475 0.170554  ]\n",
      "iteration 97650, loss = 0.09862270134750444, prediction = [0.14602161 0.97441838 0.97650527 0.17051626]\n",
      "iteration 97700, loss = 0.09859604465201896, prediction = [0.14598854 0.97442977 0.97651578 0.17047855]\n",
      "iteration 97750, loss = 0.09856940952091012, prediction = [0.14595551 0.97444115 0.97652628 0.17044087]\n",
      "iteration 97800, loss = 0.09854279592544839, prediction = [0.14592249 0.97445252 0.97653677 0.17040321]\n",
      "iteration 97850, loss = 0.09851620383695839, prediction = [0.1458895  0.97446387 0.97654725 0.17036559]\n",
      "iteration 97900, loss = 0.09848963322681667, prediction = [0.14585653 0.97447522 0.97655773 0.17032799]\n",
      "iteration 97950, loss = 0.0984630840664539, prediction = [0.14582359 0.97448655 0.97656819 0.17029041]\n",
      "iteration 98000, loss = 0.09843655632735271, prediction = [0.14579067 0.97449788 0.97657864 0.17025287]\n",
      "iteration 98050, loss = 0.09841004998104828, prediction = [0.14575778 0.97450919 0.97658908 0.17021535]\n",
      "iteration 98100, loss = 0.09838356499912927, prediction = [0.14572491 0.9745205  0.97659951 0.17017785]\n",
      "iteration 98150, loss = 0.0983571013532355, prediction = [0.14569206 0.97453179 0.97660993 0.17014039]\n",
      "iteration 98200, loss = 0.0983306590150593, prediction = [0.14565924 0.97454307 0.97662034 0.17010295]\n",
      "iteration 98250, loss = 0.09830423795634566, prediction = [0.14562644 0.97455434 0.97663074 0.17006554]\n",
      "iteration 98300, loss = 0.09827783814889146, prediction = [0.14559367 0.9745656  0.97664113 0.17002815]\n",
      "iteration 98350, loss = 0.09825145956454509, prediction = [0.14556092 0.97457685 0.97665152 0.16999079]\n",
      "iteration 98400, loss = 0.09822510217520661, prediction = [0.14552819 0.97458809 0.97666189 0.16995346]\n",
      "iteration 98450, loss = 0.09819876595282787, prediction = [0.14549549 0.97459932 0.97667225 0.16991616]\n",
      "iteration 98500, loss = 0.0981724508694129, prediction = [0.14546281 0.97461054 0.9766826  0.16987888]\n",
      "iteration 98550, loss = 0.09814615689701627, prediction = [0.14543016 0.97462175 0.97669294 0.16984163]\n",
      "iteration 98600, loss = 0.09811988400774418, prediction = [0.14539753 0.97463294 0.97670328 0.1698044 ]\n",
      "iteration 98650, loss = 0.09809363217375373, prediction = [0.14536492 0.97464413 0.9767136  0.16976721]\n",
      "iteration 98700, loss = 0.09806740136725356, prediction = [0.14533233 0.97465531 0.97672391 0.16973003]\n",
      "iteration 98750, loss = 0.09804119156050267, prediction = [0.14529977 0.97466647 0.97673422 0.16969289]\n",
      "iteration 98800, loss = 0.09801500272581115, prediction = [0.14526724 0.97467763 0.97674451 0.16965577]\n",
      "iteration 98850, loss = 0.09798883483553976, prediction = [0.14523472 0.97468877 0.9767548  0.16961868]\n",
      "iteration 98900, loss = 0.09796268786209952, prediction = [0.14520223 0.97469991 0.97676507 0.16958161]\n",
      "iteration 98950, loss = 0.0979365617779523, prediction = [0.14516977 0.97471103 0.97677533 0.16954457]\n",
      "iteration 99000, loss = 0.09791045655561051, prediction = [0.14513732 0.97472214 0.97678559 0.16950756]\n",
      "iteration 99050, loss = 0.09788437216763561, prediction = [0.1451049  0.97473325 0.97679584 0.16947057]\n",
      "iteration 99100, loss = 0.09785830858664066, prediction = [0.14507251 0.97474434 0.97680607 0.16943361]\n",
      "iteration 99150, loss = 0.0978322657852875, prediction = [0.14504014 0.97475542 0.9768163  0.16939668]\n",
      "iteration 99200, loss = 0.09780624373628838, prediction = [0.14500779 0.97476649 0.97682651 0.16935977]\n",
      "iteration 99250, loss = 0.09778024241240499, prediction = [0.14497546 0.97477756 0.97683672 0.16932289]\n",
      "iteration 99300, loss = 0.09775426178644922, prediction = [0.14494316 0.97478861 0.97684692 0.16928604]\n",
      "iteration 99350, loss = 0.09772830183128188, prediction = [0.14491088 0.97479965 0.97685711 0.16924921]\n",
      "iteration 99400, loss = 0.09770236251981321, prediction = [0.14487862 0.97481068 0.97686729 0.1692124 ]\n",
      "iteration 99450, loss = 0.09767644382500304, prediction = [0.14484639 0.9748217  0.97687746 0.16917563]\n",
      "iteration 99500, loss = 0.09765054571986047, prediction = [0.14481418 0.97483271 0.97688762 0.16913888]\n",
      "iteration 99550, loss = 0.09762466817744292, prediction = [0.14478199 0.97484371 0.97689777 0.16910215]\n",
      "iteration 99600, loss = 0.09759881117085756, prediction = [0.14474983 0.9748547  0.97690791 0.16906545]\n",
      "iteration 99650, loss = 0.0975729746732597, prediction = [0.14471769 0.97486568 0.97691804 0.16902878]\n",
      "iteration 99700, loss = 0.09754715865785404, prediction = [0.14468557 0.97487665 0.97692816 0.16899213]\n",
      "iteration 99750, loss = 0.09752136309789321, prediction = [0.14465348 0.97488761 0.97693827 0.16895551]\n",
      "iteration 99800, loss = 0.09749558796667893, prediction = [0.14462141 0.97489856 0.97694838 0.16891892]\n",
      "iteration 99850, loss = 0.0974698332375605, prediction = [0.14458936 0.9749095  0.97695847 0.16888235]\n",
      "iteration 99900, loss = 0.09744409888393671, prediction = [0.14455733 0.97492043 0.97696855 0.16884581]\n",
      "iteration 99950, loss = 0.09741838487925333, prediction = [0.14452533 0.97493135 0.97697863 0.16880929]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.0, 0.0],\n",
    "              [0.0, 1.0],\n",
    "              [1.0, 0.0],\n",
    "              [1.0, 1.0]])\n",
    "y = np.array([[0.0],\n",
    "              [1.0],\n",
    "              [1.0],\n",
    "              [0.0]])\n",
    "\n",
    "n = x.shape[0]\n",
    "p = x.shape[1]\n",
    "d = y.shape[1]\n",
    "r = 10\n",
    "np.random.seed(123)\n",
    "w1 = np.random.normal(size=(p, r))\n",
    "b1 = np.random.normal(size=(r, 1))\n",
    "w2 = np.random.normal(size=(r, d))\n",
    "b2 = np.random.normal(size=(d, 1))\n",
    "\n",
    "nepoch = 100000\n",
    "learning_rate = 0.01   #这里改动了一下学习率\n",
    "\n",
    "for i in range(nepoch):\n",
    "    z1, a1, z2, a2 = forward(x, w1, b1, w2, b2)\n",
    "    phat = a2\n",
    "    loss = loss_function(y, phat)\n",
    "    dw1, db1, dw2, db2 = backward(x, w1, b1, w2, b2, z1, a1, z2, a2, y)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(f\"iteration {i}, loss = {loss}, prediction = {phat.squeeze()}\")\n",
    "        \n",
    "    w1 = w1 - learning_rate * dw1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    w2 = w2 - learning_rate * dw2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "#根据最后迭代出的prediction，可以发现能够准确预测XOR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
